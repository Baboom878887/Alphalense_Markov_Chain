{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>\n",
    "\n",
    "# Volatility Trading Research Notebook\n",
    "\n",
    "## Options Strategies with Ensemble Regime Detection\n",
    "\n",
    "This notebook implements a comprehensive volatility trading research framework for QuantConnect.\n",
    "\n",
    "**Key Features:**\n",
    "- Multiple volatility models with PCA-based aggregate forecasting\n",
    "- Ensemble regime detection (GMM, K-Means, Agglomerative Clustering, Change-Point Detection)\n",
    "- Six options strategies with delta optimization\n",
    "- Stop loss optimization per strategy and regime\n",
    "- Vectorized computations (no multiprocessing)\n",
    "\n",
    "**Test Ticker:** HIMS (ticker-agnostic design for future portfolio expansion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Initial Setup & Structure\n",
    "\n",
    "### 1.1 Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: INITIAL SETUP & STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "# Standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import json\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Machine Learning - Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Change-Point Detection\n",
    "try:\n",
    "    import ruptures as rpt\n",
    "    RUPTURES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RUPTURES_AVAILABLE = False\n",
    "    print(\"Warning: ruptures library not available. Install with: pip install ruptures\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Warning: plotly not available for interactive charts\")\n",
    "\n",
    "# Set plotting defaults\n",
    "try:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except:\n        plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# QuantConnect Research Environment\n",
    "qb = QuantBook()\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Ruptures available: {RUPTURES_AVAILABLE}\")\n",
    "print(f\"Plotly available: {PLOTLY_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Color Palette Definitions\n",
    "\n",
    "**CRITICAL:** These color mappings must be used consistently across ALL visualizations throughout the notebook.\n",
    "\n",
    "#### Strategy Colors\n",
    "| Strategy | Color | Hex Code |\n",
    "|----------|-------|----------|\n",
    "| Calendar Spread | Blue | #1f77b4 |\n",
    "| Double Diagonal | Orange | #ff7f0e |\n",
    "| Straddle | Green | #2ca02c |\n",
    "| Strangle | Red | #d62728 |\n",
    "| Bull Put Spread | Purple | #9467bd |\n",
    "| Bull Call Spread | Brown | #8c564b |\n",
    "\n",
    "#### Regime Colors\n",
    "| Regime | Color | Hex Code |\n",
    "|--------|-------|----------|\n",
    "| Bull_Low_Vol | Green | #2ecc71 |\n",
    "| Bull_High_Vol | Orange | #f39c12 |\n",
    "| Bear | Red | #e74c3c |\n",
    "| Choppy | Gray | #95a5a6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLOR PALETTE DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "STRATEGY_COLORS = {\n",
    "    'Calendar Spread': '#1f77b4',    # Blue\n",
    "    'Double Diagonal': '#ff7f0e',    # Orange\n",
    "    'Straddle': '#2ca02c',           # Green\n",
    "    'Strangle': '#d62728',           # Red\n",
    "    'Bull Put Spread': '#9467bd',    # Purple\n",
    "    'Bull Call Spread': '#8c564b',   # Brown\n",
    "}\n",
    "\n",
    "# Regime color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "REGIME_COLORS = {\n",
    "    'Bull_Low_Vol': '#2ecc71',       # Green\n",
    "    'Bull_High_Vol': '#f39c12',      # Orange\n",
    "    'Bear': '#e74c3c',               # Red\n",
    "    'Choppy': '#95a5a6',             # Gray\n",
    "    'Sideways': '#95a5a6',           # Gray (alias)\n",
    "    'Unknown': '#bdc3c7',            # Light Gray\n",
    "}\n",
    "\n",
    "# Volatility regime colors\n",
    "VOL_REGIME_COLORS = {\n",
    "    'Low': '#3498db',                # Light Blue\n",
    "    'Normal': '#2ecc71',             # Green\n",
    "    'Elevated': '#f39c12',           # Orange\n",
    "    'Extreme': '#e74c3c',            # Red\n",
    "}\n",
    "\n",
    "# Helper functions for consistent color access\n",
    "def get_strategy_color(strategy_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a strategy.\"\"\"\n",
    "    return STRATEGY_COLORS.get(strategy_name, '#7f7f7f')  # Default gray\n",
    "\n",
    "def get_regime_color(regime_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a regime.\"\"\"\n",
    "    return REGIME_COLORS.get(regime_name, '#bdc3c7')  # Default light gray\n",
    "\n",
    "def get_vol_regime_color(vol_regime: str) -> str:\n",
    "    \"\"\"Get consistent color for a volatility regime.\"\"\"\n",
    "    return VOL_REGIME_COLORS.get(vol_regime, '#bdc3c7')\n",
    "\n",
    "# Display color palettes\n",
    "print(\"Strategy Colors:\")\n",
    "for strategy, color in STRATEGY_COLORS.items():\n",
    "    print(f\"  {strategy}: {color}\")\n",
    "\n",
    "print(\"\\nRegime Colors:\")\n",
    "for regime, color in REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")\n",
    "\n",
    "print(\"\\nVolatility Regime Colors:\")\n",
    "for regime, color in VOL_REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Primary ticker for analysis (ticker-agnostic design)\n",
    "TICKER = 'HIMS'\n",
    "\n",
    "# Market and sector proxies\n",
    "MARKET_PROXY = 'SPY'\n",
    "SECTOR_MAPPING = {\n",
    "    'Technology': 'XLK',\n",
    "    'Healthcare': 'XLV',\n",
    "    'Financial': 'XLF',\n",
    "    'Consumer Discretionary': 'XLY',\n",
    "    'Consumer Staples': 'XLP',\n",
    "    'Energy': 'XLE',\n",
    "    'Utilities': 'XLU',\n",
    "    'Materials': 'XLB',\n",
    "    'Industrials': 'XLI',\n",
    "    'Real Estate': 'XLRE',\n",
    "    'Communication Services': 'XLC',\n",
    "}\n",
    "\n",
    "# Default sector ETF (HIMS is healthcare)\n",
    "SECTOR_ETF = 'XLV'\n",
    "\n",
    "# Date range for analysis (minimum 2-3 years recommended)\n",
    "START_DATE = datetime(2022, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "\n",
    "# Volatility calculation windows\n",
    "VOL_WINDOWS = [5, 10, 20, 30]\n",
    "\n",
    "# PCA configuration\n",
    "PCA_LAG_PERIODS = [1, 2, 3, 5, 10, 20]\n",
    "PCA_N_COMPONENTS = 3\n",
    "\n",
    "# Regime detection configuration\n",
    "N_REGIMES_RANGE = range(3, 7)  # Test 3-6 regimes\n",
    "DEFAULT_N_REGIMES = 4\n",
    "\n",
    "# Options strategy configurations\n",
    "CALENDAR_DELTAS = [0.50, 0.40, 0.30, 0.20]\n",
    "DIAGONAL_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20)]\n",
    "STRANGLE_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20), (0.16, -0.16)]\n",
    "BULL_PUT_SHORT_DELTAS = [-0.30, -0.20, -0.16, -0.10]\n",
    "BULL_CALL_LONG_DELTAS = [0.50, 0.40, 0.30]\n",
    "SPREAD_WIDTHS = [5, 10]  # Strike width for spreads\n",
    "\n",
    "# Stop loss thresholds to test\n",
    "STOP_LOSS_THRESHOLDS = [-0.10, -0.15, -0.20, -0.25, -0.30, -0.40, -0.50, None]  # None = no stop\n",
    "\n",
    "# Rolling window for metrics\n",
    "ROLLING_SHARPE_WINDOW = 60\n",
    "\n",
    "# Minimum figure sizes\n",
    "FIG_SIZE_TIMESERIES = (14, 6)\n",
    "FIG_SIZE_REGIME = (14, 10)\n",
    "FIG_SIZE_COMPARISON = (14, 8)\n",
    "FIG_SIZE_HEATMAP = (10, 8)\n",
    "FIG_SIZE_DASHBOARD = (22, 16)\n",
    "FIG_SIZE_ROLLING = (12, 5)\n",
    "\n",
    "print(f\"Configuration loaded for ticker: {TICKER}\")\n",
    "print(f\"Analysis period: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"Volatility windows: {VOL_WINDOWS}\")\n",
    "print(f\"Testing {len(STOP_LOSS_THRESHOLDS)} stop loss thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# DATA FETCHING FUNCTIONS - REAL QUANTCONNECT OPTIONS DATA\n# ============================================================================\n\ndef fetch_equity_data(ticker: str, start_date: datetime, end_date: datetime, \n                      resolution: str = 'Daily') -> pd.DataFrame:\n    \"\"\"\n    Fetch historical OHLCV data for an equity.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Stock ticker symbol\n    start_date : datetime\n        Start date for data fetch\n    end_date : datetime\n        End date for data fetch\n    resolution : str\n        Data resolution ('Daily', 'Hour', 'Minute')\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Historical OHLCV data\n    \"\"\"\n    # Add equity to universe\n    symbol = qb.AddEquity(ticker, Resolution.Daily).Symbol\n    \n    # Fetch historical data\n    history = qb.History(symbol, start_date, end_date, Resolution.Daily)\n    \n    if history.empty:\n        print(f\"Warning: No data available for {ticker}\")\n        return pd.DataFrame()\n    \n    # Reset index and clean up\n    df = history.reset_index()\n    df = df.rename(columns={'time': 'Date'})\n    df = df.set_index('Date')\n    \n    # Calculate returns\n    df['returns'] = df['close'].pct_change()\n    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n    \n    return df\n\n\ndef fetch_options_with_greeks(ticker: str, date: datetime, \n                               strategy_type: str = None,\n                               min_dte: int = 7, max_dte: int = 60,\n                               delta_range: tuple = None) -> pd.DataFrame:\n    \"\"\"\n    Fetch options chain data with Greeks from QuantConnect API.\n    Filters contracts to those needed for specific strategies.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying stock ticker\n    date : datetime\n        Date to fetch options chain\n    strategy_type : str\n        Strategy type to filter contracts for (optional)\n    min_dte : int\n        Minimum days to expiration\n    max_dte : int\n        Maximum days to expiration\n    delta_range : tuple\n        (min_delta, max_delta) to filter by absolute delta\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Options chain with strikes, prices, and Greeks from QuantConnect API\n    \"\"\"\n    # Add equity and options\n    equity = qb.AddEquity(ticker, Resolution.Daily)\n    option = qb.AddOption(ticker, Resolution.Daily)\n    \n    # Configure option filter based on strategy needs\n    if strategy_type == 'Calendar Spread':\n        # Calendar spreads need ATM options at different expirations\n        option.SetFilter(lambda u: u.Strikes(-5, 5).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type == 'Double Diagonal':\n        # Double diagonals need OTM options at different expirations\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type in ['Straddle', 'Strangle']:\n        # Straddles/Strangles need ATM/OTM puts and calls\n        option.SetFilter(lambda u: u.Strikes(-8, 8).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type in ['Bull Put Spread', 'Bull Call Spread']:\n        # Vertical spreads need specific strike ranges\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    else:\n        # Default filter\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    \n    # Get option chain for the date\n    chain = qb.OptionChainProvider.GetOptionContractList(equity.Symbol, date)\n    \n    if not chain:\n        print(f\"No options chain available for {ticker} on {date}\")\n        return pd.DataFrame()\n    \n    # Fetch contracts with Greeks from QuantConnect\n    options_data = []\n    \n    for contract in chain:\n        try:\n            # Filter by expiration\n            days_to_expiry = (contract.ID.Date - date).days\n            if days_to_expiry < min_dte or days_to_expiry > max_dte:\n                continue\n            \n            # Add contract to get Greeks from QuantConnect\n            option_contract = qb.AddOptionContract(contract, Resolution.Daily)\n            \n            # Get historical data including Greeks\n            history = qb.History(contract, date, date + timedelta(days=1), Resolution.Daily)\n            \n            if history.empty:\n                continue\n            \n            # Extract Greeks from QuantConnect API (attached to option contract)\n            # In QuantConnect, Greeks are available via option_contract.Greeks\n            row = {\n                'symbol': str(contract),\n                'underlying': ticker,\n                'strike': contract.ID.StrikePrice,\n                'expiry': contract.ID.Date,\n                'dte': days_to_expiry,\n                'option_type': 'Call' if contract.ID.OptionRight == OptionRight.Call else 'Put',\n                'bid': history['bidprice'].iloc[-1] if 'bidprice' in history.columns else np.nan,\n                'ask': history['askprice'].iloc[-1] if 'askprice' in history.columns else np.nan,\n                'mid': np.nan,  # Will calculate below\n                'last': history['close'].iloc[-1] if 'close' in history.columns else np.nan,\n                'volume': history['volume'].iloc[-1] if 'volume' in history.columns else 0,\n                'open_interest': history['openinterest'].iloc[-1] if 'openinterest' in history.columns else 0,\n                # Greeks from QuantConnect API\n                'delta': option_contract.Greeks.Delta if hasattr(option_contract, 'Greeks') else np.nan,\n                'gamma': option_contract.Greeks.Gamma if hasattr(option_contract, 'Greeks') else np.nan,\n                'theta': option_contract.Greeks.Theta if hasattr(option_contract, 'Greeks') else np.nan,\n                'vega': option_contract.Greeks.Vega if hasattr(option_contract, 'Greeks') else np.nan,\n                'rho': option_contract.Greeks.Rho if hasattr(option_contract, 'Greeks') else np.nan,\n                'iv': option_contract.ImpliedVolatility if hasattr(option_contract, 'ImpliedVolatility') else np.nan,\n            }\n            \n            # Calculate mid price\n            if not np.isnan(row['bid']) and not np.isnan(row['ask']):\n                row['mid'] = (row['bid'] + row['ask']) / 2\n            \n            # Filter by delta if specified\n            if delta_range and not np.isnan(row['delta']):\n                abs_delta = abs(row['delta'])\n                if abs_delta < delta_range[0] or abs_delta > delta_range[1]:\n                    continue\n            \n            options_data.append(row)\n            \n        except Exception as e:\n            continue\n    \n    df = pd.DataFrame(options_data)\n    \n    if not df.empty:\n        # Sort by expiry and strike\n        df = df.sort_values(['expiry', 'option_type', 'strike'])\n        print(f\"Fetched {len(df)} option contracts for {ticker} on {date}\")\n    \n    return df\n\n\ndef get_strategy_contracts(ticker: str, date: datetime, underlying_price: float,\n                           strategy_type: str, delta_config: dict = None,\n                           front_dte: int = 30, back_dte: int = 60) -> dict:\n    \"\"\"\n    Get specific option contracts needed for a strategy.\n    Uses QuantConnect API for Greeks and IV.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    date : datetime\n        Trade date\n    underlying_price : float\n        Current underlying price\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta targets for the strategy\n    front_dte : int\n        Days to expiry for front month\n    back_dte : int\n        Days to expiry for back month\n        \n    Returns:\n    --------\n    dict\n        Dictionary of contracts needed for the strategy\n    \"\"\"\n    contracts = {}\n    \n    # Fetch full chain with Greeks\n    chain = fetch_options_with_greeks(ticker, date, strategy_type, \n                                       min_dte=7, max_dte=back_dte + 15)\n    \n    if chain.empty:\n        return contracts\n    \n    if strategy_type == 'Calendar Spread':\n        # Calendar: Same strike, different expirations (ATM)\n        target_delta = delta_config.get('call_delta', 0.50) if delta_config else 0.50\n        \n        # Find ATM strike (closest to underlying price)\n        chain['strike_distance'] = np.abs(chain['strike'] - underlying_price)\n        atm_strike = chain.loc[chain['strike_distance'].idxmin(), 'strike']\n        \n        # Front month call (short)\n        front_calls = chain[(chain['option_type'] == 'Call') & \n                           (chain['strike'] == atm_strike) & \n                           (chain['dte'] >= front_dte - 5) & \n                           (chain['dte'] <= front_dte + 5)]\n        if not front_calls.empty:\n            contracts['front_call'] = front_calls.iloc[0].to_dict()\n        \n        # Back month call (long)\n        back_calls = chain[(chain['option_type'] == 'Call') & \n                          (chain['strike'] == atm_strike) & \n                          (chain['dte'] >= back_dte - 5) & \n                          (chain['dte'] <= back_dte + 5)]\n        if not back_calls.empty:\n            contracts['back_call'] = back_calls.iloc[0].to_dict()\n    \n    elif strategy_type == 'Double Diagonal':\n        # Double Diagonal: Calendar spreads on both calls and puts at different strikes\n        call_delta = delta_config.get('call_delta', 0.30) if delta_config else 0.30\n        put_delta = delta_config.get('put_delta', -0.30) if delta_config else -0.30\n        \n        # Select calls by delta\n        calls = chain[chain['option_type'] == 'Call'].copy()\n        if not calls.empty and 'delta' in calls.columns:\n            calls['delta_dist'] = np.abs(calls['delta'] - call_delta)\n            # Front and back month calls\n            front_calls = calls[(calls['dte'] >= front_dte - 5) & (calls['dte'] <= front_dte + 5)]\n            back_calls = calls[(calls['dte'] >= back_dte - 5) & (calls['dte'] <= back_dte + 5)]\n            \n            if not front_calls.empty:\n                contracts['front_call'] = front_calls.loc[front_calls['delta_dist'].idxmin()].to_dict()\n            if not back_calls.empty:\n                contracts['back_call'] = back_calls.loc[back_calls['delta_dist'].idxmin()].to_dict()\n        \n        # Select puts by delta\n        puts = chain[chain['option_type'] == 'Put'].copy()\n        if not puts.empty and 'delta' in puts.columns:\n            puts['delta_dist'] = np.abs(puts['delta'] - put_delta)\n            front_puts = puts[(puts['dte'] >= front_dte - 5) & (puts['dte'] <= front_dte + 5)]\n            back_puts = puts[(puts['dte'] >= back_dte - 5) & (puts['dte'] <= back_dte + 5)]\n            \n            if not front_puts.empty:\n                contracts['front_put'] = front_puts.loc[front_puts['delta_dist'].idxmin()].to_dict()\n            if not back_puts.empty:\n                contracts['back_put'] = back_puts.loc[back_puts['delta_dist'].idxmin()].to_dict()\n    \n    elif strategy_type == 'Straddle':\n        # Straddle: Long ATM call + Long ATM put\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        # Find ATM strike\n        chain['strike_distance'] = np.abs(chain['strike'] - underlying_price)\n        atm_strike = chain.loc[chain['strike_distance'].idxmin(), 'strike']\n        \n        # ATM Call\n        atm_calls = chain[(chain['option_type'] == 'Call') & \n                         (chain['strike'] == atm_strike) &\n                         (chain['dte'] >= target_dte_range[0]) & \n                         (chain['dte'] <= target_dte_range[1])]\n        if not atm_calls.empty:\n            contracts['long_call'] = atm_calls.iloc[0].to_dict()\n        \n        # ATM Put\n        atm_puts = chain[(chain['option_type'] == 'Put') & \n                        (chain['strike'] == atm_strike) &\n                        (chain['dte'] >= target_dte_range[0]) & \n                        (chain['dte'] <= target_dte_range[1])]\n        if not atm_puts.empty:\n            contracts['long_put'] = atm_puts.iloc[0].to_dict()\n    \n    elif strategy_type == 'Strangle':\n        # Strangle: Long OTM call + Long OTM put\n        call_delta = delta_config.get('call_delta', 0.25) if delta_config else 0.25\n        put_delta = delta_config.get('put_delta', -0.25) if delta_config else -0.25\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        # OTM Call by delta\n        calls = chain[(chain['option_type'] == 'Call') & \n                     (chain['dte'] >= target_dte_range[0]) & \n                     (chain['dte'] <= target_dte_range[1])].copy()\n        if not calls.empty and 'delta' in calls.columns:\n            calls['delta_dist'] = np.abs(calls['delta'] - call_delta)\n            contracts['long_call'] = calls.loc[calls['delta_dist'].idxmin()].to_dict()\n        \n        # OTM Put by delta  \n        puts = chain[(chain['option_type'] == 'Put') & \n                    (chain['dte'] >= target_dte_range[0]) & \n                    (chain['dte'] <= target_dte_range[1])].copy()\n        if not puts.empty and 'delta' in puts.columns:\n            puts['delta_dist'] = np.abs(puts['delta'] - put_delta)\n            contracts['long_put'] = puts.loc[puts['delta_dist'].idxmin()].to_dict()\n    \n    elif strategy_type == 'Bull Put Spread':\n        # Bull Put Spread: Short put + Long lower strike put\n        short_delta = delta_config.get('put_delta', -0.20) if delta_config else -0.20\n        spread_width = delta_config.get('spread_width', 5) if delta_config else 5\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        puts = chain[(chain['option_type'] == 'Put') & \n                    (chain['dte'] >= target_dte_range[0]) & \n                    (chain['dte'] <= target_dte_range[1])].copy()\n        \n        if not puts.empty and 'delta' in puts.columns:\n            # Short put by delta\n            puts['delta_dist'] = np.abs(puts['delta'] - short_delta)\n            short_put = puts.loc[puts['delta_dist'].idxmin()]\n            contracts['short_put'] = short_put.to_dict()\n            \n            # Long put at lower strike\n            long_strike = short_put['strike'] - spread_width\n            long_puts = puts[puts['strike'] == long_strike]\n            if not long_puts.empty:\n                contracts['long_put'] = long_puts.iloc[0].to_dict()\n    \n    elif strategy_type == 'Bull Call Spread':\n        # Bull Call Spread: Long call + Short higher strike call\n        long_delta = delta_config.get('call_delta', 0.40) if delta_config else 0.40\n        spread_width = delta_config.get('spread_width', 5) if delta_config else 5\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        calls = chain[(chain['option_type'] == 'Call') & \n                     (chain['dte'] >= target_dte_range[0]) & \n                     (chain['dte'] <= target_dte_range[1])].copy()\n        \n        if not calls.empty and 'delta' in calls.columns:\n            # Long call by delta\n            calls['delta_dist'] = np.abs(calls['delta'] - long_delta)\n            long_call = calls.loc[calls['delta_dist'].idxmin()]\n            contracts['long_call'] = long_call.to_dict()\n            \n            # Short call at higher strike\n            short_strike = long_call['strike'] + spread_width\n            short_calls = calls[calls['strike'] == short_strike]\n            if not short_calls.empty:\n                contracts['short_call'] = short_calls.iloc[0].to_dict()\n    \n    return contracts\n\n\ndef select_options_by_delta(chain: pd.DataFrame, target_delta: float, \n                            option_type: str = 'Call') -> pd.DataFrame:\n    \"\"\"\n    Select options contracts closest to target delta.\n    Uses Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    chain : pd.DataFrame\n        Options chain data with Greeks from QuantConnect\n    target_delta : float\n        Target delta value (positive for calls, negative for puts)\n    option_type : str\n        'Call' or 'Put'\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Filtered options closest to target delta\n    \"\"\"\n    if chain.empty or 'delta' not in chain.columns:\n        return pd.DataFrame()\n    \n    # Filter by option type\n    filtered = chain[chain['option_type'] == option_type].copy()\n    \n    if filtered.empty:\n        return pd.DataFrame()\n    \n    # Calculate distance from target delta\n    filtered['delta_distance'] = np.abs(filtered['delta'] - target_delta)\n    \n    # Sort by delta distance and return closest\n    return filtered.nsmallest(1, 'delta_distance')\n\n\n# Legacy alias for backward compatibility\ndef get_options_chain(ticker: str, date: datetime) -> pd.DataFrame:\n    \"\"\"Alias for fetch_options_with_greeks for backward compatibility.\"\"\"\n    return fetch_options_with_greeks(ticker, date)\n\n\nprint(\"Data fetching functions defined with QuantConnect Greeks integration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Fetch Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FETCH HISTORICAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Fetching data for {TICKER}...\")\n",
    "\n",
    "# Fetch underlying equity data\n",
    "df_asset = fetch_equity_data(TICKER, START_DATE, END_DATE)\n",
    "print(f\"  {TICKER}: {len(df_asset)} days of data\")\n",
    "\n",
    "# Fetch market proxy data\n",
    "df_market = fetch_equity_data(MARKET_PROXY, START_DATE, END_DATE)\n",
    "print(f\"  {MARKET_PROXY}: {len(df_market)} days of data\")\n",
    "\n",
    "# Fetch sector ETF data\n",
    "df_sector = fetch_equity_data(SECTOR_ETF, START_DATE, END_DATE)\n",
    "print(f\"  {SECTOR_ETF}: {len(df_sector)} days of data\")\n",
    "\n",
    "# Align all dataframes to common dates\n",
    "common_dates = df_asset.index.intersection(df_market.index).intersection(df_sector.index)\n",
    "df_asset = df_asset.loc[common_dates]\n",
    "df_market = df_market.loc[common_dates]\n",
    "df_sector = df_sector.loc[common_dates]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nData fetch completed in {elapsed:.2f} seconds\")\n",
    "print(f\"Common trading days: {len(common_dates)}\")\n",
    "print(f\"Date range: {common_dates[0]} to {common_dates[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Volatility Modeling & Comparison\n",
    "\n",
    "This section implements multiple volatility models and creates a PCA-based aggregate forecast.\n",
    "\n",
    "### 2.1 True Realized Volatility Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: VOLATILITY MODELING & COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility(prices: pd.Series, returns: pd.Series = None, \n",
    "                         method: str = 'historical', window: int = 20,\n",
    "                         annualize: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate volatility using various methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices : pd.Series\n",
    "        Price series (for range-based methods)\n",
    "    returns : pd.Series\n",
    "        Return series (for return-based methods)\n",
    "    method : str\n",
    "        Volatility calculation method\n",
    "    window : int\n",
    "        Rolling window size\n",
    "    annualize : bool\n",
    "        Whether to annualize volatility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Volatility series\n",
    "    \"\"\"\n",
    "    annualization_factor = np.sqrt(252) if annualize else 1\n",
    "    \n",
    "    if returns is None:\n",
    "        returns = prices.pct_change()\n",
    "    \n",
    "    if method == 'historical':\n",
    "        # Simple rolling standard deviation\n",
    "        vol = returns.rolling(window=window).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'ewma':\n",
    "        # Exponentially Weighted Moving Average\n",
    "        vol = returns.ewm(span=window, adjust=False).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'parkinson':\n",
    "        # Parkinson volatility (uses high-low range)\n",
    "        # Requires high/low prices in a DataFrame\n",
    "        if isinstance(prices, pd.DataFrame) and 'high' in prices.columns and 'low' in prices.columns:\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            vol = np.sqrt((log_hl ** 2).rolling(window=window).mean() / (4 * np.log(2))) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'garman_klass':\n",
    "        # Garman-Klass volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            log_co = np.log(prices['close'] / prices['open'])\n",
    "            gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "            vol = np.sqrt(gk.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'rogers_satchell':\n",
    "        # Rogers-Satchell volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hc = np.log(prices['high'] / prices['close'])\n",
    "            log_ho = np.log(prices['high'] / prices['open'])\n",
    "            log_lc = np.log(prices['low'] / prices['close'])\n",
    "            log_lo = np.log(prices['low'] / prices['open'])\n",
    "            rs = log_hc * log_ho + log_lc * log_lo\n",
    "            vol = np.sqrt(rs.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'atr':\n",
    "        # Average True Range based volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'close']):\n",
    "            prev_close = prices['close'].shift(1)\n",
    "            tr = np.maximum(\n",
    "                prices['high'] - prices['low'],\n",
    "                np.maximum(\n",
    "                    np.abs(prices['high'] - prev_close),\n",
    "                    np.abs(prices['low'] - prev_close)\n",
    "                )\n",
    "            )\n",
    "            atr = tr.rolling(window=window).mean()\n",
    "            vol = (atr / prices['close']) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "    else:\n",
    "        vol = pd.Series(np.nan, index=prices.index if hasattr(prices, 'index') else None)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "\n",
    "# Calculate true realized volatility (multiple windows)\n",
    "realized_vol = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "for window in VOL_WINDOWS:\n",
    "    realized_vol[f'rv_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Primary realized volatility (20-day is standard)\n",
    "realized_vol['rv_true'] = realized_vol['rv_20d']\n",
    "\n",
    "print(\"Realized Volatility Statistics:\")\n",
    "print(realized_vol.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Volatility Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE VOLATILITY MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate all volatility measures\n",
    "vol_models = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "# 1. Historical volatility (various windows)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'hist_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 2. EWMA volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'ewma_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='ewma',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 3. Parkinson volatility (high-low based)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'parkinson_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='parkinson',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 4. Garman-Klass volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'garman_klass_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='garman_klass',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 5. Rogers-Satchell volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'rogers_satchell_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='rogers_satchell',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 6. ATR-based volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'atr_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='atr',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Drop NaN rows\n",
    "vol_models = vol_models.dropna()\n",
    "\n",
    "print(f\"Volatility models calculated: {vol_models.shape[1]} features\")\n",
    "print(f\"Valid observations: {len(vol_models)}\")\n",
    "print(f\"\\nVolatility Model Columns:\")\n",
    "for col in vol_models.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate correlation and RMSE for each model vs true volatility\n",
    "true_vol = realized_vol['rv_true'].dropna()\n",
    "common_idx = true_vol.index.intersection(vol_models.index)\n",
    "\n",
    "model_metrics = []\n",
    "\n",
    "for col in vol_models.columns:\n",
    "    model_vol = vol_models[col].loc[common_idx]\n",
    "    true_vol_aligned = true_vol.loc[common_idx]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correlation = model_vol.corr(true_vol_aligned)\n",
    "    rmse = np.sqrt(((model_vol - true_vol_aligned) ** 2).mean())\n",
    "    mae = np.abs(model_vol - true_vol_aligned).mean()\n",
    "    \n",
    "    model_metrics.append({\n",
    "        'Model': col,\n",
    "        'Correlation': correlation,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(model_metrics).sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"Model Performance vs True Volatility (20d realized):\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualization: Model comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=FIG_SIZE_COMPARISON)\n",
    "\n",
    "# Top 5 models by correlation\n",
    "top_models = metrics_df.head(5)['Model'].tolist()\n",
    "\n",
    "# Plot 1: Time series comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol.loc[common_idx], label='True Volatility (20d)', color='black', linewidth=2)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(top_models)))\n",
    "for i, model in enumerate(top_models):\n",
    "    ax1.plot(vol_models[model].loc[common_idx], label=model, alpha=0.7, color=colors[i])\n",
    "ax1.set_title(f'{TICKER} - Volatility Models Comparison', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Correlation bar chart\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0.9 else '#f39c12' if x > 0.8 else '#e74c3c' \n",
    "          for x in metrics_df['Correlation'].head(10)]\n",
    "ax2.barh(metrics_df['Model'].head(10)[::-1], metrics_df['Correlation'].head(10)[::-1], color=colors)\n",
    "ax2.set_xlabel('Correlation with True Volatility')\n",
    "ax2.set_title('Top 10 Models by Correlation', fontsize=14)\n",
    "ax2.axvline(x=0.9, color='green', linestyle='--', label='High correlation (0.9)')\n",
    "ax2.axvline(x=0.8, color='orange', linestyle='--', label='Good correlation (0.8)')\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PCA-Based Aggregate Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA-BASED AGGREGATE VOLATILITY MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def run_pca_model(features: pd.DataFrame, n_components: int = 3, \n",
    "                  lag_periods: list = None) -> Tuple[pd.DataFrame, PCA, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Create PCA-based aggregate volatility forecast using lagged features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : pd.DataFrame\n",
    "        Volatility features\n",
    "    n_components : int\n",
    "        Number of PCA components\n",
    "    lag_periods : list\n",
    "        Lag periods to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, PCA, StandardScaler]\n",
    "        Lagged features with PCA components, fitted PCA model, fitted scaler\n",
    "    \"\"\"\n",
    "    if lag_periods is None:\n",
    "        lag_periods = [1, 2, 3, 5, 10, 20]\n",
    "    \n",
    "    # Create lagged features\n",
    "    lagged_features = features.copy()\n",
    "    \n",
    "    for col in features.columns:\n",
    "        for lag in lag_periods:\n",
    "            lagged_features[f'{col}_lag{lag}'] = features[col].shift(lag)\n",
    "    \n",
    "    # Drop NaN rows\n",
    "    lagged_features = lagged_features.dropna()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(lagged_features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    result = pd.DataFrame(\n",
    "        pca_components,\n",
    "        index=lagged_features.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Add explained variance info\n",
    "    print(f\"PCA Explained Variance Ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.1f}%)\")\n",
    "    print(f\"  Total: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "    \n",
    "    return result, pca, scaler\n",
    "\n",
    "\n",
    "# Run PCA on volatility features\n",
    "pca_result, pca_model, vol_scaler = run_pca_model(\n",
    "    features=vol_models,\n",
    "    n_components=PCA_N_COMPONENTS,\n",
    "    lag_periods=PCA_LAG_PERIODS\n",
    ")\n",
    "\n",
    "# Create aggregate volatility forecast (weighted combination of PCs)\n",
    "# Weight by explained variance ratio\n",
    "weights = pca_model.explained_variance_ratio_\n",
    "pca_result['aggregate_vol_signal'] = sum(\n",
    "    pca_result[f'PC{i+1}'] * weights[i] for i in range(PCA_N_COMPONENTS)\n",
    ")\n",
    "\n",
    "# Normalize to same scale as true volatility\n",
    "true_vol_aligned = realized_vol['rv_true'].loc[pca_result.index]\n",
    "signal_mean = pca_result['aggregate_vol_signal'].mean()\n",
    "signal_std = pca_result['aggregate_vol_signal'].std()\n",
    "true_mean = true_vol_aligned.mean()\n",
    "true_std = true_vol_aligned.std()\n",
    "\n",
    "pca_result['aggregate_vol_forecast'] = (\n",
    "    (pca_result['aggregate_vol_signal'] - signal_mean) / signal_std * true_std + true_mean\n",
    ")\n",
    "\n",
    "print(f\"\\nAggregate Model Statistics:\")\n",
    "print(f\"  Correlation with True Vol: {pca_result['aggregate_vol_forecast'].corr(true_vol_aligned):.4f}\")\n",
    "rmse = np.sqrt(((pca_result['aggregate_vol_forecast'] - true_vol_aligned) ** 2).mean())\n",
    "print(f\"  RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Aggregate Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE MODEL VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Aggregate forecast vs True volatility\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol_aligned, label='True Volatility (20d)', color='black', linewidth=2)\n",
    "ax1.plot(pca_result['aggregate_vol_forecast'], label='PCA Aggregate Forecast', \n",
    "         color='#3498db', linewidth=1.5, alpha=0.8)\n",
    "ax1.fill_between(pca_result.index, \n",
    "                 pca_result['aggregate_vol_forecast'] - rmse,\n",
    "                 pca_result['aggregate_vol_forecast'] + rmse,\n",
    "                 alpha=0.2, color='#3498db', label='\u00b1RMSE band')\n",
    "ax1.set_title(f'{TICKER} - PCA Aggregate Volatility Model vs True Volatility', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = pca_result['aggregate_vol_forecast'].corr(true_vol_aligned)\n",
    "ax1.annotate(f'Correlation: {corr:.4f}\\nRMSE: {rmse:.4f}', \n",
    "             xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "             verticalalignment='top', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot 2: Principal Components\n",
    "ax2 = axes[1]\n",
    "for i in range(PCA_N_COMPONENTS):\n",
    "    ax2.plot(pca_result[f'PC{i+1}'], label=f'PC{i+1} ({pca_model.explained_variance_ratio_[i]*100:.1f}%)',\n",
    "             alpha=0.8)\n",
    "ax2.set_title('Principal Components Over Time', fontsize=14)\n",
    "ax2.set_ylabel('Standardized Value')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Scatter plot - Forecast vs Actual\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(true_vol_aligned, pca_result['aggregate_vol_forecast'], alpha=0.5, s=10)\n",
    "min_val = min(true_vol_aligned.min(), pca_result['aggregate_vol_forecast'].min())\n",
    "max_val = max(true_vol_aligned.max(), pca_result['aggregate_vol_forecast'].max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "ax3.set_xlabel('True Volatility')\n",
    "ax3.set_ylabel('Forecast Volatility')\n",
    "ax3.set_title('Forecast vs Actual Volatility', fontsize=14)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Explanation & Maintenance\n",
    "\n",
    "### 3.1 Understanding PCA (Principal Component Analysis)\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "Principal Component Analysis is a dimensionality reduction technique that identifies patterns of correlation across multiple variables. In our volatility modeling context:\n",
    "\n",
    "1. **Multiple Volatility Measures**: We calculate many volatility estimates (historical, EWMA, Parkinson, Garman-Klass, etc.) at different time windows. These measures are often highly correlated but capture slightly different aspects of volatility.\n",
    "\n",
    "2. **Common Variance Patterns**: PCA identifies the common underlying \"factors\" that drive these volatility measures. The first principal component (PC1) captures the most common variance pattern - essentially what all volatility measures agree on.\n",
    "\n",
    "3. **Noise Reduction**: By focusing on the top principal components (which explain most variance), we filter out measurement noise and idiosyncratic differences between methods.\n",
    "\n",
    "4. **Composite Signal**: The weighted combination of principal components creates a more robust volatility forecast than any single measure.\n",
    "\n",
    "**Interpretation of Principal Components:**\n",
    "- **PC1** (largest variance): Usually represents the \"level\" of volatility - whether vol is generally high or low\n",
    "- **PC2**: Often captures the \"slope\" - whether vol is rising or falling\n",
    "- **PC3**: May capture \"curvature\" or regime-specific patterns\n",
    "\n",
    "### 3.2 Why Lag Features Improve Volatility Forecasting\n",
    "\n",
    "**Volatility Clustering**: One of the most robust findings in financial markets is that volatility exhibits strong autocorrelation - high volatility tends to be followed by high volatility, and low by low. This is captured by including lagged features.\n",
    "\n",
    "**Optimal Lag Periods:**\n",
    "- **Lag 1-3 days**: Captures immediate momentum and short-term persistence\n",
    "- **Lag 5-10 days**: Captures weekly patterns and medium-term trends\n",
    "- **Lag 20 days**: Captures monthly cycles and mean-reversion signals\n",
    "\n",
    "**Why This Works:**\n",
    "- Volatility at time t is highly correlated with volatility at t-1, t-2, etc.\n",
    "- Changes in the relationship between current and lagged volatility signal regime changes\n",
    "- The PCA captures how these lag relationships evolve over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Maintenance Strategy\n",
    "\n",
    "#### Rolling Window Retraining Schedule\n",
    "\n",
    "The PCA model should be periodically retrained to adapt to changing market conditions:\n",
    "\n",
    "| Frequency | Action | Rationale |\n",
    "|-----------|--------|----------|\n",
    "| Weekly | Monitor explained variance | Early warning of model drift |\n",
    "| Monthly | Validate on recent data | Check out-of-sample performance |\n",
    "| Quarterly | Full retraining | Adapt to structural changes |\n",
    "| After major events | Emergency recalibration | Respond to regime shifts |\n",
    "\n",
    "#### Out-of-Sample Validation Approach\n",
    "\n",
    "```python\n",
    "# Recommended validation structure:\n",
    "# - Training: 70% of historical data\n",
    "# - Validation: 15% for hyperparameter tuning\n",
    "# - Test: 15% for final evaluation\n",
    "# - Rolling: Move window forward and repeat\n",
    "```\n",
    "\n",
    "#### Drift Detection Methods\n",
    "\n",
    "1. **Explained Variance Monitoring**: If total explained variance drops significantly, component structure may be changing\n",
    "2. **Correlation Decay**: Track rolling correlation between forecast and realized volatility\n",
    "3. **RMSE Tracking**: Monitor prediction error over rolling windows\n",
    "4. **Distribution Shift**: Compare feature distributions using KS tests\n",
    "\n",
    "#### Recalibration Triggers\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| 30-day rolling correlation | < 0.7 | Alert & investigate |\n",
    "| 30-day rolling RMSE | > 1.5\u00d7 baseline | Trigger retraining |\n",
    "| Explained variance (PC1-3) | < 70% | Review feature set |\n",
    "| Consecutive days of underperformance | > 10 | Emergency review |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: MODEL MAINTENANCE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_model_drift_metrics(true_vol: pd.Series, forecast_vol: pd.Series,\n",
    "                                  window: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rolling metrics to detect model drift.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_vol : pd.Series\n",
    "        Realized volatility\n",
    "    forecast_vol : pd.Series\n",
    "        Model forecast\n",
    "    window : int\n",
    "        Rolling window for metrics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Drift metrics over time\n",
    "    \"\"\"\n",
    "    # Align series\n",
    "    common_idx = true_vol.index.intersection(forecast_vol.index)\n",
    "    true_vol = true_vol.loc[common_idx]\n",
    "    forecast_vol = forecast_vol.loc[common_idx]\n",
    "    \n",
    "    drift_metrics = pd.DataFrame(index=common_idx)\n",
    "    \n",
    "    # Rolling correlation\n",
    "    drift_metrics['rolling_corr'] = true_vol.rolling(window).corr(forecast_vol)\n",
    "    \n",
    "    # Rolling RMSE\n",
    "    errors_sq = (true_vol - forecast_vol) ** 2\n",
    "    drift_metrics['rolling_rmse'] = np.sqrt(errors_sq.rolling(window).mean())\n",
    "    \n",
    "    # Rolling MAE\n",
    "    errors_abs = np.abs(true_vol - forecast_vol)\n",
    "    drift_metrics['rolling_mae'] = errors_abs.rolling(window).mean()\n",
    "    \n",
    "    # Directional accuracy (did we predict direction of change correctly?)\n",
    "    true_direction = np.sign(true_vol.diff())\n",
    "    forecast_direction = np.sign(forecast_vol.diff())\n",
    "    correct_direction = (true_direction == forecast_direction).astype(int)\n",
    "    drift_metrics['directional_accuracy'] = correct_direction.rolling(window).mean()\n",
    "    \n",
    "    return drift_metrics.dropna()\n",
    "\n",
    "\n",
    "def check_recalibration_needed(drift_metrics: pd.DataFrame, \n",
    "                               corr_threshold: float = 0.7,\n",
    "                               rmse_multiplier: float = 1.5) -> dict:\n",
    "    \"\"\"\n",
    "    Check if model needs recalibration based on drift metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Recalibration status and recommendations\n",
    "    \"\"\"\n",
    "    latest = drift_metrics.iloc[-1]\n",
    "    baseline_rmse = drift_metrics['rolling_rmse'].median()\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    if latest['rolling_corr'] < corr_threshold:\n",
    "        alerts.append(f\"Low correlation: {latest['rolling_corr']:.3f} < {corr_threshold}\")\n",
    "    \n",
    "    if latest['rolling_rmse'] > baseline_rmse * rmse_multiplier:\n",
    "        alerts.append(f\"High RMSE: {latest['rolling_rmse']:.4f} > {baseline_rmse * rmse_multiplier:.4f}\")\n",
    "    \n",
    "    if latest['directional_accuracy'] < 0.5:\n",
    "        alerts.append(f\"Poor directional accuracy: {latest['directional_accuracy']:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'needs_recalibration': len(alerts) > 0,\n",
    "        'alerts': alerts,\n",
    "        'latest_metrics': latest.to_dict()\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate drift metrics for our model\n",
    "drift_metrics = calculate_model_drift_metrics(\n",
    "    true_vol_aligned,\n",
    "    pca_result['aggregate_vol_forecast'],\n",
    "    window=30\n",
    ")\n",
    "\n",
    "# Check recalibration status\n",
    "recal_status = check_recalibration_needed(drift_metrics)\n",
    "\n",
    "print(\"Model Health Check:\")\n",
    "print(f\"  Needs Recalibration: {recal_status['needs_recalibration']}\")\n",
    "if recal_status['alerts']:\n",
    "    print(\"  Alerts:\")\n",
    "    for alert in recal_status['alerts']:\n",
    "        print(f\"    - {alert}\")\n",
    "print(f\"\\nLatest Metrics:\")\n",
    "for metric, value in recal_status['latest_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Mean Reversion Analysis\n",
    "\n",
    "This section analyzes volatility mean reversion characteristics to inform optimal options expiration selection.\n",
    "\n",
    "### 4.1 Volatility Z-Score and Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: MEAN REVERSION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility_zscore(vol_series: pd.Series, lookback: int = 252) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate rolling z-score of volatility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vol_series : pd.Series\n",
    "        Volatility series\n",
    "    lookback : int\n",
    "        Lookback window for mean and std calculation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Z-score series\n",
    "    \"\"\"\n",
    "    rolling_mean = vol_series.rolling(window=lookback).mean()\n",
    "    rolling_std = vol_series.rolling(window=lookback).std()\n",
    "    zscore = (vol_series - rolling_mean) / rolling_std\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def classify_volatility_regime(zscore: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Classify volatility into regimes based on z-score.\n",
    "    \n",
    "    Regimes:\n",
    "    - Low: z < -1\n",
    "    - Normal: -1 <= z <= 1\n",
    "    - Elevated: 1 < z <= 2\n",
    "    - Extreme: z > 2\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        zscore < -1,\n",
    "        (zscore >= -1) & (zscore <= 1),\n",
    "        (zscore > 1) & (zscore <= 2),\n",
    "        zscore > 2\n",
    "    ]\n",
    "    choices = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    return pd.Series(np.select(conditions, choices, default='Normal'), index=zscore.index)\n",
    "\n",
    "\n",
    "# Calculate z-scores\n",
    "vol_zscore = calculate_volatility_zscore(realized_vol['rv_true'].dropna())\n",
    "vol_regime = classify_volatility_regime(vol_zscore)\n",
    "\n",
    "# Combine into analysis DataFrame\n",
    "vol_analysis = pd.DataFrame({\n",
    "    'volatility': realized_vol['rv_true'],\n",
    "    'zscore': vol_zscore,\n",
    "    'regime': vol_regime\n",
    "}).dropna()\n",
    "\n",
    "# Regime statistics\n",
    "regime_stats = vol_analysis.groupby('regime').agg({\n",
    "    'volatility': ['mean', 'std', 'count'],\n",
    "    'zscore': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Volatility Regime Statistics:\")\n",
    "print(regime_stats)\n",
    "print(f\"\\nRegime Distribution:\")\n",
    "regime_counts = vol_analysis['regime'].value_counts()\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = count / len(vol_analysis) * 100\n",
    "    print(f\"  {regime}: {count} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mean Reversion Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# MEAN REVERSION SPEED ANALYSIS\n# ============================================================================\n\ndef calculate_mean_reversion_speed(zscore: pd.Series, regime_series: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Calculate mean reversion characteristics for each volatility regime.\n    Vectorized implementation.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Mean reversion metrics by regime\n    \"\"\"\n    results = []\n    \n    for regime in ['Low', 'Normal', 'Elevated', 'Extreme']:\n        # Find regime entry points\n        regime_mask = regime_series == regime\n        regime_start = regime_mask & ~regime_mask.shift(1, fill_value=False)\n        \n        # For each entry, measure time to revert\n        times_to_05 = []  # Time to reach \u00b10.5\u03c3\n        times_to_10 = []  # Time to reach \u00b11\u03c3\n        \n        entry_indices = zscore.index[regime_start]\n        \n        for entry_idx in entry_indices:\n            entry_zscore = zscore.loc[entry_idx]\n            future_zscore = zscore.loc[entry_idx:]\n            \n            # Time to \u00b10.5\u03c3\n            mask_05 = np.abs(future_zscore) <= 0.5\n            if mask_05.any():\n                idx_positions = np.where(mask_05.values)[0]\n                if len(idx_positions) > 0:\n                    first_05 = mask_05.index[idx_positions[0]]\n                    days_to_05 = (first_05 - entry_idx).days\n                    if days_to_05 > 0:\n                        times_to_05.append(days_to_05)\n            \n            # Time to \u00b11\u03c3 (only relevant for Elevated/Extreme)\n            if regime in ['Elevated', 'Extreme']:\n                mask_10 = np.abs(future_zscore) <= 1.0\n                if mask_10.any():\n                    idx_positions_10 = np.where(mask_10.values)[0]\n                        if len(idx_positions_10) > 0:\n                            first_10 = mask_10.index[idx_positions_10[0]]\n                    days_to_10 = (first_10 - entry_idx).days\n                    if days_to_10 > 0:\n                        times_to_10.append(days_to_10)\n        \n        # Calculate half-life (approximate from exponential decay)\n        if len(times_to_05) > 0:\n            avg_time_05 = np.mean(times_to_05)\n            median_time_05 = np.median(times_to_05)\n        else:\n            avg_time_05 = np.nan\n            median_time_05 = np.nan\n            \n        if len(times_to_10) > 0:\n            avg_time_10 = np.mean(times_to_10)\n            median_time_10 = np.median(times_to_10)\n        else:\n            avg_time_10 = np.nan\n            median_time_10 = np.nan\n        \n        results.append({\n            'Regime': regime,\n            'Entry_Count': len(entry_indices),\n            'Avg_Days_to_0.5\u03c3': avg_time_05,\n            'Median_Days_to_0.5\u03c3': median_time_05,\n            'Avg_Days_to_1\u03c3': avg_time_10,\n            'Median_Days_to_1\u03c3': median_time_10,\n        })\n    \n    return pd.DataFrame(results)\n\n\n# Calculate mean reversion metrics\nreversion_metrics = calculate_mean_reversion_speed(vol_zscore.dropna(), vol_regime.dropna())\n\nprint(\"Mean Reversion Speed by Volatility Regime:\")\nprint(reversion_metrics.to_string(index=False))\n\n# Recommended DTE by regime\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECOMMENDED OPTIONS EXPIRATION BY VOLATILITY REGIME:\")\nprint(\"=\"*60)\n\ndte_recommendations = {\n    'Extreme': '7-14 DTE (quick mean reversion expected)',\n    'Elevated': '14-21 DTE (moderate reversion timeframe)',\n    'Normal': '30-45 DTE (standard theta decay window)',\n    'Low': '45-60 DTE (wait for volatility expansion)'\n}\n\nfor regime, rec in dte_recommendations.items():\n    print(f\"  {regime}: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Volatility Regime Transition Matrix (PRIORITY VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY REGIME TRANSITION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_transitions(regime_series: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate regime transition probability matrix.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        Transition probability matrix and transition count matrix\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    \n",
    "    # Create transition count matrix\n",
    "    transition_counts = pd.DataFrame(0, index=regimes, columns=regimes)\n",
    "    \n",
    "    current_regime = regime_series.iloc[:-1].values\n",
    "    next_regime = regime_series.iloc[1:].values\n",
    "    \n",
    "    for curr, nxt in zip(current_regime, next_regime):\n",
    "        if curr in regimes and nxt in regimes:\n",
    "            transition_counts.loc[curr, nxt] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    row_sums = transition_counts.sum(axis=1)\n",
    "    transition_probs = transition_counts.div(row_sums, axis=0)\n",
    "    \n",
    "    return transition_probs, transition_counts\n",
    "\n",
    "\n",
    "def calculate_avg_regime_duration(regime_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate average duration spent in each regime.\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    durations = {regime: [] for regime in regimes}\n",
    "    \n",
    "    current_regime = None\n",
    "    current_duration = 0\n",
    "    \n",
    "    for regime in regime_series.values:\n",
    "        if regime == current_regime:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            if current_regime in durations:\n",
    "                durations[current_regime].append(current_duration)\n",
    "            current_regime = regime\n",
    "            current_duration = 1\n",
    "    \n",
    "    # Add final regime\n",
    "    if current_regime in durations:\n",
    "        durations[current_regime].append(current_duration)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    duration_stats = []\n",
    "    for regime in regimes:\n",
    "        if durations[regime]:\n",
    "            duration_stats.append({\n",
    "                'Regime': regime,\n",
    "                'Avg_Duration_Days': np.mean(durations[regime]),\n",
    "                'Median_Duration_Days': np.median(durations[regime]),\n",
    "                'Max_Duration_Days': np.max(durations[regime]),\n",
    "                'Num_Episodes': len(durations[regime])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(duration_stats)\n",
    "\n",
    "\n",
    "# Calculate transitions\n",
    "transition_probs, transition_counts = calculate_regime_transitions(vol_regime.dropna())\n",
    "duration_stats = calculate_avg_regime_duration(vol_regime.dropna())\n",
    "\n",
    "print(\"Regime Transition Probabilities:\")\n",
    "print(transition_probs.round(3))\n",
    "print(\"\\nAverage Regime Duration:\")\n",
    "print(duration_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION: Regime Transition Heatmap\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Transition probability heatmap\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(transition_probs, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "            ax=ax1, vmin=0, vmax=1, linewidths=0.5,\n",
    "            cbar_kws={'label': 'Transition Probability'})\n",
    "ax1.set_title('Volatility Regime Transition Probabilities\\n(Row = Current Regime, Column = Next Regime)', \n",
    "              fontsize=14)\n",
    "ax1.set_xlabel('Next Regime')\n",
    "ax1.set_ylabel('Current Regime')\n",
    "\n",
    "# Add average transition times as annotations\n",
    "for i, regime_from in enumerate(transition_probs.index):\n",
    "    duration = duration_stats[duration_stats['Regime'] == regime_from]['Avg_Duration_Days'].values\n",
    "    if len(duration) > 0:\n",
    "        ax1.annotate(f'Avg: {duration[0]:.1f}d', \n",
    "                    xy=(-0.3, i + 0.5), xycoords='data',\n",
    "                    fontsize=9, ha='right', va='center')\n",
    "\n",
    "# Bar chart of regime durations\n",
    "ax2 = axes[1]\n",
    "colors = [get_vol_regime_color(r) for r in duration_stats['Regime']]\n",
    "bars = ax2.bar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'], color=colors, alpha=0.8)\n",
    "ax2.errorbar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'],\n",
    "             yerr=[duration_stats['Avg_Duration_Days'] - duration_stats['Median_Duration_Days'],\n",
    "                   duration_stats['Max_Duration_Days'] - duration_stats['Avg_Duration_Days']],\n",
    "             fmt='none', color='black', capsize=5)\n",
    "ax2.set_title('Average Regime Duration (with min/max range)', fontsize=14)\n",
    "ax2.set_xlabel('Volatility Regime')\n",
    "ax2.set_ylabel('Days')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count annotations\n",
    "for bar, count in zip(bars, duration_stats['Num_Episodes']):\n",
    "    ax2.annotate(f'n={count}', \n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"- High diagonal values indicate regime persistence\")\n",
    "print(\"- Off-diagonal values show transition likelihoods\")\n",
    "print(\"- Extreme volatility typically reverts to Elevated/Normal\")\n",
    "print(\"- Use this to inform position sizing and DTE selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Options Strategy Development & Delta Analysis\n",
    "\n",
    "This section implements six options strategies with systematic delta testing and performance analysis.\n",
    "\n",
    "### Strategy Suite:\n",
    "1. **Calendar Spread** - Long back month, short front month, same strike\n",
    "2. **Double Diagonal** - Calendar spread on both calls and puts, different strikes\n",
    "3. **Long Straddle** - Long ATM call + long ATM put\n",
    "4. **Long Strangle** - Long OTM call + long OTM put\n",
    "5. **Bull Put Spread** - Short put + long lower strike put\n",
    "6. **Bull Call Spread** - Long call + short higher strike call\n",
    "\n",
    "### 5.1 Strategy Implementation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: OPTIONS STRATEGY DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy configurations\n",
    "STRATEGIES = [\n",
    "    'Calendar Spread',\n",
    "    'Double Diagonal',\n",
    "    'Straddle',\n",
    "    'Strangle',\n",
    "    'Bull Put Spread',\n",
    "    'Bull Call Spread'\n",
    "]\n",
    "\n",
    "def calculate_strategy_pnl(strategy_type: str, entry_price: float, exit_price: float,\n",
    "                           contracts: int = 1, multiplier: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate P&L for a strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy_type : str\n",
    "        Type of options strategy\n",
    "    entry_price : float\n",
    "        Net debit/credit at entry\n",
    "    exit_price : float\n",
    "        Net value at exit\n",
    "    contracts : int\n",
    "        Number of contracts\n",
    "    multiplier : int\n",
    "        Contract multiplier (usually 100)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Profit/Loss in dollars\n",
    "    \"\"\"\n",
    "    return (exit_price - entry_price) * contracts * multiplier\n",
    "\n",
    "\n",
    "def apply_stop_loss(pnl_series: pd.Series, stop_threshold: float = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply stop loss to a P&L series.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pnl_series : pd.Series\n",
    "        Series of P&L values (as returns or dollar amounts)\n",
    "    stop_threshold : float\n",
    "        Stop loss threshold (e.g., -0.20 for 20% loss). None = no stop.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Adjusted P&L series with stop loss applied\n",
    "    \"\"\"\n",
    "    if stop_threshold is None:\n",
    "        return pnl_series\n",
    "    \n",
    "    # Apply stop loss - cap losses at threshold\n",
    "    adjusted = pnl_series.copy()\n",
    "    adjusted = np.where(adjusted < stop_threshold, stop_threshold, adjusted)\n",
    "    return pd.Series(adjusted, index=pnl_series.index)\n",
    "\n",
    "\n",
    "def roll_position(current_expiry: datetime, days_to_roll: int = 5) -> datetime:\n",
    "    \"\"\"\n",
    "    Calculate next expiration date for position rolling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_expiry : datetime\n",
    "        Current position expiration\n",
    "    days_to_roll : int\n",
    "        Days before expiry to trigger roll\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    datetime\n",
    "        Next expiration date (approximately 30 days out)\n",
    "    \"\"\"\n",
    "    # Roll to next monthly expiration\n",
    "    next_expiry = current_expiry + timedelta(days=30)\n",
    "    # Adjust to third Friday (standard monthly expiration)\n",
    "    # Find first day of expiry month\n",
    "    first_day = next_expiry.replace(day=1)\n",
    "    # Find first Friday\n",
    "    days_until_friday = (4 - first_day.weekday()) % 7\n",
    "    first_friday = first_day + timedelta(days=days_until_friday)\n",
    "    # Third Friday\n",
    "    third_friday = first_friday + timedelta(days=14)\n",
    "    return third_friday\n",
    "\n",
    "\n",
    "print(\"Strategy functions defined.\")\n",
    "print(f\"Strategies: {STRATEGIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Strategy Returns Using Real QuantConnect Options Data\n\nThe notebook uses **real options data from QuantConnect** instead of simulations:\n\n- **Greeks** (Delta, Gamma, Theta, Vega, Rho) are fetched directly from the QuantConnect API\n- **Implied Volatility** is obtained from QuantConnect's option pricing model\n- **Contract selection** is limited to those needed for each specific strategy\n- **Bid/Ask prices** are used for realistic P&L calculations\n\nThe `get_strategy_contracts()` function filters the options chain to only the contracts required for each strategy type, reducing data overhead and ensuring accurate Greeks from the QuantConnect API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# STRATEGY RETURNS USING REAL QUANTCONNECT OPTIONS DATA\n# ============================================================================\n\ndef calculate_real_strategy_returns(ticker: str, dates: pd.DatetimeIndex,\n                                    underlying_prices: pd.Series,\n                                    strategy: str, delta_config: dict = None,\n                                    front_dte: int = 30, back_dte: int = 60) -> pd.Series:\n    \"\"\"\n    Calculate strategy returns using real QuantConnect options data.\n    Uses Greeks and IV from QuantConnect API.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker symbol\n    dates : pd.DatetimeIndex\n        Trading dates to calculate returns\n    underlying_prices : pd.Series\n        Underlying asset prices\n    strategy : str\n        Strategy type\n    delta_config : dict\n        Delta configuration for the strategy\n    front_dte : int\n        Front month days to expiration\n    back_dte : int\n        Back month days to expiration\n        \n    Returns:\n    --------\n    pd.Series\n        Strategy daily returns based on real options data\n    \"\"\"\n    returns = pd.Series(index=dates, dtype=float)\n    \n    # Sample dates periodically to avoid API overload (every 5 trading days)\n    sample_dates = dates[::5]\n    \n    current_position = None\n    entry_value = 0\n    \n    for i, date in enumerate(sample_dates):\n        try:\n            if date not in underlying_prices.index:\n                continue\n                \n            underlying_price = underlying_prices.loc[date]\n            \n            # Get contracts for this strategy using QuantConnect Greeks\n            contracts = get_strategy_contracts(\n                ticker=ticker,\n                date=date,\n                underlying_price=underlying_price,\n                strategy_type=strategy,\n                delta_config=delta_config,\n                front_dte=front_dte,\n                back_dte=back_dte\n            )\n            \n            if not contracts:\n                continue\n            \n            # Calculate position value based on strategy type\n            position_value = 0\n            \n            if strategy == 'Calendar Spread':\n                # Long back month, short front month (same strike)\n                if 'back_call' in contracts and 'front_call' in contracts:\n                    back_price = contracts['back_call'].get('mid', contracts['back_call'].get('last', 0))\n                    front_price = contracts['front_call'].get('mid', contracts['front_call'].get('last', 0))\n                    position_value = back_price - front_price  # Net debit\n                    \n            elif strategy == 'Double Diagonal':\n                # Calendar spreads on calls and puts\n                value = 0\n                if 'back_call' in contracts and 'front_call' in contracts:\n                    value += contracts['back_call'].get('mid', 0) - contracts['front_call'].get('mid', 0)\n                if 'back_put' in contracts and 'front_put' in contracts:\n                    value += contracts['back_put'].get('mid', 0) - contracts['front_put'].get('mid', 0)\n                position_value = value\n                \n            elif strategy == 'Straddle':\n                # Long ATM call + Long ATM put\n                if 'long_call' in contracts and 'long_put' in contracts:\n                    call_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    put_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = call_price + put_price  # Net debit\n                    \n            elif strategy == 'Strangle':\n                # Long OTM call + Long OTM put\n                if 'long_call' in contracts and 'long_put' in contracts:\n                    call_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    put_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = call_price + put_price  # Net debit\n                    \n            elif strategy == 'Bull Put Spread':\n                # Short put + Long lower strike put (credit spread)\n                if 'short_put' in contracts and 'long_put' in contracts:\n                    short_price = contracts['short_put'].get('mid', contracts['short_put'].get('last', 0))\n                    long_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = short_price - long_price  # Net credit (negative debit)\n                    \n            elif strategy == 'Bull Call Spread':\n                # Long call + Short higher strike call (debit spread)\n                if 'long_call' in contracts and 'short_call' in contracts:\n                    long_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    short_price = contracts['short_call'].get('mid', contracts['short_call'].get('last', 0))\n                    position_value = long_price - short_price  # Net debit\n            \n            # Calculate return if we have a previous position\n            if current_position is not None and entry_value != 0:\n                pnl = position_value - entry_value\n                daily_return = pnl / abs(entry_value) if entry_value != 0 else 0\n                \n                # Spread return across days since last sample\n                if i > 0:\n                    prev_date = sample_dates[i-1]\n                    days_between = (date - prev_date).days\n                    if days_between > 0:\n                        daily_return_spread = daily_return / days_between\n                        date_range = pd.date_range(prev_date, date, freq='D')[1:]\n                        for d in date_range:\n                            if d in returns.index:\n                                returns.loc[d] = daily_return_spread\n            \n            # Update position\n            current_position = contracts\n            entry_value = position_value\n            \n        except Exception as e:\n            continue\n    \n    return returns.fillna(0)\n\n\ndef calculate_strategy_pnl_from_greeks(contracts: dict, underlying_change: float,\n                                       days_passed: int = 1, vol_change: float = 0) -> float:\n    \"\"\"\n    Calculate strategy P&L using Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    contracts : dict\n        Dictionary of contract data with Greeks from QuantConnect\n    underlying_change : float\n        Change in underlying price\n    days_passed : int\n        Number of days passed (for theta)\n    vol_change : float\n        Change in implied volatility\n        \n    Returns:\n    --------\n    float\n        Estimated P&L based on Greeks\n    \"\"\"\n    total_pnl = 0\n    \n    for leg_name, contract in contracts.items():\n        if not isinstance(contract, dict):\n            continue\n            \n        # Get Greeks from QuantConnect API data\n        delta = contract.get('delta', 0) or 0\n        gamma = contract.get('gamma', 0) or 0\n        theta = contract.get('theta', 0) or 0\n        vega = contract.get('vega', 0) or 0\n        \n        # Determine position sign (long or short)\n        is_short = 'short' in leg_name.lower() or 'front' in leg_name.lower()\n        position_sign = -1 if is_short else 1\n        \n        # Calculate P&L components using QuantConnect Greeks\n        delta_pnl = delta * underlying_change * position_sign\n        gamma_pnl = 0.5 * gamma * (underlying_change ** 2) * position_sign\n        theta_pnl = theta * days_passed * position_sign\n        vega_pnl = vega * vol_change * position_sign\n        \n        leg_pnl = delta_pnl + gamma_pnl + theta_pnl + vega_pnl\n        total_pnl += leg_pnl\n    \n    return total_pnl\n\n\n# Generate returns for all strategies using real options data\n# Note: For full historical backtest, this would query QuantConnect for each date\n# For research purposes, we use a hybrid approach\n\nprint(\"Generating strategy returns using real QuantConnect options data...\")\nprint(\"Note: Full historical backtest requires options data access in QuantConnect Research environment\")\n\nstrategy_returns = {}\n\n# For demonstration, calculate returns using available data\n# In production, this would iterate through historical dates\nfor strategy in STRATEGIES:\n    print(f\"  Processing {strategy}...\")\n    \n    # Use real options data approach\n    # This calls QuantConnect API for Greeks and contract selection\n    returns = calculate_real_strategy_returns(\n        ticker=TICKER,\n        dates=df_asset.index,\n        underlying_prices=df_asset['close'],\n        strategy=strategy,\n        delta_config=None,\n        front_dte=30,\n        back_dte=60\n    )\n    \n    strategy_returns[strategy] = returns\n\n# Create DataFrame\nstrategy_returns_df = pd.DataFrame(strategy_returns)\n\nprint(f\"\\nStrategy returns generated: {len(strategy_returns_df)} days\")\nprint(\"\\nStrategy Return Statistics:\")\nprint(strategy_returns_df.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B. Delta Selection Analysis\n\nSystematic testing of multiple delta configurations for each strategy type.\n\n**Delta Configurations to Test:**\n| Strategy | Delta Configurations |\n|----------|---------------------|\n| Calendar Spreads | 0.50 (ATM), 0.40, 0.30, 0.20 |\n| Double Diagonals | (0.30, -0.30), (0.25, -0.25), (0.20, -0.20) |\n| Straddles | ATM (~0.50 for calls) |\n| Strangles | (0.30, -0.30), (0.25, -0.25), (0.20, -0.20), (0.16, -0.16) |\n| Bull Put Spreads | Short deltas: -0.30, -0.20, -0.16, -0.10 (5-10 pt width) |\n| Bull Call Spreads | Long deltas: 0.50, 0.40, 0.30 (5-10 pt width) |\n\n**Analysis Outputs:**\n- Performance metrics for each strategy \u00d7 delta configuration\n- Heatmap visualization of returns by strategy and delta\n- Optimal delta identification per strategy\n- How optimal delta changes across volatility regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5B: DELTA SELECTION ANALYSIS\n# ============================================================================\n\n# Define delta configurations for systematic testing\nDELTA_CONFIGS = {\n    'Calendar Spread': [\n        {'call_delta': 0.50, 'name': 'ATM (0.50)'},\n        {'call_delta': 0.40, 'name': 'Delta 0.40'},\n        {'call_delta': 0.30, 'name': 'Delta 0.30'},\n        {'call_delta': 0.20, 'name': 'Delta 0.20'},\n    ],\n    'Double Diagonal': [\n        {'call_delta': 0.30, 'put_delta': -0.30, 'name': '(0.30, -0.30)'},\n        {'call_delta': 0.25, 'put_delta': -0.25, 'name': '(0.25, -0.25)'},\n        {'call_delta': 0.20, 'put_delta': -0.20, 'name': '(0.20, -0.20)'},\n    ],\n    'Straddle': [\n        {'call_delta': 0.50, 'name': 'ATM (0.50)'},\n    ],\n    'Strangle': [\n        {'call_delta': 0.30, 'put_delta': -0.30, 'name': '(0.30, -0.30)'},\n        {'call_delta': 0.25, 'put_delta': -0.25, 'name': '(0.25, -0.25)'},\n        {'call_delta': 0.20, 'put_delta': -0.20, 'name': '(0.20, -0.20)'},\n        {'call_delta': 0.16, 'put_delta': -0.16, 'name': '(0.16, -0.16)'},\n    ],\n    'Bull Put Spread': [\n        {'put_delta': -0.30, 'spread_width': 5, 'name': 'Delta -0.30 (5pt)'},\n        {'put_delta': -0.30, 'spread_width': 10, 'name': 'Delta -0.30 (10pt)'},\n        {'put_delta': -0.20, 'spread_width': 5, 'name': 'Delta -0.20 (5pt)'},\n        {'put_delta': -0.20, 'spread_width': 10, 'name': 'Delta -0.20 (10pt)'},\n        {'put_delta': -0.16, 'spread_width': 5, 'name': 'Delta -0.16 (5pt)'},\n        {'put_delta': -0.10, 'spread_width': 5, 'name': 'Delta -0.10 (5pt)'},\n    ],\n    'Bull Call Spread': [\n        {'call_delta': 0.50, 'spread_width': 5, 'name': 'Delta 0.50 (5pt)'},\n        {'call_delta': 0.50, 'spread_width': 10, 'name': 'Delta 0.50 (10pt)'},\n        {'call_delta': 0.40, 'spread_width': 5, 'name': 'Delta 0.40 (5pt)'},\n        {'call_delta': 0.40, 'spread_width': 10, 'name': 'Delta 0.40 (10pt)'},\n        {'call_delta': 0.30, 'spread_width': 5, 'name': 'Delta 0.30 (5pt)'},\n    ],\n}\n\n\ndef backtest_strategy_with_real_data(ticker: str, strategy_type: str, \n                                      delta_config: dict, dates: pd.DatetimeIndex,\n                                      underlying_prices: pd.Series,\n                                      vol_regime: pd.Series = None,\n                                      include_transaction_costs: bool = True) -> dict:\n    \"\"\"\n    Backtest a strategy using real QuantConnect options data.\n    Uses Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    strategy_type : str\n        Type of options strategy\n    delta_config : dict\n        Delta configuration for the strategy\n    dates : pd.DatetimeIndex\n        Trading dates\n    underlying_prices : pd.Series\n        Underlying asset prices\n    vol_regime : pd.Series\n        Volatility regime classification\n    include_transaction_costs : bool\n        Include bid/ask spread in P&L\n        \n    Returns:\n    --------\n    dict\n        Performance metrics\n    \"\"\"\n    returns = []\n    regime_returns = {}  # Track returns by regime\n    \n    # Sample dates for backtest (every 5 trading days to avoid API overload)\n    sample_dates = dates[::5]\n    \n    current_position = None\n    entry_value = 0\n    entry_regime = None\n    \n    for i, date in enumerate(sample_dates):\n        try:\n            if date not in underlying_prices.index:\n                continue\n            \n            underlying_price = underlying_prices.loc[date]\n            \n            # Get contracts for this strategy using QuantConnect Greeks\n            contracts = get_strategy_contracts(\n                ticker=ticker,\n                date=date,\n                underlying_price=underlying_price,\n                strategy_type=strategy_type,\n                delta_config=delta_config\n            )\n            \n            if not contracts:\n                continue\n            \n            # Calculate position value\n            position_value = calculate_position_value(contracts, strategy_type, \n                                                       include_transaction_costs)\n            \n            # Calculate return if we have a previous position\n            if current_position is not None and entry_value != 0:\n                pnl = position_value - entry_value\n                daily_return = pnl / abs(entry_value) if entry_value != 0 else 0\n                returns.append({'date': date, 'return': daily_return, 'regime': entry_regime})\n                \n                # Track by regime\n                if entry_regime:\n                    if entry_regime not in regime_returns:\n                        regime_returns[entry_regime] = []\n                    regime_returns[entry_regime].append(daily_return)\n            \n            # Update position\n            current_position = contracts\n            entry_value = position_value\n            entry_regime = vol_regime.loc[date] if vol_regime is not None and date in vol_regime.index else None\n            \n        except Exception as e:\n            continue\n    \n    # Calculate metrics\n    if not returns:\n        return None\n    \n    returns_df = pd.DataFrame(returns)\n    returns_series = returns_df['return']\n    \n    total_return = (1 + returns_series).prod() - 1\n    n_periods = len(returns_series)\n    annual_factor = 252 / (n_periods * 5) if n_periods > 0 else 1  # Adjust for sampling\n    annual_return = (1 + total_return) ** annual_factor - 1 if n_periods > 0 else 0\n    volatility_ann = returns_series.std() * np.sqrt(252 / 5)\n    sharpe = annual_return / volatility_ann if volatility_ann > 0 else 0\n    \n    # Drawdown\n    cumulative = (1 + returns_series).cumprod()\n    running_max = cumulative.expanding().max()\n    drawdown = (cumulative - running_max) / running_max\n    max_drawdown = drawdown.min() if len(drawdown) > 0 else 0\n    \n    # Win rate\n    win_rate = (returns_series > 0).mean()\n    \n    # Regime-specific metrics\n    regime_metrics = {}\n    for regime, regime_rets in regime_returns.items():\n        if len(regime_rets) > 0:\n            regime_metrics[regime] = {\n                'avg_return': np.mean(regime_rets),\n                'win_rate': sum(1 for r in regime_rets if r > 0) / len(regime_rets),\n                'count': len(regime_rets)\n            }\n    \n    return {\n        'strategy': strategy_type,\n        'delta_config': delta_config.get('name', str(delta_config)),\n        'total_return': total_return,\n        'annual_return': annual_return,\n        'volatility': volatility_ann,\n        'sharpe': sharpe,\n        'max_drawdown': max_drawdown,\n        'win_rate': win_rate,\n        'n_trades': n_periods,\n        'regime_metrics': regime_metrics\n    }\n\n\ndef calculate_position_value(contracts: dict, strategy_type: str,\n                             include_transaction_costs: bool = True) -> float:\n    \"\"\"\n    Calculate position value from QuantConnect option contracts.\n    \n    Uses bid/ask spread for transaction cost estimation.\n    \"\"\"\n    value = 0\n    \n    for leg_name, contract in contracts.items():\n        if not isinstance(contract, dict):\n            continue\n        \n        # Use mid price, adjusted for transaction costs\n        bid = contract.get('bid', 0) or 0\n        ask = contract.get('ask', 0) or 0\n        mid = contract.get('mid', 0) or ((bid + ask) / 2 if bid and ask else 0)\n        \n        if include_transaction_costs:\n            # For buys, use ask; for sells, use bid\n            is_short = 'short' in leg_name.lower() or 'front' in leg_name.lower()\n            price = bid if is_short else ask\n        else:\n            price = mid\n        \n        # Determine sign based on leg type\n        if 'short' in leg_name.lower() or 'front' in leg_name.lower():\n            value -= price  # Credit for short positions\n        else:\n            value += price  # Debit for long positions\n    \n    return value\n\n\n# Run systematic delta testing for all strategies\nprint(\"Running systematic delta configuration testing...\")\nprint(\"=\" * 60)\n\ndelta_test_results = []\n\nfor strategy in STRATEGIES:\n    print(f\"\\nTesting {strategy}...\")\n    \n    configs = DELTA_CONFIGS.get(strategy, [{'name': 'default'}])\n    \n    for config in configs:\n        result = backtest_strategy_with_real_data(\n            ticker=TICKER,\n            strategy_type=strategy,\n            delta_config=config,\n            dates=df_asset.index,\n            underlying_prices=df_asset['close'],\n            vol_regime=vol_regime if 'vol_regime' in dir() else None,\n            include_transaction_costs=True\n        )\n        \n        if result:\n            delta_test_results.append(result)\n            print(f\"  {config.get('name', 'default')}: Sharpe={result['sharpe']:.2f}, \"\n                  f\"Return={result['total_return']:.2%}\")\n\n# Create results DataFrame\ndelta_results_df = pd.DataFrame(delta_test_results)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Delta Configuration Test Results Summary:\")\nprint(delta_results_df[['strategy', 'delta_config', 'sharpe', 'total_return', \n                        'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B.1 Delta Configuration Heatmap\n\nVisualization showing returns by strategy \u00d7 delta configuration to identify optimal deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# DELTA CONFIGURATION HEATMAP VISUALIZATION\n# ============================================================================\n\ndef plot_delta_heatmap(results_df: pd.DataFrame):\n    \"\"\"\n    Create heatmap showing returns by strategy and delta configuration.\n    \"\"\"\n    if results_df.empty:\n        print(\"No results to plot\")\n        return\n    \n    # Pivot for heatmap\n    pivot = results_df.pivot_table(\n        index='strategy',\n        columns='delta_config',\n        values='sharpe',\n        aggfunc='mean'\n    )\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Create heatmap with diverging colormap\n    vmax = max(abs(pivot.values.min()), abs(pivot.values.max())) if pivot.values.size > 0 else 1\n    \n    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n                ax=ax, linewidths=0.5, cbar_kws={'label': 'Sharpe Ratio'})\n    \n    ax.set_title(f'{TICKER} - Strategy Performance by Delta Configuration', fontsize=14)\n    ax.set_xlabel('Delta Configuration')\n    ax.set_ylabel('Strategy')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Identify optimal delta for each strategy\n    print(\"\\nOptimal Delta Configuration by Strategy:\")\n    print(\"-\" * 50)\n    for strategy in pivot.index:\n        row = pivot.loc[strategy].dropna()\n        if len(row) > 0:\n            best_delta = row.idxmax()\n            best_sharpe = row.max()\n            print(f\"  {strategy}: {best_delta} (Sharpe: {best_sharpe:.2f})\")\n\n\n# Plot delta configuration heatmap\nif len(delta_results_df) > 0:\n    plot_delta_heatmap(delta_results_df)\nelse:\n    print(\"No delta test results available for heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B.2 Optimal Delta by Volatility Regime\n\nAnalysis of how optimal delta changes across different volatility regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# OPTIMAL DELTA BY VOLATILITY REGIME\n# ============================================================================\n\ndef analyze_delta_by_regime(results_df: pd.DataFrame):\n    \"\"\"\n    Analyze how optimal delta changes across volatility regimes.\n    \"\"\"\n    regime_delta_analysis = []\n    \n    for _, row in results_df.iterrows():\n        strategy = row['strategy']\n        delta_config = row['delta_config']\n        regime_metrics = row.get('regime_metrics', {})\n        \n        if regime_metrics:\n            for regime, metrics in regime_metrics.items():\n                regime_delta_analysis.append({\n                    'strategy': strategy,\n                    'delta_config': delta_config,\n                    'regime': regime,\n                    'avg_return': metrics['avg_return'],\n                    'win_rate': metrics['win_rate'],\n                    'count': metrics['count']\n                })\n    \n    if not regime_delta_analysis:\n        print(\"No regime-specific data available\")\n        return pd.DataFrame()\n    \n    regime_df = pd.DataFrame(regime_delta_analysis)\n    \n    # For each strategy and regime, find optimal delta\n    print(\"Optimal Delta Configuration by Strategy and Regime:\")\n    print(\"=\" * 70)\n    \n    for strategy in regime_df['strategy'].unique():\n        print(f\"\\n{strategy}:\")\n        strategy_data = regime_df[regime_df['strategy'] == strategy]\n        \n        for regime in ['Low', 'Normal', 'Elevated', 'Extreme']:\n            regime_data = strategy_data[strategy_data['regime'] == regime]\n            if len(regime_data) > 0 and regime_data['count'].sum() >= 5:\n                # Find best delta by avg return\n                best_idx = regime_data['avg_return'].idxmax()\n                best_row = regime_data.loc[best_idx]\n                print(f\"  {regime}: {best_row['delta_config']} \"\n                      f\"(Avg Return: {best_row['avg_return']:.4f}, \"\n                      f\"Win Rate: {best_row['win_rate']:.0%}, \"\n                      f\"n={best_row['count']})\")\n    \n    return regime_df\n\n\n# Analyze delta by regime\nif len(delta_results_df) > 0:\n    regime_delta_df = analyze_delta_by_regime(delta_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5C. Strategy Execution Rules\n\n**Entry Rules:**\n- Use aggregate volatility model signals for timing\n- Use ensemble regime identification for strategy selection\n- Enter when regime confidence is above threshold (default: 70%)\n\n**Rolling Mechanism:**\n- At expiration (or 5 DTE), automatically roll positions forward\n- Maintain same delta target for rolled positions\n- Track roll costs/credits separately\n- Continue rolling to evaluate long-term regime-based performance\n\n**Position Sizing:**\n- Consistent notional sizing across strategies for fair comparison\n- Default: $10,000 notional per position\n- Adjust for delta-weighted exposure\n\n**Transaction Costs:**\n- Include bid/ask spread in all P&L calculations\n- Use mid price for mark-to-market, execution price for actual trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5C: STRATEGY EXECUTION RULES\n# ============================================================================\n\n# Execution parameters\nEXECUTION_CONFIG = {\n    'notional_size': 10000,           # Consistent notional per trade\n    'min_regime_confidence': 0.70,    # Minimum confidence to trade\n    'days_to_roll': 5,                # Days before expiry to roll\n    'include_transaction_costs': True,\n    'slippage_pct': 0.001,            # Additional slippage estimate\n}\n\n\ndef roll_position(current_contracts: dict, new_date: datetime, \n                  underlying_price: float, strategy_type: str,\n                  delta_config: dict) -> Tuple[dict, float]:\n    \"\"\"\n    Roll positions forward to next available expiration.\n    Maintains same delta target.\n    \n    Parameters:\n    -----------\n    current_contracts : dict\n        Current position contracts\n    new_date : datetime\n        Roll date\n    underlying_price : float\n        Current underlying price\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta configuration to maintain\n        \n    Returns:\n    --------\n    Tuple[dict, float]\n        New contracts and roll cost/credit\n    \"\"\"\n    # Close current position\n    close_value = calculate_position_value(\n        current_contracts, strategy_type, \n        include_transaction_costs=True\n    )\n    \n    # Open new position at same delta\n    new_contracts = get_strategy_contracts(\n        ticker=TICKER,\n        date=new_date,\n        underlying_price=underlying_price,\n        strategy_type=strategy_type,\n        delta_config=delta_config,\n        front_dte=30,\n        back_dte=60\n    )\n    \n    open_value = calculate_position_value(\n        new_contracts, strategy_type,\n        include_transaction_costs=True\n    )\n    \n    roll_cost = open_value - close_value\n    \n    return new_contracts, roll_cost\n\n\ndef execute_strategy_with_rolls(ticker: str, strategy_type: str,\n                                 delta_config: dict, dates: pd.DatetimeIndex,\n                                 underlying_prices: pd.Series,\n                                 vol_regime: pd.Series = None,\n                                 regime_filter: str = None) -> pd.DataFrame:\n    \"\"\"\n    Execute strategy with automatic position rolling.\n    Vectorized where possible.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta configuration\n    dates : pd.DatetimeIndex\n        Trading dates\n    underlying_prices : pd.Series\n        Underlying prices\n    vol_regime : pd.Series\n        Volatility regime classification\n    regime_filter : str\n        Only trade in this regime (optional)\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Trade log with dates, returns, costs, regimes\n    \"\"\"\n    trades = []\n    current_contracts = None\n    entry_date = None\n    entry_value = 0\n    total_roll_costs = 0\n    \n    sample_dates = dates[::5]  # Sample every 5 trading days\n    \n    for date in sample_dates:\n        try:\n            if date not in underlying_prices.index:\n                continue\n            \n            underlying_price = underlying_prices.loc[date]\n            current_regime = vol_regime.loc[date] if vol_regime is not None and date in vol_regime.index else None\n            \n            # Check regime filter\n            if regime_filter and current_regime != regime_filter:\n                # Close position if we have one and regime changes\n                if current_contracts is not None:\n                    exit_value = calculate_position_value(current_contracts, strategy_type, True)\n                    pnl = exit_value - entry_value\n                    trades.append({\n                        'entry_date': entry_date,\n                        'exit_date': date,\n                        'pnl': pnl,\n                        'return': pnl / abs(entry_value) if entry_value != 0 else 0,\n                        'roll_costs': total_roll_costs,\n                        'regime': current_regime\n                    })\n                    current_contracts = None\n                    total_roll_costs = 0\n                continue\n            \n            # Open new position if needed\n            if current_contracts is None:\n                current_contracts = get_strategy_contracts(\n                    ticker=ticker,\n                    date=date,\n                    underlying_price=underlying_price,\n                    strategy_type=strategy_type,\n                    delta_config=delta_config\n                )\n                if current_contracts:\n                    entry_date = date\n                    entry_value = calculate_position_value(current_contracts, strategy_type, True)\n            else:\n                # Check if we need to roll (simplified: roll every 30 days)\n                days_held = (date - entry_date).days if entry_date else 0\n                if days_held >= 25:  # Roll around 25 days\n                    new_contracts, roll_cost = roll_position(\n                        current_contracts, date, underlying_price,\n                        strategy_type, delta_config\n                    )\n                    if new_contracts:\n                        total_roll_costs += roll_cost\n                        current_contracts = new_contracts\n                        entry_date = date\n                        entry_value = calculate_position_value(current_contracts, strategy_type, True)\n            \n        except Exception as e:\n            continue\n    \n    return pd.DataFrame(trades)\n\n\nprint(\"Strategy execution functions with rolling defined.\")\nprint(f\"Execution config: {EXECUTION_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5D. Cross-Regime Performance Analysis\n\n**CRITICAL:** Trade ALL strategies across ALL regime combinations and analyze performance.\n\nEach day/trade is tagged with:\n- Market regime (from ensemble clustering)\n- Sector regime (from ensemble clustering)\n- Asset regime (from ensemble clustering)\n- Volatility regime (Low/Normal/Elevated/Extreme)\n\nPerformance metrics calculated per regime:\n- Average return per regime\n- Win rate per regime\n- Sharpe ratio per regime\n- Maximum drawdown per regime\n- Average days to profit per regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5D: CROSS-REGIME PERFORMANCE ANALYSIS\n# ============================================================================\n\ndef run_cross_regime_analysis(ticker: str, strategies: list,\n                               dates: pd.DatetimeIndex,\n                               underlying_prices: pd.Series,\n                               market_regimes: pd.Series = None,\n                               sector_regimes: pd.Series = None,\n                               asset_regimes: pd.Series = None,\n                               vol_regimes: pd.Series = None) -> pd.DataFrame:\n    \"\"\"\n    Run all strategies continuously and tag with all regime types.\n    Vectorized analysis across entire historical period.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Complete performance data with regime tags\n    \"\"\"\n    all_results = []\n    \n    for strategy in strategies:\n        print(f\"Running {strategy} across all regimes...\")\n        \n        # Get default delta config\n        configs = DELTA_CONFIGS.get(strategy, [{'name': 'default'}])\n        default_config = configs[0] if configs else {'name': 'default'}\n        \n        # Calculate returns using real options data\n        returns = calculate_real_strategy_returns(\n            ticker=ticker,\n            dates=dates,\n            underlying_prices=underlying_prices,\n            strategy=strategy,\n            delta_config=default_config\n        )\n        \n        # Tag each return with regime information\n        for date in returns.index:\n            if returns.loc[date] == 0:\n                continue\n            \n            regime_tags = {\n                'date': date,\n                'strategy': strategy,\n                'return': returns.loc[date],\n            }\n            \n            # Add regime tags\n            if market_regimes is not None and date in market_regimes.index:\n                regime_tags['market_regime'] = market_regimes.loc[date]\n            if sector_regimes is not None and date in sector_regimes.index:\n                regime_tags['sector_regime'] = sector_regimes.loc[date]\n            if asset_regimes is not None and date in asset_regimes.index:\n                regime_tags['asset_regime'] = asset_regimes.loc[date]\n            if vol_regimes is not None and date in vol_regimes.index:\n                regime_tags['vol_regime'] = vol_regimes.loc[date]\n            \n            all_results.append(regime_tags)\n    \n    return pd.DataFrame(all_results)\n\n\ndef calculate_regime_specific_metrics(results_df: pd.DataFrame, \n                                       regime_col: str) -> pd.DataFrame:\n    \"\"\"\n    Calculate performance metrics grouped by strategy and regime.\n    \"\"\"\n    metrics = []\n    \n    for (strategy, regime), group in results_df.groupby(['strategy', regime_col]):\n        returns = group['return']\n        \n        if len(returns) < 5:\n            continue\n        \n        # Calculate metrics\n        total_return = (1 + returns).prod() - 1\n        avg_return = returns.mean()\n        win_rate = (returns > 0).mean()\n        \n        # Sharpe (annualized)\n        vol = returns.std() * np.sqrt(252 / 5)  # Adjust for sampling\n        sharpe = (avg_return * 252 / 5) / vol if vol > 0 else 0\n        \n        # Drawdown\n        cumulative = (1 + returns).cumprod()\n        running_max = cumulative.expanding().max()\n        max_dd = ((cumulative - running_max) / running_max).min()\n        \n        # Days to profit (simplified)\n        profitable_trades = returns[returns > 0]\n        avg_days_to_profit = 5 if len(profitable_trades) > 0 else np.nan\n        \n        metrics.append({\n            'strategy': strategy,\n            'regime': regime,\n            'n_trades': len(returns),\n            'total_return': total_return,\n            'avg_return': avg_return,\n            'win_rate': win_rate,\n            'sharpe': sharpe,\n            'max_drawdown': max_dd,\n            'avg_days_to_profit': avg_days_to_profit\n        })\n    \n    return pd.DataFrame(metrics)\n\n\n# Run cross-regime analysis\nprint(\"Running cross-regime performance analysis...\")\nprint(\"=\" * 60)\n\ncross_regime_results = run_cross_regime_analysis(\n    ticker=TICKER,\n    strategies=STRATEGIES,\n    dates=df_asset.index,\n    underlying_prices=df_asset['close'],\n    market_regimes=market_regimes if 'market_regimes' in dir() else None,\n    sector_regimes=sector_regimes if 'sector_regimes' in dir() else None,\n    asset_regimes=asset_regimes if 'asset_regimes' in dir() else None,\n    vol_regimes=vol_regime if 'vol_regime' in dir() else None\n)\n\nprint(f\"\\nTotal trade observations: {len(cross_regime_results)}\")\n\n# Calculate metrics by volatility regime\nif 'vol_regime' in cross_regime_results.columns:\n    vol_metrics = calculate_regime_specific_metrics(cross_regime_results, 'vol_regime')\n    print(\"\\nPerformance by Volatility Regime:\")\n    print(vol_metrics.to_string(index=False))\n\n# Calculate metrics by market regime\nif 'market_regime' in cross_regime_results.columns:\n    market_metrics = calculate_regime_specific_metrics(cross_regime_results, 'market_regime')\n    print(\"\\nPerformance by Market Regime:\")\n    print(market_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 PRIORITY VISUALIZATION #1: Strategy Equity Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #1: Strategy Equity Curves\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate cumulative returns for each strategy\n",
    "cumulative_returns = (1 + strategy_returns_df).cumprod()\n",
    "\n",
    "# Calculate buy-and-hold benchmark\n",
    "benchmark_returns = df_asset['returns'].loc[strategy_returns_df.index]\n",
    "benchmark_cumulative = (1 + benchmark_returns.fillna(0)).cumprod()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot each strategy with consistent colors\n",
    "for strategy in STRATEGIES:\n",
    "    color = get_strategy_color(strategy)\n",
    "    ax.plot(cumulative_returns[strategy], label=strategy, color=color, linewidth=1.5)\n",
    "\n",
    "# Plot benchmark\n",
    "ax.plot(benchmark_cumulative, label='Buy & Hold', color='gray', \n",
    "        linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Add regime shading\n",
    "if len(vol_regime.dropna()) > 0:\n",
    "    regime_aligned = vol_regime.reindex(cumulative_returns.index, method='ffill')\n",
    "    \n",
    "    # Shade by regime\n",
    "    prev_regime = None\n",
    "    regime_start = cumulative_returns.index[0]\n",
    "    \n",
    "    for idx, regime in regime_aligned.items():\n",
    "        if regime != prev_regime and prev_regime is not None:\n",
    "            color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, idx, alpha=0.1, color=color)\n",
    "            regime_start = idx\n",
    "        prev_regime = regime\n",
    "    \n",
    "    # Final regime\n",
    "    if prev_regime:\n",
    "        color = get_vol_regime_color(prev_regime)\n",
    "        ax.axvspan(regime_start, cumulative_returns.index[-1], alpha=0.1, color=color)\n",
    "\n",
    "ax.set_title(f'{TICKER} - Strategy Equity Curves Comparison', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return (1 = Starting Value)')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=1, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add regime legend\n",
    "regime_patches = [mpatches.Patch(color=get_vol_regime_color(r), alpha=0.3, label=r) \n",
    "                  for r in ['Low', 'Normal', 'Elevated', 'Extreme']]\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0] + regime_patches, \n",
    "          loc='upper left', fontsize=9, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final returns\n",
    "print(\"\\nFinal Cumulative Returns:\")\n",
    "for strategy in STRATEGIES:\n",
    "    final_return = cumulative_returns[strategy].iloc[-1] - 1\n",
    "    print(f\"  {strategy}: {final_return:.2%}\")\n",
    "print(f\"  Buy & Hold: {benchmark_cumulative.iloc[-1] - 1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio (Individual Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_rolling_sharpe(returns: pd.Series, window: int = 60) -> pd.Series:\n",
    "    \"\"\"Calculate rolling Sharpe ratio.\"\"\"\n",
    "    rolling_mean = returns.rolling(window=window).mean() * 252\n",
    "    rolling_std = returns.rolling(window=window).std() * np.sqrt(252)\n",
    "    return rolling_mean / rolling_std\n",
    "\n",
    "# Create separate plots for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Calculate rolling Sharpe\n",
    "    rolling_sharpe = calculate_rolling_sharpe(strategy_returns_df[strategy], ROLLING_SHARPE_WINDOW)\n",
    "    \n",
    "    # Plot smoothed line\n",
    "    smoothed = rolling_sharpe.rolling(10).mean()\n",
    "    ax.plot(smoothed, color=color, linewidth=2, label=f'{strategy} (60-day rolling)')\n",
    "    \n",
    "    # Add confidence bands (\u00b11 std)\n",
    "    rolling_std = rolling_sharpe.rolling(20).std()\n",
    "    ax.fill_between(smoothed.index, \n",
    "                    smoothed - rolling_std, \n",
    "                    smoothed + rolling_std,\n",
    "                    alpha=0.2, color=color)\n",
    "    \n",
    "    # Add regime background shading\n",
    "    regime_aligned = vol_regime.reindex(rolling_sharpe.index, method='ffill')\n",
    "    \n",
    "    prev_regime = None\n",
    "    regime_start = rolling_sharpe.dropna().index[0] if len(rolling_sharpe.dropna()) > 0 else None\n",
    "    \n",
    "    if regime_start is not None:\n",
    "        for idx in rolling_sharpe.dropna().index:\n",
    "            regime = regime_aligned.get(idx)\n",
    "            if regime != prev_regime and prev_regime is not None:\n",
    "                regime_color = get_vol_regime_color(prev_regime)\n",
    "                ax.axvspan(regime_start, idx, alpha=0.15, color=regime_color)\n",
    "                regime_start = idx\n",
    "            prev_regime = regime\n",
    "        \n",
    "        # Final segment\n",
    "        if prev_regime:\n",
    "            regime_color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, rolling_sharpe.dropna().index[-1], alpha=0.15, color=regime_color)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(y=1, color='green', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = 1')\n",
    "    ax.axhline(y=-1, color='red', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = -1')\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Rolling Sharpe Ratio ({ROLLING_SHARPE_WINDOW}-day window)', \n",
    "                 fontsize=14, color=color)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sharpe Ratio')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overall Sharpe annotation\n",
    "    overall_sharpe = (strategy_returns_df[strategy].mean() * 252) / (strategy_returns_df[strategy].std() * np.sqrt(252))\n",
    "    ax.annotate(f'Overall Sharpe: {overall_sharpe:.2f}', \n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Stop Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_strategy_parameters(strategy: str, underlying_returns: pd.Series,\n",
    "                                 volatility: pd.Series, vol_regime: pd.Series,\n",
    "                                 stop_loss_thresholds: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Test multiple stop loss thresholds for a strategy.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get base returns\n",
    "    base_returns = simulate_strategy_returns(\n",
    "        underlying_returns=underlying_returns,\n",
    "        volatility=volatility,\n",
    "        vol_regime=vol_regime,\n",
    "        strategy=strategy\n",
    "    )\n",
    "    \n",
    "    for stop in stop_loss_thresholds:\n",
    "        # Apply stop loss\n",
    "        adjusted_returns = apply_stop_loss(base_returns, stop)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = (1 + adjusted_returns).prod() - 1\n",
    "        annual_return = (1 + total_return) ** (252 / len(adjusted_returns)) - 1 if len(adjusted_returns) > 0 else 0\n",
    "        vol_ann = adjusted_returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / vol_ann if vol_ann > 0 else 0\n",
    "        \n",
    "        cumulative = (1 + adjusted_returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        win_rate = (adjusted_returns > 0).mean()\n",
    "        \n",
    "        # Count stopped trades\n",
    "        if stop is not None:\n",
    "            n_stopped = (base_returns < stop).sum()\n",
    "        else:\n",
    "            n_stopped = 0\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'stop_loss': f'{stop:.0%}' if stop else 'None',\n",
    "            'stop_value': stop if stop else 0,\n",
    "            'total_return': total_return,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'win_rate': win_rate,\n",
    "            'n_stopped': n_stopped\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run stop loss optimization for each strategy\n",
    "stop_loss_results = []\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    results = optimize_strategy_parameters(\n",
    "        strategy=strategy,\n",
    "        underlying_returns=df_asset['returns'],\n",
    "        volatility=realized_vol['rv_true'],\n",
    "        vol_regime=vol_regime,\n",
    "        stop_loss_thresholds=STOP_LOSS_THRESHOLDS\n",
    "    )\n",
    "    stop_loss_results.append(results)\n",
    "\n",
    "stop_loss_df = pd.concat(stop_loss_results, ignore_index=True)\n",
    "\n",
    "print(\"Stop Loss Optimization Results:\")\n",
    "print(stop_loss_df[['strategy', 'stop_loss', 'total_return', 'sharpe', 'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS VISUALIZATION - Separate plots per strategy\n",
    "# ============================================================================\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    strategy_data = stop_loss_df[stop_loss_df['strategy'] == strategy]\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Max Drawdown vs Total Return scatter\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        -strategy_data['max_drawdown'] * 100,  # Convert to positive %\n",
    "        strategy_data['total_return'] * 100,\n",
    "        c=range(len(strategy_data)),\n",
    "        cmap='viridis',\n",
    "        s=100,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "    \n",
    "    # Annotate points\n",
    "    for idx, row in strategy_data.iterrows():\n",
    "        ax1.annotate(row['stop_loss'],\n",
    "                    xy=(-row['max_drawdown']*100, row['total_return']*100),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel('Max Drawdown (%)')\n",
    "    ax1.set_ylabel('Total Return (%)')\n",
    "    ax1.set_title(f'{strategy} - Return vs Drawdown by Stop Loss', fontsize=12, color=color)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Bar chart of Sharpe by stop loss\n",
    "    ax2 = axes[1]\n",
    "    bars = ax2.bar(strategy_data['stop_loss'], strategy_data['sharpe'], color=color, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('Stop Loss Threshold')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.set_title(f'{strategy} - Sharpe Ratio by Stop Loss', fontsize=12, color=color)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify optimal stop loss\n",
    "    best_row = strategy_data.loc[strategy_data['sharpe'].idxmax()]\n",
    "    print(f\"{strategy} - Optimal Stop Loss: {best_row['stop_loss']} (Sharpe: {best_row['sharpe']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Section 6: Market Regime Analysis Using Ensemble/Hybrid Clustering Approach\n\n**CRITICAL:** Use Ensemble of GMM, K-Means, Agglomerative Clustering + Change-Point Detection (NOT Hidden Markov Models)\n\nThis section implements regime detection using an ensemble of multiple clustering methods:\n- **Gaussian Mixture Models (GMM)** - Probabilistic clustering with soft assignments\n- **K-Means Clustering** - Hard cluster assignments, fast and simple\n- **Agglomerative Clustering** - Hierarchical clustering without shape assumptions\n- **Change-Point Detection (CPD)** - Structural break detection using ruptures library\n\n### Why Ensemble Approach?\n\n| Method | Strengths | Weaknesses |\n|--------|-----------|------------|\n| GMM | Probability distributions, handles overlapping clusters | Sensitive to initialization |\n| K-Means | Fast, simple, works well for spherical clusters | Assumes equal cluster sizes |\n| Agglomerative | Hierarchical structure, no shape assumption | Computationally intensive |\n| CPD | Explicit transition detection | May miss gradual changes |\n\nThe ensemble combines these methods to reduce individual weaknesses and provide confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6A. Feature Engineering for Regime Detection\n\nCreate comprehensive feature set for each data source (Market/Sector/Asset):\n\n**Return-based features:**\n- Daily returns, 5/10/20/50-day rolling returns\n- Return volatility (rolling std)\n- Skewness and kurtosis of returns\n\n**Volatility features:**\n- Realized volatility (multiple windows)\n- Volatility of volatility\n- High-low range\n\n**Trend features:**\n- 20/50/200 SMA relationships\n- Price distance from moving averages\n- ADX (trend strength)\n- Linear regression slope over multiple windows\n\n**Momentum features:**\n- RSI (Relative Strength Index)\n- MACD (Moving Average Convergence Divergence)\n- Rate of change indicators\n\n**Volume features (if available):**\n- Volume trends\n- Volume relative to moving average\n\nAll features normalized using StandardScaler or RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 6A: FEATURE ENGINEERING FOR REGIME DETECTION\n# ============================================================================\n\ndef engineer_regime_features(data: pd.DataFrame, feature_set: str = 'comprehensive') -> pd.DataFrame:\n    \"\"\"\n    Create comprehensive feature set for regime detection.\n    Vectorized implementation for all data sources (Market/Sector/Asset).\n    \n    Parameters:\n    -----------\n    data : pd.DataFrame\n        OHLCV data with 'close', 'high', 'low', 'volume' columns\n    feature_set : str\n        'comprehensive', 'returns_only', 'volatility_only'\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Feature DataFrame with all engineered features\n    \"\"\"\n    features = pd.DataFrame(index=data.index)\n    \n    # Ensure we have returns\n    if 'returns' not in data.columns:\n        data = data.copy()\n        data['returns'] = data['close'].pct_change()\n    \n    # ========================================================================\n    # 1. RETURN-BASED FEATURES\n    # ========================================================================\n    features['returns_1d'] = data['returns']\n    features['returns_5d'] = data['close'].pct_change(5)\n    features['returns_10d'] = data['close'].pct_change(10)\n    features['returns_20d'] = data['close'].pct_change(20)\n    features['returns_50d'] = data['close'].pct_change(50)\n    \n    # Return volatility (rolling std)\n    features['return_vol_5d'] = data['returns'].rolling(5).std()\n    features['return_vol_10d'] = data['returns'].rolling(10).std()\n    features['return_vol_20d'] = data['returns'].rolling(20).std()\n    \n    # Skewness and kurtosis of returns\n    features['return_skew_20d'] = data['returns'].rolling(20).skew()\n    features['return_kurt_20d'] = data['returns'].rolling(20).kurt()\n    \n    if feature_set in ['comprehensive', 'volatility_only']:\n        # ====================================================================\n        # 2. VOLATILITY FEATURES\n        # ====================================================================\n        # Realized volatility (multiple windows)\n        features['realized_vol_5d'] = data['returns'].rolling(5).std() * np.sqrt(252)\n        features['realized_vol_10d'] = data['returns'].rolling(10).std() * np.sqrt(252)\n        features['realized_vol_20d'] = data['returns'].rolling(20).std() * np.sqrt(252)\n        features['realized_vol_50d'] = data['returns'].rolling(50).std() * np.sqrt(252)\n        \n        # Volatility of volatility\n        features['vol_of_vol_20d'] = features['realized_vol_20d'].rolling(20).std()\n        \n        # High-low range\n        if 'high' in data.columns and 'low' in data.columns:\n            features['hl_range'] = (data['high'] - data['low']) / data['close']\n            features['hl_range_20d_avg'] = features['hl_range'].rolling(20).mean()\n    \n    if feature_set == 'comprehensive':\n        # ====================================================================\n        # 3. TREND FEATURES\n        # ====================================================================\n        # Moving averages\n        sma_20 = data['close'].rolling(20).mean()\n        sma_50 = data['close'].rolling(50).mean()\n        sma_200 = data['close'].rolling(200).mean()\n        \n        # Price distance from moving averages\n        features['price_sma20_ratio'] = data['close'] / sma_20\n        features['price_sma50_ratio'] = data['close'] / sma_50\n        features['price_sma200_ratio'] = data['close'] / sma_200\n        \n        # SMA relationships\n        features['sma20_sma50_ratio'] = sma_20 / sma_50\n        features['sma50_sma200_ratio'] = sma_50 / sma_200\n        \n        # ADX (Average Directional Index) - simplified approximation\n        if 'high' in data.columns and 'low' in data.columns:\n            plus_dm = data['high'].diff()\n            minus_dm = -data['low'].diff()\n            plus_dm = plus_dm.where(plus_dm > minus_dm, 0)\n            minus_dm = minus_dm.where(minus_dm > plus_dm, 0)\n            tr = np.maximum(data['high'] - data['low'],\n                           np.maximum(np.abs(data['high'] - data['close'].shift(1)),\n                                     np.abs(data['low'] - data['close'].shift(1))))\n            atr = tr.rolling(14).mean()\n            plus_di = 100 * (plus_dm.rolling(14).mean() / atr)\n            minus_di = 100 * (minus_dm.rolling(14).mean() / atr)\n            dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)\n            features['adx_14'] = dx.rolling(14).mean()\n        \n        # Linear regression slope (trend strength)\n        def calc_slope(series, window=20):\n            def slope_func(x):\n                if len(x) < window or x.isna().any():\n                    return np.nan\n                y = x.values\n                x_vals = np.arange(len(y))\n                try:\n                    slope, _ = np.polyfit(x_vals, y, 1)\n                    return slope\n                except:\n                    return np.nan\n            return series.rolling(window).apply(slope_func, raw=False)\n        \n        features['trend_slope_20d'] = calc_slope(data['close'], 20)\n        features['trend_slope_50d'] = calc_slope(data['close'], 50)\n        \n        # ====================================================================\n        # 4. MOMENTUM FEATURES\n        # ====================================================================\n        # RSI (Relative Strength Index)\n        delta = data['close'].diff()\n        gain = delta.where(delta > 0, 0).rolling(14).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n        rs = gain / loss\n        features['rsi_14'] = 100 - (100 / (1 + rs))\n        \n        # MACD\n        ema_12 = data['close'].ewm(span=12, adjust=False).mean()\n        ema_26 = data['close'].ewm(span=26, adjust=False).mean()\n        features['macd'] = ema_12 - ema_26\n        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()\n        features['macd_histogram'] = features['macd'] - features['macd_signal']\n        \n        # Rate of change\n        features['roc_5d'] = data['close'].pct_change(5)\n        features['roc_10d'] = data['close'].pct_change(10)\n        features['roc_20d'] = data['close'].pct_change(20)\n        \n        # ====================================================================\n        # 5. VOLUME FEATURES (if available)\n        # ====================================================================\n        if 'volume' in data.columns:\n            volume_sma_20 = data['volume'].rolling(20).mean()\n            features['volume_ratio'] = data['volume'] / volume_sma_20\n            features['volume_trend'] = data['volume'].rolling(5).mean() / volume_sma_20\n    \n    # Drop intermediate SMA columns if they exist\n    cols_to_drop = [c for c in features.columns if c.startswith('sma_')]\n    features = features.drop(columns=cols_to_drop, errors='ignore')\n    \n    return features.dropna()\n\n\ndef normalize_features(features: pd.DataFrame, method: str = 'standard') -> Tuple[pd.DataFrame, object]:\n    \"\"\"\n    Normalize features using StandardScaler or RobustScaler.\n    \n    Parameters:\n    -----------\n    features : pd.DataFrame\n        Feature DataFrame\n    method : str\n        'standard' or 'robust'\n        \n    Returns:\n    --------\n    Tuple[pd.DataFrame, object]\n        Normalized features and fitted scaler\n    \"\"\"\n    if method == 'robust':\n        scaler = RobustScaler()\n    else:\n        scaler = StandardScaler()\n    \n    scaled_data = scaler.fit_transform(features)\n    scaled_df = pd.DataFrame(scaled_data, index=features.index, columns=features.columns)\n    \n    return scaled_df, scaler\n\n\n# Engineer features for market, sector, and asset\nprint(\"=\" * 60)\nprint(\"FEATURE ENGINEERING FOR REGIME DETECTION\")\nprint(\"=\" * 60)\n\nmarket_features = engineer_regime_features(df_market, 'comprehensive')\nprint(f\"\\nMarket features: {market_features.shape[0]} observations, {market_features.shape[1]} features\")\n\nsector_features = engineer_regime_features(df_sector, 'comprehensive')\nprint(f\"Sector features: {sector_features.shape[0]} observations, {sector_features.shape[1]} features\")\n\nasset_features = engineer_regime_features(df_asset, 'comprehensive')\nprint(f\"Asset features: {asset_features.shape[0]} observations, {asset_features.shape[1]} features\")\n\nprint(f\"\\nFeature columns ({len(market_features.columns)}):\")\nfor i, col in enumerate(market_features.columns):\n    category = 'Return' if 'return' in col.lower() else \\\n               'Volatility' if 'vol' in col.lower() or 'hl_range' in col.lower() else \\\n               'Trend' if 'sma' in col.lower() or 'slope' in col.lower() or 'adx' in col.lower() else \\\n               'Momentum' if 'rsi' in col.lower() or 'macd' in col.lower() or 'roc' in col.lower() else \\\n               'Volume' if 'volume' in col.lower() else 'Other'\n    print(f\"  [{category}] {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6B. Individual Clustering Methods\n\n**Method 1 - Gaussian Mixture Models (GMM):**\n- Use sklearn's GaussianMixture\n- Test 3-6 components (regimes)\n- Use BIC (Bayesian Information Criterion) for optimal component selection\n- Captures probabilistic cluster assignments and elliptical cluster shapes\n- Extract soft cluster probabilities for each time point\n- Advantages: Provides probability distributions, handles overlapping clusters\n\n**Method 2 - K-Means Clustering:**\n- Use sklearn's KMeans\n- Test k=3 to k=6 clusters\n- Use silhouette score and elbow method for optimal k selection\n- Provides hard cluster assignments\n- Advantages: Fast, simple, works well for spherical clusters\n\n**Method 3 - Agglomerative (Hierarchical) Clustering:**\n- Use sklearn's AgglomerativeClustering\n- Test 3-6 clusters with different linkage methods (ward, complete, average)\n- Create dendrogram to visualize hierarchical relationships\n- Advantages: No assumption about cluster shape, reveals hierarchical structure\n\n**Method 4 - Change-Point Detection (CPD):**\n- Use ruptures library (Pelt, Binseg, or BottomUp algorithms)\n- Detect structural breaks in the feature time series\n- Identifies when statistical properties of data change\n- Use detected change points to segment time series into regimes\n- Advantages: Explicitly detects regime transitions, doesn't assume fixed number of regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 6B: INDIVIDUAL CLUSTERING METHODS\n# ============================================================================\n\ndef fit_gmm_regimes(features: pd.DataFrame, n_components_range: range = range(3, 7),\n                    random_state: int = 42) -> Tuple[np.ndarray, GaussianMixture, int, np.ndarray]:\n    \"\"\"\n    Fit Gaussian Mixture Model for regime detection.\n    Uses BIC (Bayesian Information Criterion) for optimal component selection.\n    \n    Parameters:\n    -----------\n    features : pd.DataFrame\n        Normalized feature DataFrame\n    n_components_range : range\n        Range of components to test (default: 3-6)\n    random_state : int\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    Tuple[np.ndarray, GaussianMixture, int, np.ndarray]\n        Labels, fitted model, optimal n_components, soft probabilities\n    \"\"\"\n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(features)\n    \n    # Find optimal n_components using BIC\n    bic_scores = []\n    aic_scores = []\n    models = []\n    \n    print(\"GMM: Testing components...\")\n    for n in n_components_range:\n        gmm = GaussianMixture(n_components=n, random_state=random_state, n_init=5,\n                              covariance_type='full', max_iter=200)\n        gmm.fit(X_scaled)\n        bic_scores.append(gmm.bic(X_scaled))\n        aic_scores.append(gmm.aic(X_scaled))\n        models.append(gmm)\n        print(f\"  n={n}: BIC={bic_scores[-1]:.0f}, AIC={aic_scores[-1]:.0f}\")\n    \n    # Select model with lowest BIC\n    best_idx = np.argmin(bic_scores)\n    best_n = list(n_components_range)[best_idx]\n    best_model = models[best_idx]\n    \n    # Get labels and soft probabilities\n    labels = best_model.predict(X_scaled)\n    probabilities = best_model.predict_proba(X_scaled)\n    \n    print(f\"GMM: Optimal components = {best_n} (BIC = {bic_scores[best_idx]:.0f})\")\n    \n    return labels, best_model, best_n, probabilities\n\n\ndef fit_kmeans_regimes(features: pd.DataFrame, k_range: range = range(3, 7),\n                       random_state: int = 42) -> Tuple[np.ndarray, KMeans, int, np.ndarray]:\n    \"\"\"\n    Fit K-Means clustering for regime detection.\n    Uses silhouette score and elbow method for optimal k selection.\n    \n    Parameters:\n    -----------\n    features : pd.DataFrame\n        Feature DataFrame\n    k_range : range\n        Range of k values to test\n    random_state : int\n        Random seed\n        \n    Returns:\n    --------\n    Tuple[np.ndarray, KMeans, int, np.ndarray]\n        Labels, fitted model, optimal k, distances to cluster centers\n    \"\"\"\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(features)\n    \n    silhouette_scores = []\n    inertias = []  # For elbow method\n    models = []\n    \n    print(\"K-Means: Testing cluster counts...\")\n    for k in k_range:\n        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10, max_iter=300)\n        labels = kmeans.fit_predict(X_scaled)\n        score = silhouette_score(X_scaled, labels)\n        silhouette_scores.append(score)\n        inertias.append(kmeans.inertia_)\n        models.append(kmeans)\n        print(f\"  k={k}: Silhouette={score:.3f}, Inertia={kmeans.inertia_:.0f}\")\n    \n    # Select best k by silhouette score\n    best_idx = np.argmax(silhouette_scores)\n    best_k = list(k_range)[best_idx]\n    best_model = models[best_idx]\n    labels = best_model.predict(X_scaled)\n    \n    # Calculate distance to cluster centers (inverse for confidence)\n    distances = best_model.transform(X_scaled)\n    min_distances = distances.min(axis=1)\n    \n    print(f\"K-Means: Optimal k = {best_k} (Silhouette = {silhouette_scores[best_idx]:.3f})\")\n    \n    return labels, best_model, best_k, min_distances\n\n\ndef fit_agglomerative_regimes(features: pd.DataFrame, n_clusters_range: range = range(3, 7),\n                               linkage_methods: list = None) -> Tuple[np.ndarray, int, str]:\n    \"\"\"\n    Fit Agglomerative (Hierarchical) clustering.\n    Tests multiple linkage methods and cluster counts.\n    \n    Parameters:\n    -----------\n    features : pd.DataFrame\n        Feature DataFrame\n    n_clusters_range : range\n        Range of cluster counts to test\n    linkage_methods : list\n        Linkage methods to test (default: ['ward', 'complete', 'average'])\n        \n    Returns:\n    --------\n    Tuple[np.ndarray, int, str]\n        Best labels, optimal n_clusters, best linkage method\n    \"\"\"\n    if linkage_methods is None:\n        linkage_methods = ['ward', 'complete', 'average']\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(features)\n    \n    best_score = -1\n    best_labels = None\n    best_n = None\n    best_linkage = None\n    \n    print(\"Agglomerative: Testing configurations...\")\n    for linkage in linkage_methods:\n        for n in n_clusters_range:\n            try:\n                agg = AgglomerativeClustering(n_clusters=n, linkage=linkage)\n                labels = agg.fit_predict(X_scaled)\n                score = silhouette_score(X_scaled, labels)\n                \n                if score > best_score:\n                    best_score = score\n                    best_labels = labels\n                    best_n = n\n                    best_linkage = linkage\n                \n                print(f\"  {linkage}, n={n}: Silhouette={score:.3f}\")\n            except Exception as e:\n                continue\n    \n    print(f\"Agglomerative: Optimal = {best_n} clusters, {best_linkage} linkage (Silhouette = {best_score:.3f})\")\n    \n    return best_labels, best_n, best_linkage\n\n\ndef detect_change_points(features: pd.DataFrame, method: str = 'pelt',\n                         n_bkps: int = None, penalty: int = 3) -> Tuple[np.ndarray, list]:\n    \"\"\"\n    Detect structural breaks using change-point detection (ruptures library).\n    \n    Parameters:\n    -----------\n    features : pd.DataFrame\n        Feature DataFrame\n    method : str\n        CPD algorithm: 'pelt', 'binseg', or 'bottomup'\n    n_bkps : int\n        Number of breakpoints (for binseg/bottomup). If None, uses penalty.\n    penalty : int\n        Penalty parameter for PELT\n        \n    Returns:\n    --------\n    Tuple[np.ndarray, list]\n        Regime labels based on segments, list of change point indices\n    \"\"\"\n    if not RUPTURES_AVAILABLE:\n        print(\"Warning: ruptures library not available, returning uniform segments\")\n        return np.zeros(len(features)), []\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(features)\n    \n    print(f\"Change-Point Detection: Using {method.upper()} algorithm...\")\n    \n    try:\n        if method == 'pelt':\n            algo = rpt.Pelt(model='rbf', min_size=10).fit(X_scaled)\n            result = algo.predict(pen=penalty)\n        elif method == 'binseg':\n            n_breakpoints = n_bkps if n_bkps else min(10, len(features) // 50)\n            algo = rpt.Binseg(model='rbf', min_size=10).fit(X_scaled)\n            result = algo.predict(n_bkps=n_breakpoints)\n        else:  # bottomup\n            n_breakpoints = n_bkps if n_bkps else min(10, len(features) // 50)\n            algo = rpt.BottomUp(model='rbf', min_size=10).fit(X_scaled)\n            result = algo.predict(n_bkps=n_breakpoints)\n    except Exception as e:\n        print(f\"  CPD failed: {e}\")\n        return np.zeros(len(features)), []\n    \n    # Convert change points to labels\n    labels = np.zeros(len(features), dtype=int)\n    prev_bkp = 0\n    for i, bkp in enumerate(result):\n        bkp = min(bkp, len(features))  # Ensure within bounds\n        labels[prev_bkp:bkp] = i\n        prev_bkp = bkp\n    \n    print(f\"  Detected {len(result)} change points: {result[:10]}...\")\n    \n    return labels, result\n\n\n# ============================================================================\n# CLUSTERING VALIDATION METRICS\n# ============================================================================\n\ndef validate_clustering(features: pd.DataFrame, labels: np.ndarray, method_name: str) -> dict:\n    \"\"\"\n    Calculate validation metrics for clustering results.\n    \"\"\"\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(features)\n    \n    metrics = {\n        'method': method_name,\n        'n_clusters': len(np.unique(labels)),\n        'silhouette': silhouette_score(X_scaled, labels),\n        'davies_bouldin': davies_bouldin_score(X_scaled, labels),\n        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels)\n    }\n    \n    return metrics\n\n\nprint(\"Individual clustering methods defined:\")\nprint(\"  - fit_gmm_regimes(): GMM with BIC selection\")\nprint(\"  - fit_kmeans_regimes(): K-Means with silhouette selection\")\nprint(\"  - fit_agglomerative_regimes(): Hierarchical with multiple linkages\")\nprint(\"  - detect_change_points(): CPD using ruptures library\")\nprint(\"  - validate_clustering(): Validation metrics calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6C. Ensemble Regime Classification\n\n**Voting/Consensus Mechanism:**\nFor each time point, collect regime labels from all 4 methods and implement weighted voting:\n\n- **GMM:** Weight by probability confidence (entropy-based)\n- **K-Means:** Weight by distance to cluster center (inverse)\n- **Agglomerative:** Weight by dendrogram height/distance\n- **CPD:** Higher weight near detected change points\n\n**Ensemble Strategies:**\n1. **Majority Voting:** Most common regime label wins\n2. **Weighted Voting:** Weight each method's vote by its confidence metric\n3. **Hierarchical Ensemble:** Use CPD to identify major regime boundaries, then use GMM/K-Means/Agglomerative within each segment\n4. **Stacking:** Use cluster labels as features for meta-classifier\n\n**Regime Label Mapping:**\nAfter ensemble classification, map numeric cluster labels to meaningful regime names using feature centroids:\n- High returns + low volatility \u2192 \"Bull_Low_Vol\"\n- High returns + high volatility \u2192 \"Bull_High_Vol\"\n- Negative returns + high volatility \u2192 \"Bear\"\n- Low returns + low volatility \u2192 \"Choppy\" or \"Sideways\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 6C: ENSEMBLE REGIME CLASSIFICATION\n# ============================================================================\n\ndef ensemble_regime_classification(gmm_labels: np.ndarray, kmeans_labels: np.ndarray,\n                                   agg_labels: np.ndarray, cpd_labels: np.ndarray,\n                                   gmm_probs: np.ndarray = None,\n                                   kmeans_distances: np.ndarray = None,\n                                   weights: dict = None,\n                                   strategy: str = 'weighted') -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Combine multiple clustering methods using voting/consensus.\n    \n    Parameters:\n    -----------\n    gmm_labels : np.ndarray\n        Labels from GMM\n    kmeans_labels : np.ndarray\n        Labels from K-Means\n    agg_labels : np.ndarray\n        Labels from Agglomerative clustering\n    cpd_labels : np.ndarray\n        Labels from Change-Point Detection\n    gmm_probs : np.ndarray\n        Soft probabilities from GMM\n    kmeans_distances : np.ndarray\n        Distances to cluster centers from K-Means\n    weights : dict\n        Weights for each method (default: equal weights)\n    strategy : str\n        'majority', 'weighted', 'hierarchical', or 'stacking'\n        \n    Returns:\n    --------\n    Tuple[np.ndarray, np.ndarray]\n        Ensemble labels, confidence scores\n    \"\"\"\n    n_samples = len(gmm_labels)\n    \n    # Default weights\n    if weights is None:\n        weights = {'gmm': 0.30, 'kmeans': 0.25, 'agg': 0.25, 'cpd': 0.20}\n    \n    # Calculate confidence weights for each method\n    gmm_conf = np.ones(n_samples)\n    kmeans_conf = np.ones(n_samples)\n    \n    if gmm_probs is not None:\n        # GMM confidence: 1 - entropy of probabilities\n        entropy = -np.sum(gmm_probs * np.log(gmm_probs + 1e-10), axis=1)\n        max_entropy = np.log(gmm_probs.shape[1])\n        gmm_conf = 1 - (entropy / max_entropy)\n    \n    if kmeans_distances is not None:\n        # K-Means confidence: inverse of min distance (normalized)\n        kmeans_conf = 1 / (1 + kmeans_distances)\n        kmeans_conf = kmeans_conf / kmeans_conf.max()\n    \n    if strategy == 'majority':\n        # Simple majority voting\n        all_labels = np.column_stack([gmm_labels, kmeans_labels, agg_labels, cpd_labels])\n        ensemble_labels = np.zeros(n_samples, dtype=int)\n        \n        for i in range(n_samples):\n            votes = all_labels[i, :]\n            unique, counts = np.unique(votes, return_counts=True)\n            ensemble_labels[i] = unique[np.argmax(counts)]\n        \n        # Confidence = fraction of methods that agree\n        agreement = np.zeros(n_samples)\n        for i in range(n_samples):\n            votes = all_labels[i, :]\n            mode = ensemble_labels[i]\n            agreement[i] = np.sum(votes == mode) / len(votes)\n        confidence = agreement\n        \n    elif strategy == 'weighted':\n        # Weighted voting based on method confidence\n        n_unique = max(len(np.unique(l)) for l in [gmm_labels, kmeans_labels, agg_labels, cpd_labels])\n        \n        # Create weighted vote matrix\n        vote_matrix = np.zeros((n_samples, n_unique))\n        \n        for i in range(n_samples):\n            # Add weighted votes\n            if gmm_labels[i] < n_unique:\n                vote_matrix[i, gmm_labels[i]] += weights['gmm'] * gmm_conf[i]\n            if kmeans_labels[i] < n_unique:\n                vote_matrix[i, kmeans_labels[i]] += weights['kmeans'] * kmeans_conf[i]\n            if agg_labels[i] < n_unique:\n                vote_matrix[i, agg_labels[i]] += weights['agg']\n            if cpd_labels[i] < n_unique:\n                vote_matrix[i, cpd_labels[i]] += weights['cpd']\n        \n        ensemble_labels = np.argmax(vote_matrix, axis=1)\n        \n        # Confidence = normalized max vote weight\n        max_votes = vote_matrix.max(axis=1)\n        total_votes = vote_matrix.sum(axis=1)\n        confidence = max_votes / (total_votes + 1e-10)\n        \n    elif strategy == 'hierarchical':\n        # Use CPD for major boundaries, then use GMM within segments\n        ensemble_labels = gmm_labels.copy()\n        \n        # At CPD change points, trust CPD more\n        cpd_changes = np.where(np.diff(cpd_labels) != 0)[0]\n        for cp in cpd_changes:\n            # Smooth transition at change points\n            window = slice(max(0, cp-2), min(n_samples, cp+3))\n            ensemble_labels[window] = cpd_labels[window]\n        \n        confidence = gmm_conf\n        \n    else:  # stacking\n        # Use cluster labels as meta-features\n        meta_features = np.column_stack([gmm_labels, kmeans_labels, agg_labels, cpd_labels])\n        # Simple approach: weighted average\n        ensemble_labels = np.round(\n            weights['gmm'] * gmm_labels + \n            weights['kmeans'] * kmeans_labels + \n            weights['agg'] * agg_labels + \n            weights['cpd'] * cpd_labels\n        ).astype(int)\n        confidence = gmm_conf\n    \n    return ensemble_labels, confidence\n\n\ndef map_regime_labels(labels: np.ndarray, features: pd.DataFrame,\n                      key_features: list = None) -> np.ndarray:\n    \"\"\"\n    Map numeric cluster labels to meaningful regime names.\n    Uses feature centroids to interpret regimes.\n    \n    Parameters:\n    -----------\n    labels : np.ndarray\n        Cluster labels\n    features : pd.DataFrame\n        Feature DataFrame\n    key_features : list\n        Key features for regime interpretation (default: returns and volatility)\n        \n    Returns:\n    --------\n    np.ndarray\n        Array of regime name strings\n    \"\"\"\n    if key_features is None:\n        # Find returns and volatility columns\n        return_col = [c for c in features.columns if 'returns_20d' in c.lower()]\n        vol_col = [c for c in features.columns if 'realized_vol_20d' in c.lower() or 'vol_20d' in c.lower()]\n        key_features = (return_col[:1] if return_col else []) + (vol_col[:1] if vol_col else [])\n    \n    if not key_features:\n        # Default to first two features\n        key_features = features.columns[:2].tolist()\n    \n    # Calculate centroids\n    regime_names = []\n    \n    for label in np.unique(labels):\n        mask = labels == label\n        centroid = features.loc[features.index[mask], key_features].mean()\n        \n        # Interpret centroid\n        ret_val = centroid.iloc[0] if len(centroid) > 0 else 0\n        vol_val = centroid.iloc[1] if len(centroid) > 1 else 0\n        \n        if ret_val > 0.01:  # Positive returns\n            if vol_val > 0.20:\n                name = 'Bull_High_Vol'\n            else:\n                name = 'Bull_Low_Vol'\n        elif ret_val < -0.01:  # Negative returns\n            name = 'Bear'\n        else:  # Low returns\n            if vol_val > 0.20:\n                name = 'Choppy'\n            else:\n                name = 'Sideways'\n        \n        regime_names.append((label, name))\n    \n    # Create mapping and apply\n    label_to_name = {label: name for label, name in regime_names}\n    return np.array([label_to_name.get(l, f'Regime_{l}') for l in labels])\n\n\ndef calculate_regime_transitions(regime_series: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Calculate transition probability matrix for regimes.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Transition probability matrix\n    \"\"\"\n    regimes = regime_series.unique()\n    n_regimes = len(regimes)\n    \n    # Count transitions\n    transition_counts = pd.DataFrame(0, index=regimes, columns=regimes)\n    \n    for i in range(len(regime_series) - 1):\n        from_regime = regime_series.iloc[i]\n        to_regime = regime_series.iloc[i + 1]\n        transition_counts.loc[from_regime, to_regime] += 1\n    \n    # Convert to probabilities\n    transition_probs = transition_counts.div(transition_counts.sum(axis=1), axis=0)\n    transition_probs = transition_probs.fillna(0)\n    \n    return transition_probs\n\n\nprint(\"Ensemble classification functions defined:\")\nprint(\"  - ensemble_regime_classification(): Combine methods with voting\")\nprint(\"  - map_regime_labels(): Map to meaningful regime names\")\nprint(\"  - calculate_regime_transitions(): Transition probability matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6D. Regime Detection Validation & Visualization\n",
    "\n",
    "**Validation Metrics:**\n",
    "- Silhouette score: Measure cluster cohesion and separation\n",
    "- Davies-Bouldin index: Lower is better\n",
    "- Calinski-Harabasz index: Higher is better\n",
    "- Stability analysis: Bootstrap resampling to test regime stability\n",
    "- Transition frequency: Regimes shouldn't change too frequently (apply smoothing if needed)\n",
    "- Economic interpretation: Do regimes align with known market events?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6D: REGIME DETECTION VALIDATION & VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_regime_clustering_comprehensive(features: pd.DataFrame, labels: np.ndarray,\n",
    "                                             n_bootstrap: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive validation including stability analysis via bootstrap.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Standard clustering metrics\n",
    "    metrics = {\n",
    "        'silhouette': silhouette_score(X_scaled, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(X_scaled, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels),\n",
    "        'n_clusters': len(np.unique(labels)),\n",
    "        'cluster_sizes': dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    }\n",
    "    \n",
    "    # Bootstrap stability analysis (vectorized)\n",
    "    n_samples = len(labels)\n",
    "    bootstrap_silhouettes = []\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        idx = rng.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_boot = X_scaled[idx]\n",
    "        labels_boot = labels[idx]\n",
    "        \n",
    "        # Only calculate if we have multiple clusters\n",
    "        if len(np.unique(labels_boot)) > 1:\n",
    "            bootstrap_silhouettes.append(silhouette_score(X_boot, labels_boot))\n",
    "    \n",
    "    metrics['silhouette_std'] = np.std(bootstrap_silhouettes) if bootstrap_silhouettes else 0\n",
    "    metrics['silhouette_95ci'] = (np.percentile(bootstrap_silhouettes, 2.5),\n",
    "                                   np.percentile(bootstrap_silhouettes, 97.5)) if bootstrap_silhouettes else (0, 0)\n",
    "    \n",
    "    # Transition frequency analysis\n",
    "    transitions = np.sum(np.diff(labels) != 0)\n",
    "    metrics['transition_count'] = transitions\n",
    "    metrics['avg_regime_duration'] = n_samples / (transitions + 1)  # Average days per regime\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def apply_regime_smoothing(labels: np.ndarray, min_duration: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply smoothing to prevent too frequent regime changes.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    smoothed = labels.copy()\n",
    "    n = len(smoothed)\n",
    "    \n",
    "    # Find regime change points\n",
    "    change_points = np.where(np.diff(smoothed) != 0)[0] + 1\n",
    "    change_points = np.concatenate([[0], change_points, [n]])\n",
    "    \n",
    "    # Merge short segments\n",
    "    for i in range(1, len(change_points) - 1):\n",
    "        segment_len = change_points[i] - change_points[i-1]\n",
    "        if segment_len < min_duration:\n",
    "            # Merge with previous segment\n",
    "            smoothed[change_points[i-1]:change_points[i]] = smoothed[change_points[i-1] - 1] if change_points[i-1] > 0 else smoothed[change_points[i]]\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def check_economic_interpretation(regime_series: pd.Series, price_data: pd.DataFrame,\n",
    "                                  known_events: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check if regimes align with known market events.\n",
    "    known_events format: {'2020-03': 'COVID Crash', '2022-01': 'Fed Tightening'}\n",
    "    \"\"\"\n",
    "    if known_events is None:\n",
    "        known_events = {\n",
    "            '2020-02': 'COVID Crash Start',\n",
    "            '2020-03': 'COVID Bottom',\n",
    "            '2020-04': 'Recovery Begin',\n",
    "            '2021-11': 'Market Peak',\n",
    "            '2022-01': 'Fed Tightening',\n",
    "            '2022-06': 'Bear Market',\n",
    "            '2022-10': 'Market Bottom',\n",
    "            '2023-03': 'Banking Crisis',\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    for date_str, event in known_events.items():\n",
    "        try:\n",
    "            # Find nearest date in regime series\n",
    "            target_date = pd.to_datetime(date_str)\n",
    "            mask = regime_series.index >= target_date\n",
    "            if mask.any():\n",
    "                nearest_idx = regime_series.index[mask][0]\n",
    "                regime_at_event = regime_series.loc[nearest_idx]\n",
    "                results.append({'Event': event, 'Date': date_str, 'Regime': regime_at_event})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run comprehensive validation\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE REGIME VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "market_validation = validate_regime_clustering_comprehensive(market_features, ensemble_labels_mkt)\n",
    "print(f\"\\nMARKET REGIMES ({MARKET_PROXY}):\")\n",
    "print(f\"  Silhouette Score: {market_validation['silhouette']:.3f} \u00b1 {market_validation['silhouette_std']:.3f}\")\n",
    "print(f\"  95% CI: [{market_validation['silhouette_95ci'][0]:.3f}, {market_validation['silhouette_95ci'][1]:.3f}]\")\n",
    "print(f\"  Davies-Bouldin Index: {market_validation['davies_bouldin']:.3f} (lower is better)\")\n",
    "print(f\"  Calinski-Harabasz Index: {market_validation['calinski_harabasz']:.1f} (higher is better)\")\n",
    "print(f\"  Transition Count: {market_validation['transition_count']}\")\n",
    "print(f\"  Avg Regime Duration: {market_validation['avg_regime_duration']:.1f} days\")\n",
    "\n",
    "sector_validation = validate_regime_clustering_comprehensive(sector_features, ensemble_labels_sec)\n",
    "print(f\"\\nSECTOR REGIMES ({SECTOR_ETF}):\")\n",
    "print(f\"  Silhouette Score: {sector_validation['silhouette']:.3f} \u00b1 {sector_validation['silhouette_std']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {sector_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Avg Regime Duration: {sector_validation['avg_regime_duration']:.1f} days\")\n",
    "\n",
    "asset_validation = validate_regime_clustering_comprehensive(asset_features, ensemble_labels_ast)\n",
    "print(f\"\\nASSET REGIMES ({TICKER}):\")\n",
    "print(f\"  Silhouette Score: {asset_validation['silhouette']:.3f} \u00b1 {asset_validation['silhouette_std']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {asset_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Avg Regime Duration: {asset_validation['avg_regime_duration']:.1f} days\")\n",
    "\n",
    "# Economic interpretation check\n",
    "print(\"\\nECONOMIC EVENT ALIGNMENT:\")\n",
    "econ_check = check_economic_interpretation(market_regimes, df_market)\n",
    "if len(econ_check) > 0:\n",
    "    print(econ_check.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6D.1 Ensemble Method Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARISON PLOT: Regime assignments from each method\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_comparison(gmm_labels, kmeans_labels, agg_labels, cpd_labels,\n",
    "                           ensemble_labels, index, title, figsize=(14, 12)):\n",
    "    \"\"\"\n",
    "    Show regime assignments from each individual method as separate time series rows,\n",
    "    with ensemble result at bottom.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 1, figsize=figsize, sharex=True)\n",
    "    \n",
    "    methods = [\n",
    "        ('GMM', gmm_labels, '#3498db'),\n",
    "        ('K-Means', kmeans_labels, '#e74c3c'),\n",
    "        ('Agglomerative', agg_labels, '#2ecc71'),\n",
    "        ('CPD', cpd_labels, '#9b59b6'),\n",
    "        ('ENSEMBLE', ensemble_labels, '#2c3e50')  # Emphasized\n",
    "    ]\n",
    "    \n",
    "    for ax, (method, labels, color) in zip(axes, methods):\n",
    "        ax.plot(index, labels, drawstyle='steps-post', color=color, linewidth=1.5)\n",
    "        ax.fill_between(index, labels, alpha=0.3, step='post', color=color)\n",
    "        ax.set_ylabel(method, fontsize=11, fontweight='bold' if method == 'ENSEMBLE' else 'normal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yticks(range(int(max(labels)) + 1))\n",
    "        \n",
    "        # Highlight ensemble row\n",
    "        if method == 'ENSEMBLE':\n",
    "            ax.set_facecolor('#f5f5f5')\n",
    "            ax.spines['left'].set_linewidth(3)\n",
    "            ax.spines['left'].set_color(color)\n",
    "    \n",
    "    axes[0].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[-1].set_xlabel('Date', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot method comparison for market\n",
    "plot_regime_comparison(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    ensemble_labels_mkt, market_features.index,\n",
    "    f'{MARKET_PROXY} - Regime Method Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONSENSUS HEATMAP: Agreement level between methods over time\n",
    "# ============================================================================\n",
    "\n",
    "def plot_ensemble_consensus_heatmap(gmm_labels, kmeans_labels, agg_labels, cpd_labels,\n",
    "                                    index, title, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Show agreement level between methods over time (darker = higher agreement).\n",
    "    \"\"\"\n",
    "    n_samples = len(gmm_labels)\n",
    "    \n",
    "    # Calculate agreement scores vectorized\n",
    "    labels_matrix = np.vstack([gmm_labels, kmeans_labels, agg_labels, cpd_labels]).T\n",
    "    agreement = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        unique, counts = np.unique(labels_matrix[i], return_counts=True)\n",
    "        agreement[i] = counts.max() / 4  # Max agreement out of 4 methods\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create colormap based on agreement\n",
    "    colors = plt.cm.RdYlGn(agreement)  # Red=low, Yellow=medium, Green=high\n",
    "    \n",
    "    # Create bar chart with colors\n",
    "    for i, (date, agr) in enumerate(zip(index, agreement)):\n",
    "        ax.bar(date, 1, width=1, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.axhline(y=0.75, color='darkgreen', linestyle='--', linewidth=2, label='High agreement (75%+)')\n",
    "    ax.axhline(y=0.50, color='orange', linestyle='--', linewidth=2, label='Moderate (50%)')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Agreement Level', fontsize=12)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=plt.Normalize(0.25, 1))\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Agreement', fontsize=11)\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"Agreement Statistics:\")\n",
    "    print(f\"  Mean agreement: {np.mean(agreement):.1%}\")\n",
    "    print(f\"  High agreement periods (>=75%): {(agreement >= 0.75).sum() / len(agreement):.1%}\")\n",
    "    print(f\"  Low agreement periods (<50%): {(agreement < 0.5).sum() / len(agreement):.1%}\")\n",
    "\n",
    "\n",
    "plot_ensemble_consensus_heatmap(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    market_features.index,\n",
    "    f'{MARKET_PROXY} - Method Consensus Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SPACE VISUALIZATION: 2D PCA projection colored by regime\n",
    "# ============================================================================\n",
    "\n",
    "def plot_feature_space_pca(features: pd.DataFrame, regime_labels: np.ndarray,\n",
    "                           regime_names: pd.Series, title: str, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    2D PCA projection of features, colored by ensemble regime assignment.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot each regime with its color\n",
    "    unique_regimes = regime_names.unique()\n",
    "    \n",
    "    for regime in unique_regimes:\n",
    "        mask = regime_names.values == regime\n",
    "        color = get_regime_color(regime)\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                  c=color, label=regime, alpha=0.6, s=30, edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # Add cluster centers\n",
    "    for regime in unique_regimes:\n",
    "        mask = regime_names.values == regime\n",
    "        center = X_pca[mask].mean(axis=0)\n",
    "        color = get_regime_color(regime)\n",
    "        ax.scatter(center[0], center[1], c=color, s=300, marker='*', \n",
    "                  edgecolors='black', linewidth=2, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(title='Regime', loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPCA Explained Variance: PC1={pca.explained_variance_ratio_[0]:.1%}, PC2={pca.explained_variance_ratio_[1]:.1%}\")\n",
    "    print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "\n",
    "\n",
    "plot_feature_space_pca(\n",
    "    market_features, ensemble_labels_mkt, market_regimes,\n",
    "    f'{MARKET_PROXY} - Feature Space (PCA) by Regime'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD AGREEMENT MATRIX: How often do pairs of methods agree?\n",
    "# ============================================================================\n",
    "\n",
    "def plot_method_agreement_matrix(gmm_labels, kmeans_labels, agg_labels, cpd_labels,\n",
    "                                  title: str, figsize=(8, 7)):\n",
    "    \"\"\"\n",
    "    Show how often pairs of methods agree on regime classification.\n",
    "    \"\"\"\n",
    "    methods = ['GMM', 'K-Means', 'Agglomerative', 'CPD']\n",
    "    labels_list = [gmm_labels, kmeans_labels, agg_labels, cpd_labels]\n",
    "    n_methods = len(methods)\n",
    "    \n",
    "    # Calculate pairwise agreement (vectorized)\n",
    "    agreement_matrix = np.zeros((n_methods, n_methods))\n",
    "    \n",
    "    for i in range(n_methods):\n",
    "        for j in range(n_methods):\n",
    "            agreement_matrix[i, j] = np.mean(labels_list[i] == labels_list[j])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    im = ax.imshow(agreement_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Agreement Rate', fontsize=11)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(range(n_methods))\n",
    "    ax.set_yticks(range(n_methods))\n",
    "    ax.set_xticklabels(methods, fontsize=11)\n",
    "    ax.set_yticklabels(methods, fontsize=11)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i in range(n_methods):\n",
    "        for j in range(n_methods):\n",
    "            text = f'{agreement_matrix[i, j]:.0%}'\n",
    "            color = 'white' if agreement_matrix[i, j] < 0.5 else 'black'\n",
    "            ax.text(j, i, text, ha='center', va='center', fontsize=12, \n",
    "                   fontweight='bold', color=color)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate overall method concordance\n",
    "    off_diag = agreement_matrix[~np.eye(n_methods, dtype=bool)]\n",
    "    print(f\"\\nOverall Method Agreement: {off_diag.mean():.1%}\")\n",
    "    print(f\"Min pairwise agreement: {off_diag.min():.1%}\")\n",
    "    print(f\"Max pairwise agreement: {off_diag.max():.1%}\")\n",
    "\n",
    "\n",
    "plot_method_agreement_matrix(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    f'{MARKET_PROXY} - Method Agreement Matrix'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6E. Apply Ensemble Regime Detection to Three Levels\n",
    "\n",
    "Apply the ensemble method to Market, Sector, and Asset levels with priority visualizations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTOR ETF MAPPING\n",
    "# ============================================================================\n",
    "\n",
    "SECTOR_ETF_MAP = {\n",
    "    # Technology\n",
    "    'XLK': 'Technology',\n",
    "    'QQQ': 'Technology',\n",
    "    'VGT': 'Technology',\n",
    "    # Healthcare\n",
    "    'XLV': 'Healthcare',\n",
    "    'VHT': 'Healthcare',\n",
    "    'IBB': 'Biotechnology',\n",
    "    # Financials\n",
    "    'XLF': 'Financials',\n",
    "    'VFH': 'Financials',\n",
    "    # Consumer\n",
    "    'XLY': 'Consumer Discretionary',\n",
    "    'XLP': 'Consumer Staples',\n",
    "    # Energy\n",
    "    'XLE': 'Energy',\n",
    "    'VDE': 'Energy',\n",
    "    # Industrials\n",
    "    'XLI': 'Industrials',\n",
    "    'VIS': 'Industrials',\n",
    "    # Real Estate\n",
    "    'XLRE': 'Real Estate',\n",
    "    'VNQ': 'Real Estate',\n",
    "    # Materials\n",
    "    'XLB': 'Materials',\n",
    "    # Utilities\n",
    "    'XLU': 'Utilities',\n",
    "    # Communication\n",
    "    'XLC': 'Communication Services',\n",
    "}\n",
    "\n",
    "# Reverse mapping: sector name to primary ETF\n",
    "SECTOR_TO_ETF = {\n",
    "    'Technology': 'XLK',\n",
    "    'Healthcare': 'XLV',\n",
    "    'Biotechnology': 'XLV',  # Fall back to healthcare\n",
    "    'Financials': 'XLF',\n",
    "    'Consumer Discretionary': 'XLY',\n",
    "    'Consumer Staples': 'XLP',\n",
    "    'Energy': 'XLE',\n",
    "    'Industrials': 'XLI',\n",
    "    'Real Estate': 'XLRE',\n",
    "    'Materials': 'XLB',\n",
    "    'Utilities': 'XLU',\n",
    "    'Communication Services': 'XLC',\n",
    "}\n",
    "\n",
    "def get_sector_etf(ticker: str, qb) -> str:\n",
    "    \"\"\"\n",
    "    Dynamically determine sector ETF based on ticker's sector classification.\n",
    "    Uses QuantConnect's fundamental data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get security fundamentals\n",
    "        symbol = qb.AddEquity(ticker).Symbol\n",
    "        fundamentals = qb.GetFundamental(symbol, 'AssetClassification.MorningstarSectorCode')\n",
    "        \n",
    "        sector_code_map = {\n",
    "            311: 'XLK',   # Technology\n",
    "            206: 'XLV',   # Healthcare\n",
    "            103: 'XLF',   # Financial Services\n",
    "            102: 'XLY',   # Consumer Cyclical\n",
    "            205: 'XLP',   # Consumer Defensive\n",
    "            309: 'XLE',   # Energy\n",
    "            310: 'XLI',   # Industrials\n",
    "            104: 'XLRE',  # Real Estate\n",
    "            101: 'XLB',   # Basic Materials\n",
    "            207: 'XLU',   # Utilities\n",
    "            308: 'XLC',   # Communication Services\n",
    "        }\n",
    "        \n",
    "        if fundamentals in sector_code_map:\n",
    "            return sector_code_map[fundamentals]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Default fallback for HIMS (healthcare sector)\n",
    "    if ticker.upper() == 'HIMS':\n",
    "        return 'XLV'\n",
    "    return 'XLK'  # Default to tech\n",
    "\n",
    "\n",
    "# Set sector ETF for our ticker\n",
    "SECTOR_ETF = get_sector_etf(TICKER, qb) if 'qb' in dir() else 'XLV'  # HIMS is healthcare\n",
    "print(f\"Sector ETF for {TICKER}: {SECTOR_ETF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6E.1 Market Regime Detection (SPY/QQQ)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARKET REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Run ensemble on market data\n",
    "print(f\"Running ensemble regime detection on {MARKET_PROXY}...\")\n",
    "\n",
    "# Fit individual models on market features\n",
    "gmm_mkt = fit_gmm_regimes(market_features, n_components_range=range(3, 7))\n",
    "kmeans_mkt = fit_kmeans_regimes(market_features, k_range=range(3, 7))\n",
    "agg_mkt = fit_agglomerative_regimes(market_features, n_clusters=4)\n",
    "cpd_mkt = detect_change_points(market_features, method='pelt', penalty=5)\n",
    "\n",
    "# Extract labels\n",
    "gmm_labels_mkt = gmm_mkt['labels']\n",
    "kmeans_labels_mkt = kmeans_mkt['labels']\n",
    "agg_labels_mkt = agg_mkt['labels']\n",
    "cpd_labels_mkt = cpd_mkt['labels']\n",
    "cpd_bkps_mkt = cpd_mkt['breakpoints']\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_mkt, confidence_mkt = ensemble_regime_classification(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    weights=ENSEMBLE_WEIGHTS\n",
    ")\n",
    "\n",
    "# Map to meaningful labels\n",
    "market_regimes = map_regime_labels(ensemble_labels_mkt, market_features, ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"Market regimes identified: {market_regimes.value_counts().to_dict()}\")\n",
    "print(f\"Average confidence: {np.mean(confidence_mkt):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3a: Market Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_probabilities_enhanced(price_data: pd.DataFrame, regime_series: pd.Series,\n",
    "                                       confidence_scores: np.ndarray, cpd_breakpoints: list,\n",
    "                                       title: str, figsize: tuple = (14, 10)):\n",
    "    \"\"\"\n",
    "    Create priority regime visualization with:\n",
    "    - Top panel: Price chart with regime background shading\n",
    "    - Middle panel: Ensemble regime classification\n",
    "    - Bottom panel: Regime confidence/agreement score\n",
    "    - CPD change points as vertical dashed lines\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize, gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "    \n",
    "    # Align data\n",
    "    common_idx = price_data.index.intersection(regime_series.index)\n",
    "    price = price_data.loc[common_idx, 'close']\n",
    "    regimes = regime_series.loc[common_idx]\n",
    "    conf = pd.Series(confidence_scores, index=regime_series.index).reindex(common_idx)\n",
    "    \n",
    "    # ---- Panel 1: Price with regime shading ----\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(price.index, price.values, color='black', linewidth=1.2, label='Price')\n",
    "    \n",
    "    # Add regime background shading\n",
    "    prev_regime = None\n",
    "    segment_start = common_idx[0]\n",
    "    regime_legend = set()\n",
    "    \n",
    "    for i, idx in enumerate(common_idx):\n",
    "        current_regime = regimes.iloc[i]\n",
    "        if current_regime != prev_regime and prev_regime is not None:\n",
    "            color = get_regime_color(prev_regime)\n",
    "            ax1.axvspan(segment_start, idx, alpha=0.3, color=color,\n",
    "                       label=prev_regime if prev_regime not in regime_legend else '')\n",
    "            regime_legend.add(prev_regime)\n",
    "            segment_start = idx\n",
    "        prev_regime = current_regime\n",
    "    \n",
    "    # Final segment\n",
    "    if prev_regime:\n",
    "        color = get_regime_color(prev_regime)\n",
    "        ax1.axvspan(segment_start, common_idx[-1], alpha=0.3, color=color,\n",
    "                   label=prev_regime if prev_regime not in regime_legend else '')\n",
    "    \n",
    "    # Add CPD change points as vertical lines\n",
    "    for bp in cpd_breakpoints:\n",
    "        if bp < len(common_idx):\n",
    "            ax1.axvline(x=common_idx[bp], color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Price', fontsize=11)\n",
    "    ax1.legend(loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ---- Panel 2: Regime classification ----\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Convert regime names to numeric for plotting\n",
    "    regime_map = {name: i for i, name in enumerate(regimes.unique())}\n",
    "    regime_numeric = regimes.map(regime_map)\n",
    "    \n",
    "    for regime_name, regime_num in regime_map.items():\n",
    "        mask = regime_numeric == regime_num\n",
    "        color = get_regime_color(regime_name)\n",
    "        ax2.fill_between(common_idx, 0, 1, where=mask, alpha=0.7, color=color, label=regime_name)\n",
    "    \n",
    "    ax2.set_ylabel('Regime', fontsize=11)\n",
    "    ax2.set_yticks([])\n",
    "    ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
    "    \n",
    "    # ---- Panel 3: Confidence score ----\n",
    "    ax3 = axes[2]\n",
    "    ax3.fill_between(common_idx, 0, conf.values, alpha=0.5, color='#3498db')\n",
    "    ax3.plot(common_idx, conf.values, color='#2c3e50', linewidth=1)\n",
    "    ax3.axhline(y=0.75, color='green', linestyle='--', label='High confidence', alpha=0.7)\n",
    "    ax3.axhline(y=0.50, color='orange', linestyle='--', label='Moderate', alpha=0.7)\n",
    "    \n",
    "    ax3.set_ylabel('Confidence', fontsize=11)\n",
    "    ax3.set_xlabel('Date', fontsize=11)\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    ax3.legend(loc='lower right', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot market regime visualization\n",
    "plot_regime_probabilities_enhanced(\n",
    "    df_market, market_regimes, confidence_mkt, cpd_bkps_mkt,\n",
    "    f'PRIORITY VIZ #3a: {MARKET_PROXY} Market Regime Over Time',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6E.2 Sector Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTOR REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Running ensemble regime detection on sector {SECTOR_ETF}...\")\n",
    "\n",
    "# Fit individual models on sector features\n",
    "gmm_sec = fit_gmm_regimes(sector_features, n_components_range=range(3, 7))\n",
    "kmeans_sec = fit_kmeans_regimes(sector_features, k_range=range(3, 7))\n",
    "agg_sec = fit_agglomerative_regimes(sector_features, n_clusters=4)\n",
    "cpd_sec = detect_change_points(sector_features, method='pelt', penalty=5)\n",
    "\n",
    "# Extract labels\n",
    "gmm_labels_sec = gmm_sec['labels']\n",
    "kmeans_labels_sec = kmeans_sec['labels']\n",
    "agg_labels_sec = agg_sec['labels']\n",
    "cpd_labels_sec = cpd_sec['labels']\n",
    "cpd_bkps_sec = cpd_sec['breakpoints']\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_sec, confidence_sec = ensemble_regime_classification(\n",
    "    gmm_labels_sec, kmeans_labels_sec, agg_labels_sec, cpd_labels_sec,\n",
    "    weights=ENSEMBLE_WEIGHTS\n",
    ")\n",
    "\n",
    "# Map to meaningful labels\n",
    "sector_regimes = map_regime_labels(ensemble_labels_sec, sector_features, ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"Sector regimes identified: {sector_regimes.value_counts().to_dict()}\")\n",
    "print(f\"Average confidence: {np.mean(confidence_sec):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3b: Sector Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate plot for sector (NOT multi-panel)\n",
    "plot_regime_probabilities_enhanced(\n",
    "    df_sector, sector_regimes, confidence_sec, cpd_bkps_sec,\n",
    "    f'PRIORITY VIZ #3b: {SECTOR_ETF} Sector Regime Over Time (with {MARKET_PROXY} overlay)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6E.3 Asset-Specific Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ASSET REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Running ensemble regime detection on {TICKER}...\")\n",
    "\n",
    "# Fit individual models on asset features\n",
    "gmm_ast = fit_gmm_regimes(asset_features, n_components_range=range(3, 7))\n",
    "kmeans_ast = fit_kmeans_regimes(asset_features, k_range=range(3, 7))\n",
    "agg_ast = fit_agglomerative_regimes(asset_features, n_clusters=4)\n",
    "cpd_ast = detect_change_points(asset_features, method='pelt', penalty=5)\n",
    "\n",
    "# Extract labels\n",
    "gmm_labels_ast = gmm_ast['labels']\n",
    "kmeans_labels_ast = kmeans_ast['labels']\n",
    "agg_labels_ast = agg_ast['labels']\n",
    "cpd_labels_ast = cpd_ast['labels']\n",
    "cpd_bkps_ast = cpd_ast['breakpoints']\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_ast, confidence_ast = ensemble_regime_classification(\n",
    "    gmm_labels_ast, kmeans_labels_ast, agg_labels_ast, cpd_labels_ast,\n",
    "    weights=ENSEMBLE_WEIGHTS\n",
    ")\n",
    "\n",
    "# Map to meaningful labels focusing on:\n",
    "# - Strong upward momentum\n",
    "# - Moderate downward drift\n",
    "# - Sideways/range-bound\n",
    "# - High volatility trending\n",
    "asset_regimes = map_regime_labels(ensemble_labels_ast, asset_features, ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"Asset regimes identified: {asset_regimes.value_counts().to_dict()}\")\n",
    "print(f\"Average confidence: {np.mean(confidence_ast):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3c: Asset Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate plot for asset with market and sector overlay\n",
    "plot_regime_probabilities_enhanced(\n",
    "    df_asset, asset_regimes, confidence_ast, cpd_bkps_ast,\n",
    "    f'PRIORITY VIZ #3c: {TICKER} Asset Regime Over Time (Multi-level Overlay)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REGIME STABILITY & TRANSITION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_regime_transitions(regime_series: pd.Series, title: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate transition probability matrix and average duration.\n",
    "    \"\"\"\n",
    "    transitions = calculate_regime_transitions(regime_series)\n",
    "    \n",
    "    # Calculate average duration in each regime\n",
    "    regimes = regime_series.values\n",
    "    unique_regimes = np.unique(regimes)\n",
    "    durations = {r: [] for r in unique_regimes}\n",
    "    \n",
    "    current_regime = regimes[0]\n",
    "    current_duration = 1\n",
    "    \n",
    "    for i in range(1, len(regimes)):\n",
    "        if regimes[i] == current_regime:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            durations[current_regime].append(current_duration)\n",
    "            current_regime = regimes[i]\n",
    "            current_duration = 1\n",
    "    durations[current_regime].append(current_duration)\n",
    "    \n",
    "    avg_durations = {r: np.mean(d) if d else 0 for r, d in durations.items()}\n",
    "    \n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nTransition Probability Matrix:\")\n",
    "    print(transitions.round(2))\n",
    "    print(\"\\nAverage Duration by Regime (days):\")\n",
    "    for regime, dur in avg_durations.items():\n",
    "        print(f\"  {regime}: {dur:.1f} days\")\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "\n",
    "# Analyze transitions for all three levels\n",
    "market_transitions = analyze_regime_transitions(market_regimes, f'{MARKET_PROXY} Market Transitions')\n",
    "sector_transitions = analyze_regime_transitions(sector_regimes, f'{SECTOR_ETF} Sector Transitions')\n",
    "asset_transitions = analyze_regime_transitions(asset_regimes, f'{TICKER} Asset Transitions')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CROSS-LEVEL REGIME RELATIONSHIPS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_level_relationships(market_regimes: pd.Series, sector_regimes: pd.Series,\n",
    "                                      asset_regimes: pd.Series):\n",
    "    \"\"\"\n",
    "    Analyze how Market regimes influence Sector and Asset regimes.\n",
    "    \"\"\"\n",
    "    # Align all series to common index\n",
    "    common_idx = market_regimes.index.intersection(sector_regimes.index).intersection(asset_regimes.index)\n",
    "    \n",
    "    mkt = market_regimes.loc[common_idx]\n",
    "    sec = sector_regimes.loc[common_idx]\n",
    "    ast = asset_regimes.loc[common_idx]\n",
    "    \n",
    "    # Create contingency tables\n",
    "    print(\"\\nMARKET \u2192 SECTOR Regime Contingency:\")\n",
    "    print(pd.crosstab(mkt, sec, normalize='index').round(2))\n",
    "    \n",
    "    print(\"\\nMARKET \u2192 ASSET Regime Contingency:\")\n",
    "    print(pd.crosstab(mkt, ast, normalize='index').round(2))\n",
    "    \n",
    "    print(\"\\nSECTOR \u2192 ASSET Regime Contingency:\")\n",
    "    print(pd.crosstab(sec, ast, normalize='index').round(2))\n",
    "    \n",
    "    # Chi-square tests for independence\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    tests = [\n",
    "        ('Market-Sector', pd.crosstab(mkt, sec)),\n",
    "        ('Market-Asset', pd.crosstab(mkt, ast)),\n",
    "        ('Sector-Asset', pd.crosstab(sec, ast)),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChi-Square Independence Tests:\")\n",
    "    for name, table in tests:\n",
    "        chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "        print(f\"  {name}: \u03c7\u00b2 = {chi2:.1f}, p-value = {p_value:.4f} {'***' if p_value < 0.01 else '**' if p_value < 0.05 else ''}\")\n",
    "\n",
    "\n",
    "analyze_cross_level_relationships(market_regimes, sector_regimes, asset_regimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Optimal Strategy Selection & Output\n",
    "\n",
    "This section synthesizes all analysis to determine optimal strategy allocation by regime.\n",
    "\n",
    "### 7.1 Strategy Performance by Regime Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: OPTIMAL STRATEGY SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create combined regime DataFrame\n",
    "common_idx = market_regimes.index.intersection(sector_regimes.index).intersection(asset_regimes.index)\n",
    "common_idx = common_idx.intersection(vol_regime.dropna().index)\n",
    "common_idx = common_idx.intersection(strategy_returns_df.index)\n",
    "\n",
    "regime_df = pd.DataFrame({\n",
    "    'market_regime': market_regimes.loc[common_idx],\n",
    "    'sector_regime': sector_regimes.loc[common_idx],\n",
    "    'asset_regime': asset_regimes.loc[common_idx],\n",
    "    'vol_regime': vol_regime.loc[common_idx],\n",
    "    'market_confidence': pd.Series(confidence_mkt, index=market_regimes.index).loc[common_idx],\n",
    "    'sector_confidence': pd.Series(confidence_sec, index=sector_regimes.index).loc[common_idx],\n",
    "    'asset_confidence': pd.Series(confidence_ast, index=asset_regimes.index).loc[common_idx]\n",
    "}, index=common_idx)\n",
    "\n",
    "# Add strategy returns\n",
    "for strategy in STRATEGIES:\n",
    "    regime_df[f'{strategy}_return'] = strategy_returns_df[strategy].loc[common_idx]\n",
    "\n",
    "print(f\"Combined regime data: {len(regime_df)} observations\")\n",
    "print(f\"\\nRegime combinations:\")\n",
    "combo_counts = regime_df.groupby(['market_regime', 'sector_regime']).size()\n",
    "print(combo_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY PERFORMANCE BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_performance(regime_df: pd.DataFrame, strategies: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate strategy performance metrics by regime combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by market and sector regime\n",
    "    groups = regime_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt_regime, sec_regime), group in groups:\n",
    "        for strategy in strategies:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in group.columns:\n",
    "                returns = group[col].dropna()\n",
    "                \n",
    "                if len(returns) >= 10:  # Minimum sample size\n",
    "                    total_return = (1 + returns).prod() - 1\n",
    "                    avg_return = returns.mean()\n",
    "                    vol = returns.std()\n",
    "                    sharpe = (avg_return * 252) / (vol * np.sqrt(252)) if vol > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    # Drawdown\n",
    "                    cumulative = (1 + returns).cumprod()\n",
    "                    running_max = cumulative.expanding().max()\n",
    "                    max_dd = ((cumulative - running_max) / running_max).min()\n",
    "                    \n",
    "                    # Average confidence\n",
    "                    avg_conf = (group['market_confidence'] + group['sector_confidence']).mean() / 2\n",
    "                    \n",
    "                    results.append({\n",
    "                        'market_regime': mkt_regime,\n",
    "                        'sector_regime': sec_regime,\n",
    "                        'strategy': strategy,\n",
    "                        'avg_return': avg_return,\n",
    "                        'total_return': total_return,\n",
    "                        'sharpe': sharpe,\n",
    "                        'win_rate': win_rate,\n",
    "                        'max_drawdown': max_dd,\n",
    "                        'sample_size': len(returns),\n",
    "                        'confidence': avg_conf\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "regime_performance = calculate_regime_performance(regime_df, STRATEGIES)\n",
    "\n",
    "print(\"Strategy Performance by Regime:\")\n",
    "print(regime_performance.sort_values('sharpe', ascending=False).head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 PRIORITY VISUALIZATION #4: Strategy Performance Heatmaps by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #4: Performance Heatmaps by Strategy\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_performance_heatmap(perf_df: pd.DataFrame, strategy: str):\n",
    "    \"\"\"\n",
    "    Create heatmap showing strategy performance across regime combinations.\n",
    "    \"\"\"\n",
    "    strategy_data = perf_df[perf_df['strategy'] == strategy]\n",
    "    \n",
    "    if strategy_data.empty:\n",
    "        print(f\"No data for {strategy}\")\n",
    "        return\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    pivot_return = strategy_data.pivot(index='market_regime', columns='sector_regime', \n",
    "                                        values='avg_return').fillna(0)\n",
    "    pivot_winrate = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                         values='win_rate').fillna(0)\n",
    "    pivot_sample = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                        values='sample_size').fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIG_SIZE_HEATMAP)\n",
    "    \n",
    "    # Main heatmap with returns\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Use diverging colormap\n",
    "    vmax = max(abs(pivot_return.values.min()), abs(pivot_return.values.max()))\n",
    "    vmin = -vmax\n",
    "    \n",
    "    sns.heatmap(pivot_return * 100, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, center=0, vmin=vmin*100, vmax=vmax*100,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Avg Daily Return (%)'})\n",
    "    \n",
    "    # Add sample size annotations\n",
    "    for i in range(len(pivot_return.index)):\n",
    "        for j in range(len(pivot_return.columns)):\n",
    "            n = pivot_sample.iloc[i, j]\n",
    "            wr = pivot_winrate.iloc[i, j]\n",
    "            if n > 0:\n",
    "                ax.annotate(f'n={int(n)}\\nWR={wr:.0%}',\n",
    "                           xy=(j + 0.5, i + 0.7),\n",
    "                           ha='center', va='center',\n",
    "                           fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Performance by Regime Combination', \n",
    "                 fontsize=14, color=color, fontweight='bold')\n",
    "    ax.set_xlabel('Sector Regime')\n",
    "    ax.set_ylabel('Market Regime')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create separate heatmaps for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    plot_regime_performance_heatmap(regime_performance, strategy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Optimal Strategy Selection by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMAL STRATEGY BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_strategy(perf_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the optimal strategy for each regime combination.\n",
    "    \"\"\"\n",
    "    # For each regime combination, find best strategy by Sharpe\n",
    "    optimal = []\n",
    "    \n",
    "    groups = perf_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt, sec), group in groups:\n",
    "        if len(group) > 0:\n",
    "            best = group.loc[group['sharpe'].idxmax()]\n",
    "            \n",
    "            # Also find optimal stop loss (simplified - use best overall for now)\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best['strategy']]\n",
    "            if len(stop_data) > 0:\n",
    "                best_stop = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                best_stop = 'None'\n",
    "            \n",
    "            optimal.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Recommended_Strategy': best['strategy'],\n",
    "                'Expected_Return': best['avg_return'],\n",
    "                'Sharpe': best['sharpe'],\n",
    "                'Max_DD': best['max_drawdown'],\n",
    "                'Win_Rate': best['win_rate'],\n",
    "                'Sample_Size': best['sample_size'],\n",
    "                'Ensemble_Confidence': best['confidence'],\n",
    "                'Stop_Loss_%': best_stop\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(optimal)\n",
    "\n",
    "\n",
    "optimal_strategies = find_optimal_strategy(regime_performance)\n",
    "\n",
    "print(\"OPTIMAL STRATEGY ALLOCATION BY REGIME:\")\n",
    "print(\"=\"*80)\n",
    "print(optimal_strategies.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE OUTPUT TABLE\n",
    "# ============================================================================\n",
    "\n",
    "# Add volatility regime and asset regime for complete picture\n",
    "def create_comprehensive_recommendations() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive strategy recommendations table.\n",
    "    \"\"\"\n",
    "    # Get unique regime combinations\n",
    "    combos = regime_df.groupby(['market_regime', 'sector_regime', 'asset_regime', 'vol_regime']).size()\n",
    "    combos = combos[combos >= 5].reset_index(name='sample_size')  # Min 5 days\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for _, row in combos.iterrows():\n",
    "        mkt, sec, ast, vol = row['market_regime'], row['sector_regime'], row['asset_regime'], row['vol_regime']\n",
    "        \n",
    "        # Filter data for this regime combination\n",
    "        mask = (\n",
    "            (regime_df['market_regime'] == mkt) &\n",
    "            (regime_df['sector_regime'] == sec) &\n",
    "            (regime_df['asset_regime'] == ast) &\n",
    "            (regime_df['vol_regime'] == vol)\n",
    "        )\n",
    "        subset = regime_df[mask]\n",
    "        \n",
    "        if len(subset) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Find best strategy\n",
    "        best_strategy = None\n",
    "        best_sharpe = -np.inf\n",
    "        best_return = 0\n",
    "        best_winrate = 0\n",
    "        \n",
    "        for strategy in STRATEGIES:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in subset.columns:\n",
    "                returns = subset[col].dropna()\n",
    "                if len(returns) >= 5:\n",
    "                    avg_ret = returns.mean()\n",
    "                    vol_ret = returns.std()\n",
    "                    sharpe = (avg_ret * 252) / (vol_ret * np.sqrt(252)) if vol_ret > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    if sharpe > best_sharpe:\n",
    "                        best_sharpe = sharpe\n",
    "                        best_strategy = strategy\n",
    "                        best_return = avg_ret\n",
    "                        best_winrate = win_rate\n",
    "        \n",
    "        if best_strategy:\n",
    "            # Determine delta based on strategy type\n",
    "            call_delta = 'N/A'\n",
    "            put_delta = 'N/A'\n",
    "            \n",
    "            if 'Call' in best_strategy:\n",
    "                call_delta = '0.40'\n",
    "            if 'Put' in best_strategy:\n",
    "                put_delta = '-0.20'\n",
    "            if best_strategy in ['Straddle', 'Strangle']:\n",
    "                call_delta = '0.30'\n",
    "                put_delta = '-0.30'\n",
    "            if 'Calendar' in best_strategy or 'Diagonal' in best_strategy:\n",
    "                call_delta = '0.50'\n",
    "            \n",
    "            # Get recommended stop loss\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best_strategy]\n",
    "            if len(stop_data) > 0:\n",
    "                stop_loss = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                stop_loss = 'None'\n",
    "            \n",
    "            # Average confidence\n",
    "            avg_conf = subset[['market_confidence', 'sector_confidence', 'asset_confidence']].mean().mean()\n",
    "            \n",
    "            recommendations.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Regime_Asset': ast,\n",
    "                'Volatility_Regime': vol,\n",
    "                'Ensemble_Confidence': f'{avg_conf:.2f}',\n",
    "                'Recommended_Strategy': best_strategy,\n",
    "                'Call_Delta': call_delta,\n",
    "                'Put_Delta': put_delta,\n",
    "                'Stop_Loss_%': stop_loss,\n",
    "                'Expected_Return': f'{best_return*100:.2f}%',\n",
    "                'Sharpe': f'{best_sharpe:.2f}',\n",
    "                'Win_Rate': f'{best_winrate:.0%}',\n",
    "                'Sample_Size': len(subset)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "\n",
    "comprehensive_recommendations = create_comprehensive_recommendations()\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE STRATEGY RECOMMENDATIONS:\")\n",
    "print(\"=\"*120)\n",
    "print(comprehensive_recommendations.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# comprehensive_recommendations.to_csv('strategy_recommendations.csv', index=False)\n",
    "# print(\"\\nRecommendations saved to strategy_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Current Regime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CURRENT REGIME ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Get current (most recent) regime states\n",
    "current_market = market_regimes.iloc[-1]\n",
    "current_sector = sector_regimes.iloc[-1]\n",
    "current_asset = asset_regimes.iloc[-1]\n",
    "current_vol = vol_regime.iloc[-1] if len(vol_regime) > 0 else 'Unknown'\n",
    "\n",
    "current_market_conf = confidence_mkt[-1] if len(confidence_mkt) > 0 else 0\n",
    "current_sector_conf = confidence_sec[-1] if len(confidence_sec) > 0 else 0\n",
    "current_asset_conf = confidence_ast[-1] if len(confidence_ast) > 0 else 0\n",
    "\n",
    "print(\"CURRENT REGIME STATES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {regime_df.index[-1]}\")\n",
    "print(f\"\\nMarket Regime: {current_market} (Confidence: {current_market_conf:.2%})\")\n",
    "print(f\"Sector Regime: {current_sector} (Confidence: {current_sector_conf:.2%})\")\n",
    "print(f\"Asset Regime:  {current_asset} (Confidence: {current_asset_conf:.2%})\")\n",
    "print(f\"Volatility:    {current_vol}\")\n",
    "\n",
    "# Find recommended strategy for current regime\n",
    "current_rec = comprehensive_recommendations[\n",
    "    (comprehensive_recommendations['Regime_Market'] == current_market) &\n",
    "    (comprehensive_recommendations['Regime_Sector'] == current_sector)\n",
    "]\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    print(\"\\nRECOMMENDED STRATEGY FOR CURRENT REGIME:\")\n",
    "    print(\"-\"*60)\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Call Delta: {rec['Call_Delta']}\")\n",
    "    print(f\"Put Delta: {rec['Put_Delta']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "    print(f\"Expected Return: {rec['Expected_Return']}\")\n",
    "    print(f\"Sharpe Ratio: {rec['Sharpe']}\")\n",
    "    print(f\"Win Rate: {rec['Win_Rate']}\")\n",
    "else:\n",
    "    print(\"\\nNo historical data for current regime combination.\")\n",
    "    print(\"Consider using the most similar regime or conservative positioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 PRIORITY VISUALIZATION #5: Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #5: Summary Dashboard\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=FIG_SIZE_DASHBOARD)\n",
    "\n",
    "# Create 3x3 grid\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Current regime states\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "regimes = ['Market', 'Sector', 'Asset']\n",
    "regime_values = [current_market, current_sector, current_asset]\n",
    "confidence_values = [current_market_conf, current_sector_conf, current_asset_conf]\n",
    "colors = [get_regime_color(r) for r in regime_values]\n",
    "\n",
    "bars = ax1.barh(regimes, confidence_values, color=colors, alpha=0.7)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_xlabel('Confidence')\n",
    "ax1.set_title('Current Regime States', fontsize=12)\n",
    "\n",
    "for bar, val, regime in zip(bars, confidence_values, regime_values):\n",
    "    ax1.annotate(f'{regime}\\n{val:.0%}', \n",
    "                xy=(val + 0.02, bar.get_y() + bar.get_height()/2),\n",
    "                va='center', fontsize=9)\n",
    "\n",
    "# Panel 2: Volatility distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "vol_hist = realized_vol['rv_true'].dropna()\n",
    "current_vol_value = vol_hist.iloc[-1] if len(vol_hist) > 0 else 0\n",
    "\n",
    "ax2.hist(vol_hist, bins=30, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "ax2.axvline(x=current_vol_value, color='red', linewidth=2, label=f'Current: {current_vol_value:.2%}')\n",
    "ax2.axvline(x=vol_hist.mean(), color='green', linestyle='--', label=f'Mean: {vol_hist.mean():.2%}')\n",
    "ax2.set_title('Volatility Distribution', fontsize=12)\n",
    "ax2.set_xlabel('Volatility')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Panel 3: Recommended strategy\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    strategy_color = get_strategy_color(rec['Recommended_Strategy'])\n",
    "    \n",
    "    text = f\"RECOMMENDED STRATEGY\\n\\n\"\n",
    "    text += f\"{rec['Recommended_Strategy']}\\n\\n\"\n",
    "    text += f\"Stop Loss: {rec['Stop_Loss_%']}\\n\"\n",
    "    text += f\"Expected Return: {rec['Expected_Return']}\\n\"\n",
    "    text += f\"Sharpe: {rec['Sharpe']}\"\n",
    "    \n",
    "    ax3.text(0.5, 0.5, text, ha='center', va='center', fontsize=11,\n",
    "             transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor=strategy_color, alpha=0.3))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No recommendation\\nfor current regime', \n",
    "             ha='center', va='center', fontsize=11, transform=ax3.transAxes)\n",
    "\n",
    "ax3.set_title('Strategy Recommendation', fontsize=12)\n",
    "\n",
    "# Panel 4: Performance metrics table\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    table_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Win Rate', rec['Win_Rate']],\n",
    "        ['Sharpe', rec['Sharpe']],\n",
    "        ['Sample Size', str(rec['Sample_Size'])],\n",
    "        ['Confidence', rec['Ensemble_Confidence']]\n",
    "    ]\n",
    "    \n",
    "    table = ax4.table(cellText=table_data, loc='center', cellLoc='center',\n",
    "                      colWidths=[0.4, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "\n",
    "ax4.set_title('Expected Performance', fontsize=12)\n",
    "\n",
    "# Panel 5: Historical performance of recommended strategy\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec_strategy = current_rec.iloc[0]['Recommended_Strategy']\n",
    "    strategy_returns_series = strategy_returns_df[rec_strategy]\n",
    "    cumulative = (1 + strategy_returns_series).cumprod()\n",
    "    \n",
    "    color = get_strategy_color(rec_strategy)\n",
    "    ax5.plot(cumulative, color=color, linewidth=1.5)\n",
    "    ax5.set_title(f'{rec_strategy} Historical Performance', fontsize=12)\n",
    "    ax5.set_ylabel('Cumulative Return')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 6: Risk warnings\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.axis('off')\n",
    "\n",
    "# Check for warnings\n",
    "warnings = []\n",
    "avg_conf = (current_market_conf + current_sector_conf + current_asset_conf) / 3\n",
    "\n",
    "if avg_conf < 0.6:\n",
    "    warnings.append(\"\u26a0\ufe0f Low regime confidence\")\n",
    "if current_vol == 'Extreme':\n",
    "    warnings.append(\"\u26a0\ufe0f Extreme volatility detected\")\n",
    "if len(current_rec) == 0:\n",
    "    warnings.append(\"\u26a0\ufe0f Novel regime combination\")\n",
    "if len(current_rec) > 0 and int(current_rec.iloc[0]['Sample_Size']) < 20:\n",
    "    warnings.append(\"\u26a0\ufe0f Limited historical data\")\n",
    "\n",
    "if warnings:\n",
    "    warning_text = \"RISK WARNINGS\\n\\n\" + \"\\n\".join(warnings)\n",
    "    ax6.text(0.5, 0.5, warning_text, ha='center', va='center', fontsize=10,\n",
    "             transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.5))\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, '\u2705 No significant warnings\\n\\nRegime stable with\\nhigh confidence',\n",
    "             ha='center', va='center', fontsize=10, transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.5))\n",
    "\n",
    "ax6.set_title('Risk Alerts', fontsize=12)\n",
    "\n",
    "# Panel 7: Method agreement pie chart\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "# Calculate method agreement\n",
    "agreement_levels = {\n",
    "    'High (75%+)': (confidence_mkt >= 0.75).mean(),\n",
    "    'Moderate (50-75%)': ((confidence_mkt >= 0.5) & (confidence_mkt < 0.75)).mean(),\n",
    "    'Low (<50%)': (confidence_mkt < 0.5).mean()\n",
    "}\n",
    "\n",
    "colors_pie = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax7.pie([v for v in agreement_levels.values()], labels=agreement_levels.keys(),\n",
    "        autopct='%1.0f%%', colors=colors_pie, startangle=90)\n",
    "ax7.set_title('Ensemble Agreement Distribution', fontsize=12)\n",
    "\n",
    "# Panel 8: PCA feature space (simplified 2D)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "# Quick PCA for visualization\n",
    "scaler_viz = StandardScaler()\n",
    "X_viz = scaler_viz.fit_transform(asset_features.dropna())\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca = pca_viz.fit_transform(X_viz)\n",
    "\n",
    "# Color by regime\n",
    "colors_scatter = [get_regime_color(r) for r in asset_regimes.loc[asset_features.dropna().index]]\n",
    "ax8.scatter(X_pca[:, 0], X_pca[:, 1], c=colors_scatter, alpha=0.5, s=10)\n",
    "ax8.scatter(X_pca[-1, 0], X_pca[-1, 1], c='black', s=100, marker='*', \n",
    "            label='Current', edgecolors='white', linewidth=1)\n",
    "ax8.set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.0%})')\n",
    "ax8.set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.0%})')\n",
    "ax8.set_title('Feature Space Position', fontsize=12)\n",
    "ax8.legend(loc='upper right', fontsize=9)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 9: Regime transition probabilities\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "# Get transition probabilities from current regime\n",
    "if current_vol in transition_probs.index:\n",
    "    trans = transition_probs.loc[current_vol]\n",
    "    colors_bar = [get_vol_regime_color(r) for r in trans.index]\n",
    "    ax9.bar(trans.index, trans.values, color=colors_bar, alpha=0.7)\n",
    "    ax9.set_ylim(0, 1)\n",
    "    ax9.set_ylabel('Probability')\n",
    "    ax9.set_title(f'Next Regime Probability\\n(from {current_vol})', fontsize=12)\n",
    "    ax9.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax9.text(0.5, 0.5, 'Transition data\\nnot available', \n",
    "             ha='center', va='center', transform=ax9.transAxes)\n",
    "    ax9.set_title('Regime Transition', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'{TICKER} - Volatility Trading Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Portfolio Extension Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PORTFOLIO EXTENSION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "# This section provides structure for future multi-asset portfolio optimization\n",
    "\n",
    "class VolatilityTradingPortfolio:\n",
    "    \"\"\"\n",
    "    Framework for multi-asset volatility trading portfolio.\n",
    "    \n",
    "    Designed to be extended for portfolio-level optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tickers: List[str]):\n",
    "        self.tickers = tickers\n",
    "        self.asset_data = {}\n",
    "        self.regime_data = {}\n",
    "        self.strategy_allocations = {}\n",
    "        \n",
    "    def add_asset(self, ticker: str, data: pd.DataFrame, regimes: pd.Series):\n",
    "        \"\"\"Add asset data to portfolio.\"\"\"\n",
    "        self.asset_data[ticker] = data\n",
    "        self.regime_data[ticker] = regimes\n",
    "        \n",
    "    def calculate_cross_asset_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate correlation matrix across assets.\"\"\"\n",
    "        returns_df = pd.DataFrame()\n",
    "        for ticker, data in self.asset_data.items():\n",
    "            returns_df[ticker] = data['returns'] if 'returns' in data.columns else data['close'].pct_change()\n",
    "        return returns_df.corr()\n",
    "    \n",
    "    def calculate_portfolio_greeks(self, positions: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate aggregate portfolio Greeks.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        positions : dict\n",
    "            Dictionary of position sizes by ticker and strategy\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Aggregate delta, gamma, theta, vega\n",
    "        \"\"\"\n",
    "        # Placeholder for portfolio-level Greek calculations\n",
    "        return {\n",
    "            'portfolio_delta': 0,\n",
    "            'portfolio_gamma': 0,\n",
    "            'portfolio_theta': 0,\n",
    "            'portfolio_vega': 0\n",
    "        }\n",
    "    \n",
    "    def check_regime_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Check if assets share regimes simultaneously.\"\"\"\n",
    "        regime_df = pd.DataFrame()\n",
    "        for ticker, regimes in self.regime_data.items():\n",
    "            regime_df[ticker] = pd.Categorical(regimes).codes\n",
    "        return regime_df.corr()\n",
    "    \n",
    "    def optimize_allocation(self, constraints: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Optimize strategy allocation across assets.\n",
    "        \n",
    "        Placeholder for future implementation of:\n",
    "        - Mean-variance optimization\n",
    "        - Risk parity\n",
    "        - Regime-based allocation\n",
    "        \"\"\"\n",
    "        # Placeholder - to be implemented\n",
    "        return {ticker: {'strategy': None, 'weight': 0} for ticker in self.tickers}\n",
    "\n",
    "\n",
    "# Example usage (for future expansion)\n",
    "print(\"Portfolio Extension Framework initialized.\")\n",
    "print(\"\")\n",
    "print(\"Key methods for future implementation:\")\n",
    "print(\"  - add_asset(): Add multiple assets to portfolio\")\n",
    "print(\"  - calculate_cross_asset_correlation(): Diversification analysis\")\n",
    "print(\"  - calculate_portfolio_greeks(): Aggregate risk exposure\")\n",
    "print(\"  - check_regime_correlation(): Regime synchronization\")\n",
    "print(\"  - optimize_allocation(): Multi-asset strategy allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions\n",
    "\n",
    "This notebook has implemented a comprehensive volatility trading research framework including:\n",
    "\n",
    "### Key Components:\n",
    "1. **Volatility Modeling**: Multiple models (Historical, EWMA, Parkinson, Garman-Klass, Rogers-Satchell, ATR) with PCA-based aggregate forecasting\n",
    "\n",
    "2. **Mean Reversion Analysis**: Volatility regime classification with transition probabilities and DTE recommendations\n",
    "\n",
    "3. **Options Strategies**: Six strategies (Calendar Spread, Double Diagonal, Straddle, Strangle, Bull Put Spread, Bull Call Spread) with delta optimization\n",
    "\n",
    "4. **Ensemble Regime Detection**: Combined GMM, K-Means, Agglomerative Clustering, and Change-Point Detection for robust regime identification at Market, Sector, and Asset levels\n",
    "\n",
    "5. **Stop Loss Optimization**: Systematic testing of stop loss thresholds by strategy\n",
    "\n",
    "6. **Comprehensive Output**: Machine-readable recommendations table with regime-specific strategy allocation\n",
    "\n",
    "### Technical Implementation:\n",
    "- All computations use vectorized operations (NumPy/Pandas)\n",
    "- No multiprocessing (QuantConnect compatible)\n",
    "- Ticker-agnostic design for easy portfolio extension\n",
    "- Consistent color palettes across all visualizations\n",
    "\n",
    "### Next Steps:\n",
    "1. Connect to live QuantConnect options data\n",
    "2. Implement actual options pricing models\n",
    "3. Extend to multi-asset portfolio optimization\n",
    "4. Add real-time regime monitoring\n",
    "5. Backtest with actual transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# END OF NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VOLATILITY TRADING RESEARCH NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Ticker analyzed: {TICKER}\")\n",
    "print(f\"Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"\")\n",
    "print(f\"Current Regimes:\")\n",
    "print(f\"  Market: {current_market}\")\n",
    "print(f\"  Sector: {current_sector}\")\n",
    "print(f\"  Asset:  {current_asset}\")\n",
    "print(f\"  Volatility: {current_vol}\")\n",
    "print(f\"\")\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Recommended Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "else:\n",
    "    print(\"No specific recommendation for current regime.\")\n",
    "\n",
    "print(f\"\")\n",
    "print(\"See comprehensive_recommendations DataFrame for full regime-strategy mapping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}