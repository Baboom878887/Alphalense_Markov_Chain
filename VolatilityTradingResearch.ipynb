{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>\n",
    "\n",
    "# Volatility Trading Research Notebook\n",
    "\n",
    "## Options Strategies with Ensemble Regime Detection\n",
    "\n",
    "This notebook implements a comprehensive volatility trading research framework for QuantConnect.\n",
    "\n",
    "**Key Features:**\n",
    "- Multiple volatility models with PCA-based aggregate forecasting\n",
    "- Ensemble regime detection (GMM, K-Means, Agglomerative Clustering, Change-Point Detection)\n",
    "- Six options strategies with delta optimization\n",
    "- Stop loss optimization per strategy and regime\n",
    "- Vectorized computations (no multiprocessing)\n",
    "\n",
    "**Test Ticker:** HIMS (ticker-agnostic design for future portfolio expansion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Initial Setup & Structure\n",
    "\n",
    "### 1.1 Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: INITIAL SETUP & STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "# Standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import json\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Machine Learning - Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Change-Point Detection\n",
    "try:\n",
    "    import ruptures as rpt\n",
    "    RUPTURES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RUPTURES_AVAILABLE = False\n",
    "    print(\"Warning: ruptures library not available. Install with: pip install ruptures\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Warning: plotly not available for interactive charts\")\n",
    "\n",
    "# Set plotting defaults\n",
    "try:\n    plt.style.use('seaborn-v0_8-whitegrid')\nexcept:\n    try:\n        plt.style.use('seaborn-whitegrid')\n    except:\n        plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# QuantConnect Research Environment\n",
    "qb = QuantBook()\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Ruptures available: {RUPTURES_AVAILABLE}\")\n",
    "print(f\"Plotly available: {PLOTLY_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Color Palette Definitions\n",
    "\n",
    "**CRITICAL:** These color mappings must be used consistently across ALL visualizations throughout the notebook.\n",
    "\n",
    "#### Strategy Colors\n",
    "| Strategy | Color | Hex Code |\n",
    "|----------|-------|----------|\n",
    "| Calendar Spread | Blue | #1f77b4 |\n",
    "| Double Diagonal | Orange | #ff7f0e |\n",
    "| Straddle | Green | #2ca02c |\n",
    "| Strangle | Red | #d62728 |\n",
    "| Bull Put Spread | Purple | #9467bd |\n",
    "| Bull Call Spread | Brown | #8c564b |\n",
    "\n",
    "#### Regime Colors\n",
    "| Regime | Color | Hex Code |\n",
    "|--------|-------|----------|\n",
    "| Bull_Low_Vol | Green | #2ecc71 |\n",
    "| Bull_High_Vol | Orange | #f39c12 |\n",
    "| Bear | Red | #e74c3c |\n",
    "| Choppy | Gray | #95a5a6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLOR PALETTE DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "STRATEGY_COLORS = {\n",
    "    'Calendar Spread': '#1f77b4',    # Blue\n",
    "    'Double Diagonal': '#ff7f0e',    # Orange\n",
    "    'Straddle': '#2ca02c',           # Green\n",
    "    'Strangle': '#d62728',           # Red\n",
    "    'Bull Put Spread': '#9467bd',    # Purple\n",
    "    'Bull Call Spread': '#8c564b',   # Brown\n",
    "}\n",
    "\n",
    "# Regime color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "REGIME_COLORS = {\n",
    "    'Bull_Low_Vol': '#2ecc71',       # Green\n",
    "    'Bull_High_Vol': '#f39c12',      # Orange\n",
    "    'Bear': '#e74c3c',               # Red\n",
    "    'Choppy': '#95a5a6',             # Gray\n",
    "    'Sideways': '#95a5a6',           # Gray (alias)\n",
    "    'Unknown': '#bdc3c7',            # Light Gray\n",
    "}\n",
    "\n",
    "# Volatility regime colors\n",
    "VOL_REGIME_COLORS = {\n",
    "    'Low': '#3498db',                # Light Blue\n",
    "    'Normal': '#2ecc71',             # Green\n",
    "    'Elevated': '#f39c12',           # Orange\n",
    "    'Extreme': '#e74c3c',            # Red\n",
    "}\n",
    "\n",
    "# Helper functions for consistent color access\n",
    "def get_strategy_color(strategy_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a strategy.\"\"\"\n",
    "    return STRATEGY_COLORS.get(strategy_name, '#7f7f7f')  # Default gray\n",
    "\n",
    "def get_regime_color(regime_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a regime.\"\"\"\n",
    "    return REGIME_COLORS.get(regime_name, '#bdc3c7')  # Default light gray\n",
    "\n",
    "def get_vol_regime_color(vol_regime: str) -> str:\n",
    "    \"\"\"Get consistent color for a volatility regime.\"\"\"\n",
    "    return VOL_REGIME_COLORS.get(vol_regime, '#bdc3c7')\n",
    "\n",
    "# Display color palettes\n",
    "print(\"Strategy Colors:\")\n",
    "for strategy, color in STRATEGY_COLORS.items():\n",
    "    print(f\"  {strategy}: {color}\")\n",
    "\n",
    "print(\"\\nRegime Colors:\")\n",
    "for regime, color in REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")\n",
    "\n",
    "print(\"\\nVolatility Regime Colors:\")\n",
    "for regime, color in VOL_REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Primary ticker for analysis (ticker-agnostic design)\n",
    "TICKER = 'HIMS'\n",
    "\n",
    "# Market and sector proxies\n",
    "MARKET_PROXY = 'SPY'\n",
    "SECTOR_MAPPING = {\n",
    "    'Technology': 'XLK',\n",
    "    'Healthcare': 'XLV',\n",
    "    'Financial': 'XLF',\n",
    "    'Consumer Discretionary': 'XLY',\n",
    "    'Consumer Staples': 'XLP',\n",
    "    'Energy': 'XLE',\n",
    "    'Utilities': 'XLU',\n",
    "    'Materials': 'XLB',\n",
    "    'Industrials': 'XLI',\n",
    "    'Real Estate': 'XLRE',\n",
    "    'Communication Services': 'XLC',\n",
    "}\n",
    "\n",
    "# Default sector ETF (HIMS is healthcare)\n",
    "SECTOR_ETF = 'XLV'\n",
    "\n",
    "# Date range for analysis (minimum 2-3 years recommended)\n",
    "START_DATE = datetime(2022, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "\n",
    "# Volatility calculation windows\n",
    "VOL_WINDOWS = [5, 10, 20, 30]\n",
    "\n",
    "# PCA configuration\n",
    "PCA_LAG_PERIODS = [1, 2, 3, 5, 10, 20]\n",
    "PCA_N_COMPONENTS = 3\n",
    "\n",
    "# Regime detection configuration\n",
    "N_REGIMES_RANGE = range(3, 7)  # Test 3-6 regimes\n",
    "DEFAULT_N_REGIMES = 4\n",
    "\n",
    "# Options strategy configurations\n",
    "CALENDAR_DELTAS = [0.50, 0.40, 0.30, 0.20]\n",
    "DIAGONAL_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20)]\n",
    "STRANGLE_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20), (0.16, -0.16)]\n",
    "BULL_PUT_SHORT_DELTAS = [-0.30, -0.20, -0.16, -0.10]\n",
    "BULL_CALL_LONG_DELTAS = [0.50, 0.40, 0.30]\n",
    "SPREAD_WIDTHS = [5, 10]  # Strike width for spreads\n",
    "\n",
    "# Stop loss thresholds to test\n",
    "STOP_LOSS_THRESHOLDS = [-0.10, -0.15, -0.20, -0.25, -0.30, -0.40, -0.50, None]  # None = no stop\n",
    "\n",
    "# Rolling window for metrics\n",
    "ROLLING_SHARPE_WINDOW = 60\n",
    "\n",
    "# Minimum figure sizes\n",
    "FIG_SIZE_TIMESERIES = (14, 6)\n",
    "FIG_SIZE_REGIME = (14, 10)\n",
    "FIG_SIZE_COMPARISON = (14, 8)\n",
    "FIG_SIZE_HEATMAP = (10, 8)\n",
    "FIG_SIZE_DASHBOARD = (22, 16)\n",
    "FIG_SIZE_ROLLING = (12, 5)\n",
    "\n",
    "print(f\"Configuration loaded for ticker: {TICKER}\")\n",
    "print(f\"Analysis period: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"Volatility windows: {VOL_WINDOWS}\")\n",
    "print(f\"Testing {len(STOP_LOSS_THRESHOLDS)} stop loss thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# DATA FETCHING FUNCTIONS - REAL QUANTCONNECT OPTIONS DATA\n# ============================================================================\n\ndef fetch_equity_data(ticker: str, start_date: datetime, end_date: datetime, \n                      resolution: str = 'Daily') -> pd.DataFrame:\n    \"\"\"\n    Fetch historical OHLCV data for an equity.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Stock ticker symbol\n    start_date : datetime\n        Start date for data fetch\n    end_date : datetime\n        End date for data fetch\n    resolution : str\n        Data resolution ('Daily', 'Hour', 'Minute')\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Historical OHLCV data\n    \"\"\"\n    # Add equity to universe\n    symbol = qb.AddEquity(ticker, Resolution.Daily).Symbol\n    \n    # Fetch historical data\n    history = qb.History(symbol, start_date, end_date, Resolution.Daily)\n    \n    if history.empty:\n        print(f\"Warning: No data available for {ticker}\")\n        return pd.DataFrame()\n    \n    # Reset index and clean up\n    df = history.reset_index()\n    df = df.rename(columns={'time': 'Date'})\n    df = df.set_index('Date')\n    \n    # Calculate returns\n    df['returns'] = df['close'].pct_change()\n    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n    \n    return df\n\n\ndef fetch_options_with_greeks(ticker: str, date: datetime, \n                               strategy_type: str = None,\n                               min_dte: int = 7, max_dte: int = 60,\n                               delta_range: tuple = None) -> pd.DataFrame:\n    \"\"\"\n    Fetch options chain data with Greeks from QuantConnect API.\n    Filters contracts to those needed for specific strategies.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying stock ticker\n    date : datetime\n        Date to fetch options chain\n    strategy_type : str\n        Strategy type to filter contracts for (optional)\n    min_dte : int\n        Minimum days to expiration\n    max_dte : int\n        Maximum days to expiration\n    delta_range : tuple\n        (min_delta, max_delta) to filter by absolute delta\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Options chain with strikes, prices, and Greeks from QuantConnect API\n    \"\"\"\n    # Add equity and options\n    equity = qb.AddEquity(ticker, Resolution.Daily)\n    option = qb.AddOption(ticker, Resolution.Daily)\n    \n    # Configure option filter based on strategy needs\n    if strategy_type == 'Calendar Spread':\n        # Calendar spreads need ATM options at different expirations\n        option.SetFilter(lambda u: u.Strikes(-5, 5).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type == 'Double Diagonal':\n        # Double diagonals need OTM options at different expirations\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type in ['Straddle', 'Strangle']:\n        # Straddles/Strangles need ATM/OTM puts and calls\n        option.SetFilter(lambda u: u.Strikes(-8, 8).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    elif strategy_type in ['Bull Put Spread', 'Bull Call Spread']:\n        # Vertical spreads need specific strike ranges\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    else:\n        # Default filter\n        option.SetFilter(lambda u: u.Strikes(-10, 10).Expiration(timedelta(days=min_dte), timedelta(days=max_dte)))\n    \n    # Get option chain for the date\n    chain = qb.OptionChainProvider.GetOptionContractList(equity.Symbol, date)\n    \n    if not chain:\n        print(f\"No options chain available for {ticker} on {date}\")\n        return pd.DataFrame()\n    \n    # Fetch contracts with Greeks from QuantConnect\n    options_data = []\n    \n    for contract in chain:\n        try:\n            # Filter by expiration\n            days_to_expiry = (contract.ID.Date - date).days\n            if days_to_expiry < min_dte or days_to_expiry > max_dte:\n                continue\n            \n            # Add contract to get Greeks from QuantConnect\n            option_contract = qb.AddOptionContract(contract, Resolution.Daily)\n            \n            # Get historical data including Greeks\n            history = qb.History(contract, date, date + timedelta(days=1), Resolution.Daily)\n            \n            if history.empty:\n                continue\n            \n            # Extract Greeks from QuantConnect API (attached to option contract)\n            # In QuantConnect, Greeks are available via option_contract.Greeks\n            row = {\n                'symbol': str(contract),\n                'underlying': ticker,\n                'strike': contract.ID.StrikePrice,\n                'expiry': contract.ID.Date,\n                'dte': days_to_expiry,\n                'option_type': 'Call' if contract.ID.OptionRight == OptionRight.Call else 'Put',\n                'bid': history['bidprice'].iloc[-1] if 'bidprice' in history.columns else np.nan,\n                'ask': history['askprice'].iloc[-1] if 'askprice' in history.columns else np.nan,\n                'mid': np.nan,  # Will calculate below\n                'last': history['close'].iloc[-1] if 'close' in history.columns else np.nan,\n                'volume': history['volume'].iloc[-1] if 'volume' in history.columns else 0,\n                'open_interest': history['openinterest'].iloc[-1] if 'openinterest' in history.columns else 0,\n                # Greeks from QuantConnect API\n                'delta': option_contract.Greeks.Delta if hasattr(option_contract, 'Greeks') else np.nan,\n                'gamma': option_contract.Greeks.Gamma if hasattr(option_contract, 'Greeks') else np.nan,\n                'theta': option_contract.Greeks.Theta if hasattr(option_contract, 'Greeks') else np.nan,\n                'vega': option_contract.Greeks.Vega if hasattr(option_contract, 'Greeks') else np.nan,\n                'rho': option_contract.Greeks.Rho if hasattr(option_contract, 'Greeks') else np.nan,\n                'iv': option_contract.ImpliedVolatility if hasattr(option_contract, 'ImpliedVolatility') else np.nan,\n            }\n            \n            # Calculate mid price\n            if not np.isnan(row['bid']) and not np.isnan(row['ask']):\n                row['mid'] = (row['bid'] + row['ask']) / 2\n            \n            # Filter by delta if specified\n            if delta_range and not np.isnan(row['delta']):\n                abs_delta = abs(row['delta'])\n                if abs_delta < delta_range[0] or abs_delta > delta_range[1]:\n                    continue\n            \n            options_data.append(row)\n            \n        except Exception as e:\n            continue\n    \n    df = pd.DataFrame(options_data)\n    \n    if not df.empty:\n        # Sort by expiry and strike\n        df = df.sort_values(['expiry', 'option_type', 'strike'])\n        print(f\"Fetched {len(df)} option contracts for {ticker} on {date}\")\n    \n    return df\n\n\ndef get_strategy_contracts(ticker: str, date: datetime, underlying_price: float,\n                           strategy_type: str, delta_config: dict = None,\n                           front_dte: int = 30, back_dte: int = 60) -> dict:\n    \"\"\"\n    Get specific option contracts needed for a strategy.\n    Uses QuantConnect API for Greeks and IV.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    date : datetime\n        Trade date\n    underlying_price : float\n        Current underlying price\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta targets for the strategy\n    front_dte : int\n        Days to expiry for front month\n    back_dte : int\n        Days to expiry for back month\n        \n    Returns:\n    --------\n    dict\n        Dictionary of contracts needed for the strategy\n    \"\"\"\n    contracts = {}\n    \n    # Fetch full chain with Greeks\n    chain = fetch_options_with_greeks(ticker, date, strategy_type, \n                                       min_dte=7, max_dte=back_dte + 15)\n    \n    if chain.empty:\n        return contracts\n    \n    if strategy_type == 'Calendar Spread':\n        # Calendar: Same strike, different expirations (ATM)\n        target_delta = delta_config.get('call_delta', 0.50) if delta_config else 0.50\n        \n        # Find ATM strike (closest to underlying price)\n        chain['strike_distance'] = np.abs(chain['strike'] - underlying_price)\n        atm_strike = chain.loc[chain['strike_distance'].idxmin(), 'strike']\n        \n        # Front month call (short)\n        front_calls = chain[(chain['option_type'] == 'Call') & \n                           (chain['strike'] == atm_strike) & \n                           (chain['dte'] >= front_dte - 5) & \n                           (chain['dte'] <= front_dte + 5)]\n        if not front_calls.empty:\n            contracts['front_call'] = front_calls.iloc[0].to_dict()\n        \n        # Back month call (long)\n        back_calls = chain[(chain['option_type'] == 'Call') & \n                          (chain['strike'] == atm_strike) & \n                          (chain['dte'] >= back_dte - 5) & \n                          (chain['dte'] <= back_dte + 5)]\n        if not back_calls.empty:\n            contracts['back_call'] = back_calls.iloc[0].to_dict()\n    \n    elif strategy_type == 'Double Diagonal':\n        # Double Diagonal: Calendar spreads on both calls and puts at different strikes\n        call_delta = delta_config.get('call_delta', 0.30) if delta_config else 0.30\n        put_delta = delta_config.get('put_delta', -0.30) if delta_config else -0.30\n        \n        # Select calls by delta\n        calls = chain[chain['option_type'] == 'Call'].copy()\n        if not calls.empty and 'delta' in calls.columns:\n            calls['delta_dist'] = np.abs(calls['delta'] - call_delta)\n            # Front and back month calls\n            front_calls = calls[(calls['dte'] >= front_dte - 5) & (calls['dte'] <= front_dte + 5)]\n            back_calls = calls[(calls['dte'] >= back_dte - 5) & (calls['dte'] <= back_dte + 5)]\n            \n            if not front_calls.empty:\n                contracts['front_call'] = front_calls.loc[front_calls['delta_dist'].idxmin()].to_dict()\n            if not back_calls.empty:\n                contracts['back_call'] = back_calls.loc[back_calls['delta_dist'].idxmin()].to_dict()\n        \n        # Select puts by delta\n        puts = chain[chain['option_type'] == 'Put'].copy()\n        if not puts.empty and 'delta' in puts.columns:\n            puts['delta_dist'] = np.abs(puts['delta'] - put_delta)\n            front_puts = puts[(puts['dte'] >= front_dte - 5) & (puts['dte'] <= front_dte + 5)]\n            back_puts = puts[(puts['dte'] >= back_dte - 5) & (puts['dte'] <= back_dte + 5)]\n            \n            if not front_puts.empty:\n                contracts['front_put'] = front_puts.loc[front_puts['delta_dist'].idxmin()].to_dict()\n            if not back_puts.empty:\n                contracts['back_put'] = back_puts.loc[back_puts['delta_dist'].idxmin()].to_dict()\n    \n    elif strategy_type == 'Straddle':\n        # Straddle: Long ATM call + Long ATM put\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        # Find ATM strike\n        chain['strike_distance'] = np.abs(chain['strike'] - underlying_price)\n        atm_strike = chain.loc[chain['strike_distance'].idxmin(), 'strike']\n        \n        # ATM Call\n        atm_calls = chain[(chain['option_type'] == 'Call') & \n                         (chain['strike'] == atm_strike) &\n                         (chain['dte'] >= target_dte_range[0]) & \n                         (chain['dte'] <= target_dte_range[1])]\n        if not atm_calls.empty:\n            contracts['long_call'] = atm_calls.iloc[0].to_dict()\n        \n        # ATM Put\n        atm_puts = chain[(chain['option_type'] == 'Put') & \n                        (chain['strike'] == atm_strike) &\n                        (chain['dte'] >= target_dte_range[0]) & \n                        (chain['dte'] <= target_dte_range[1])]\n        if not atm_puts.empty:\n            contracts['long_put'] = atm_puts.iloc[0].to_dict()\n    \n    elif strategy_type == 'Strangle':\n        # Strangle: Long OTM call + Long OTM put\n        call_delta = delta_config.get('call_delta', 0.25) if delta_config else 0.25\n        put_delta = delta_config.get('put_delta', -0.25) if delta_config else -0.25\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        # OTM Call by delta\n        calls = chain[(chain['option_type'] == 'Call') & \n                     (chain['dte'] >= target_dte_range[0]) & \n                     (chain['dte'] <= target_dte_range[1])].copy()\n        if not calls.empty and 'delta' in calls.columns:\n            calls['delta_dist'] = np.abs(calls['delta'] - call_delta)\n            contracts['long_call'] = calls.loc[calls['delta_dist'].idxmin()].to_dict()\n        \n        # OTM Put by delta  \n        puts = chain[(chain['option_type'] == 'Put') & \n                    (chain['dte'] >= target_dte_range[0]) & \n                    (chain['dte'] <= target_dte_range[1])].copy()\n        if not puts.empty and 'delta' in puts.columns:\n            puts['delta_dist'] = np.abs(puts['delta'] - put_delta)\n            contracts['long_put'] = puts.loc[puts['delta_dist'].idxmin()].to_dict()\n    \n    elif strategy_type == 'Bull Put Spread':\n        # Bull Put Spread: Short put + Long lower strike put\n        short_delta = delta_config.get('put_delta', -0.20) if delta_config else -0.20\n        spread_width = delta_config.get('spread_width', 5) if delta_config else 5\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        puts = chain[(chain['option_type'] == 'Put') & \n                    (chain['dte'] >= target_dte_range[0]) & \n                    (chain['dte'] <= target_dte_range[1])].copy()\n        \n        if not puts.empty and 'delta' in puts.columns:\n            # Short put by delta\n            puts['delta_dist'] = np.abs(puts['delta'] - short_delta)\n            short_put = puts.loc[puts['delta_dist'].idxmin()]\n            contracts['short_put'] = short_put.to_dict()\n            \n            # Long put at lower strike\n            long_strike = short_put['strike'] - spread_width\n            long_puts = puts[puts['strike'] == long_strike]\n            if not long_puts.empty:\n                contracts['long_put'] = long_puts.iloc[0].to_dict()\n    \n    elif strategy_type == 'Bull Call Spread':\n        # Bull Call Spread: Long call + Short higher strike call\n        long_delta = delta_config.get('call_delta', 0.40) if delta_config else 0.40\n        spread_width = delta_config.get('spread_width', 5) if delta_config else 5\n        target_dte_range = (front_dte - 5, front_dte + 5)\n        \n        calls = chain[(chain['option_type'] == 'Call') & \n                     (chain['dte'] >= target_dte_range[0]) & \n                     (chain['dte'] <= target_dte_range[1])].copy()\n        \n        if not calls.empty and 'delta' in calls.columns:\n            # Long call by delta\n            calls['delta_dist'] = np.abs(calls['delta'] - long_delta)\n            long_call = calls.loc[calls['delta_dist'].idxmin()]\n            contracts['long_call'] = long_call.to_dict()\n            \n            # Short call at higher strike\n            short_strike = long_call['strike'] + spread_width\n            short_calls = calls[calls['strike'] == short_strike]\n            if not short_calls.empty:\n                contracts['short_call'] = short_calls.iloc[0].to_dict()\n    \n    return contracts\n\n\ndef select_options_by_delta(chain: pd.DataFrame, target_delta: float, \n                            option_type: str = 'Call') -> pd.DataFrame:\n    \"\"\"\n    Select options contracts closest to target delta.\n    Uses Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    chain : pd.DataFrame\n        Options chain data with Greeks from QuantConnect\n    target_delta : float\n        Target delta value (positive for calls, negative for puts)\n    option_type : str\n        'Call' or 'Put'\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Filtered options closest to target delta\n    \"\"\"\n    if chain.empty or 'delta' not in chain.columns:\n        return pd.DataFrame()\n    \n    # Filter by option type\n    filtered = chain[chain['option_type'] == option_type].copy()\n    \n    if filtered.empty:\n        return pd.DataFrame()\n    \n    # Calculate distance from target delta\n    filtered['delta_distance'] = np.abs(filtered['delta'] - target_delta)\n    \n    # Sort by delta distance and return closest\n    return filtered.nsmallest(1, 'delta_distance')\n\n\n# Legacy alias for backward compatibility\ndef get_options_chain(ticker: str, date: datetime) -> pd.DataFrame:\n    \"\"\"Alias for fetch_options_with_greeks for backward compatibility.\"\"\"\n    return fetch_options_with_greeks(ticker, date)\n\n\nprint(\"Data fetching functions defined with QuantConnect Greeks integration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Fetch Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FETCH HISTORICAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Fetching data for {TICKER}...\")\n",
    "\n",
    "# Fetch underlying equity data\n",
    "df_asset = fetch_equity_data(TICKER, START_DATE, END_DATE)\n",
    "print(f\"  {TICKER}: {len(df_asset)} days of data\")\n",
    "\n",
    "# Fetch market proxy data\n",
    "df_market = fetch_equity_data(MARKET_PROXY, START_DATE, END_DATE)\n",
    "print(f\"  {MARKET_PROXY}: {len(df_market)} days of data\")\n",
    "\n",
    "# Fetch sector ETF data\n",
    "df_sector = fetch_equity_data(SECTOR_ETF, START_DATE, END_DATE)\n",
    "print(f\"  {SECTOR_ETF}: {len(df_sector)} days of data\")\n",
    "\n",
    "# Align all dataframes to common dates\n",
    "common_dates = df_asset.index.intersection(df_market.index).intersection(df_sector.index)\n",
    "df_asset = df_asset.loc[common_dates]\n",
    "df_market = df_market.loc[common_dates]\n",
    "df_sector = df_sector.loc[common_dates]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nData fetch completed in {elapsed:.2f} seconds\")\n",
    "print(f\"Common trading days: {len(common_dates)}\")\n",
    "print(f\"Date range: {common_dates[0]} to {common_dates[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Volatility Modeling & Comparison\n",
    "\n",
    "This section implements multiple volatility models and creates a PCA-based aggregate forecast.\n",
    "\n",
    "### 2.1 True Realized Volatility Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: VOLATILITY MODELING & COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility(prices: pd.Series, returns: pd.Series = None, \n",
    "                         method: str = 'historical', window: int = 20,\n",
    "                         annualize: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate volatility using various methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices : pd.Series\n",
    "        Price series (for range-based methods)\n",
    "    returns : pd.Series\n",
    "        Return series (for return-based methods)\n",
    "    method : str\n",
    "        Volatility calculation method\n",
    "    window : int\n",
    "        Rolling window size\n",
    "    annualize : bool\n",
    "        Whether to annualize volatility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Volatility series\n",
    "    \"\"\"\n",
    "    annualization_factor = np.sqrt(252) if annualize else 1\n",
    "    \n",
    "    if returns is None:\n",
    "        returns = prices.pct_change()\n",
    "    \n",
    "    if method == 'historical':\n",
    "        # Simple rolling standard deviation\n",
    "        vol = returns.rolling(window=window).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'ewma':\n",
    "        # Exponentially Weighted Moving Average\n",
    "        vol = returns.ewm(span=window, adjust=False).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'parkinson':\n",
    "        # Parkinson volatility (uses high-low range)\n",
    "        # Requires high/low prices in a DataFrame\n",
    "        if isinstance(prices, pd.DataFrame) and 'high' in prices.columns and 'low' in prices.columns:\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            vol = np.sqrt((log_hl ** 2).rolling(window=window).mean() / (4 * np.log(2))) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'garman_klass':\n",
    "        # Garman-Klass volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            log_co = np.log(prices['close'] / prices['open'])\n",
    "            gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "            vol = np.sqrt(gk.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'rogers_satchell':\n",
    "        # Rogers-Satchell volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hc = np.log(prices['high'] / prices['close'])\n",
    "            log_ho = np.log(prices['high'] / prices['open'])\n",
    "            log_lc = np.log(prices['low'] / prices['close'])\n",
    "            log_lo = np.log(prices['low'] / prices['open'])\n",
    "            rs = log_hc * log_ho + log_lc * log_lo\n",
    "            vol = np.sqrt(rs.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'atr':\n",
    "        # Average True Range based volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'close']):\n",
    "            prev_close = prices['close'].shift(1)\n",
    "            tr = np.maximum(\n",
    "                prices['high'] - prices['low'],\n",
    "                np.maximum(\n",
    "                    np.abs(prices['high'] - prev_close),\n",
    "                    np.abs(prices['low'] - prev_close)\n",
    "                )\n",
    "            )\n",
    "            atr = tr.rolling(window=window).mean()\n",
    "            vol = (atr / prices['close']) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "    else:\n",
    "        vol = pd.Series(np.nan, index=prices.index if hasattr(prices, 'index') else None)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "\n",
    "# Calculate true realized volatility (multiple windows)\n",
    "realized_vol = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "for window in VOL_WINDOWS:\n",
    "    realized_vol[f'rv_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Primary realized volatility (20-day is standard)\n",
    "realized_vol['rv_true'] = realized_vol['rv_20d']\n",
    "\n",
    "print(\"Realized Volatility Statistics:\")\n",
    "print(realized_vol.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Volatility Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE VOLATILITY MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate all volatility measures\n",
    "vol_models = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "# 1. Historical volatility (various windows)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'hist_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 2. EWMA volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'ewma_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='ewma',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 3. Parkinson volatility (high-low based)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'parkinson_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='parkinson',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 4. Garman-Klass volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'garman_klass_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='garman_klass',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 5. Rogers-Satchell volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'rogers_satchell_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='rogers_satchell',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 6. ATR-based volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'atr_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='atr',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Drop NaN rows\n",
    "vol_models = vol_models.dropna()\n",
    "\n",
    "print(f\"Volatility models calculated: {vol_models.shape[1]} features\")\n",
    "print(f\"Valid observations: {len(vol_models)}\")\n",
    "print(f\"\\nVolatility Model Columns:\")\n",
    "for col in vol_models.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate correlation and RMSE for each model vs true volatility\n",
    "true_vol = realized_vol['rv_true'].dropna()\n",
    "common_idx = true_vol.index.intersection(vol_models.index)\n",
    "\n",
    "model_metrics = []\n",
    "\n",
    "for col in vol_models.columns:\n",
    "    model_vol = vol_models[col].loc[common_idx]\n",
    "    true_vol_aligned = true_vol.loc[common_idx]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correlation = model_vol.corr(true_vol_aligned)\n",
    "    rmse = np.sqrt(((model_vol - true_vol_aligned) ** 2).mean())\n",
    "    mae = np.abs(model_vol - true_vol_aligned).mean()\n",
    "    \n",
    "    model_metrics.append({\n",
    "        'Model': col,\n",
    "        'Correlation': correlation,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(model_metrics).sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"Model Performance vs True Volatility (20d realized):\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualization: Model comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=FIG_SIZE_COMPARISON)\n",
    "\n",
    "# Top 5 models by correlation\n",
    "top_models = metrics_df.head(5)['Model'].tolist()\n",
    "\n",
    "# Plot 1: Time series comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol.loc[common_idx], label='True Volatility (20d)', color='black', linewidth=2)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(top_models)))\n",
    "for i, model in enumerate(top_models):\n",
    "    ax1.plot(vol_models[model].loc[common_idx], label=model, alpha=0.7, color=colors[i])\n",
    "ax1.set_title(f'{TICKER} - Volatility Models Comparison', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Correlation bar chart\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0.9 else '#f39c12' if x > 0.8 else '#e74c3c' \n",
    "          for x in metrics_df['Correlation'].head(10)]\n",
    "ax2.barh(metrics_df['Model'].head(10)[::-1], metrics_df['Correlation'].head(10)[::-1], color=colors)\n",
    "ax2.set_xlabel('Correlation with True Volatility')\n",
    "ax2.set_title('Top 10 Models by Correlation', fontsize=14)\n",
    "ax2.axvline(x=0.9, color='green', linestyle='--', label='High correlation (0.9)')\n",
    "ax2.axvline(x=0.8, color='orange', linestyle='--', label='Good correlation (0.8)')\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PCA-Based Aggregate Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA-BASED AGGREGATE VOLATILITY MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def run_pca_model(features: pd.DataFrame, n_components: int = 3, \n",
    "                  lag_periods: list = None) -> Tuple[pd.DataFrame, PCA, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Create PCA-based aggregate volatility forecast using lagged features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : pd.DataFrame\n",
    "        Volatility features\n",
    "    n_components : int\n",
    "        Number of PCA components\n",
    "    lag_periods : list\n",
    "        Lag periods to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, PCA, StandardScaler]\n",
    "        Lagged features with PCA components, fitted PCA model, fitted scaler\n",
    "    \"\"\"\n",
    "    if lag_periods is None:\n",
    "        lag_periods = [1, 2, 3, 5, 10, 20]\n",
    "    \n",
    "    # Create lagged features\n",
    "    lagged_features = features.copy()\n",
    "    \n",
    "    for col in features.columns:\n",
    "        for lag in lag_periods:\n",
    "            lagged_features[f'{col}_lag{lag}'] = features[col].shift(lag)\n",
    "    \n",
    "    # Drop NaN rows\n",
    "    lagged_features = lagged_features.dropna()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(lagged_features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    result = pd.DataFrame(\n",
    "        pca_components,\n",
    "        index=lagged_features.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Add explained variance info\n",
    "    print(f\"PCA Explained Variance Ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.1f}%)\")\n",
    "    print(f\"  Total: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "    \n",
    "    return result, pca, scaler\n",
    "\n",
    "\n",
    "# Run PCA on volatility features\n",
    "pca_result, pca_model, vol_scaler = run_pca_model(\n",
    "    features=vol_models,\n",
    "    n_components=PCA_N_COMPONENTS,\n",
    "    lag_periods=PCA_LAG_PERIODS\n",
    ")\n",
    "\n",
    "# Create aggregate volatility forecast (weighted combination of PCs)\n",
    "# Weight by explained variance ratio\n",
    "weights = pca_model.explained_variance_ratio_\n",
    "pca_result['aggregate_vol_signal'] = sum(\n",
    "    pca_result[f'PC{i+1}'] * weights[i] for i in range(PCA_N_COMPONENTS)\n",
    ")\n",
    "\n",
    "# Normalize to same scale as true volatility\n",
    "true_vol_aligned = realized_vol['rv_true'].loc[pca_result.index]\n",
    "signal_mean = pca_result['aggregate_vol_signal'].mean()\n",
    "signal_std = pca_result['aggregate_vol_signal'].std()\n",
    "true_mean = true_vol_aligned.mean()\n",
    "true_std = true_vol_aligned.std()\n",
    "\n",
    "pca_result['aggregate_vol_forecast'] = (\n",
    "    (pca_result['aggregate_vol_signal'] - signal_mean) / signal_std * true_std + true_mean\n",
    ")\n",
    "\n",
    "print(f\"\\nAggregate Model Statistics:\")\n",
    "print(f\"  Correlation with True Vol: {pca_result['aggregate_vol_forecast'].corr(true_vol_aligned):.4f}\")\n",
    "rmse = np.sqrt(((pca_result['aggregate_vol_forecast'] - true_vol_aligned) ** 2).mean())\n",
    "print(f\"  RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Aggregate Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE MODEL VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Aggregate forecast vs True volatility\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol_aligned, label='True Volatility (20d)', color='black', linewidth=2)\n",
    "ax1.plot(pca_result['aggregate_vol_forecast'], label='PCA Aggregate Forecast', \n",
    "         color='#3498db', linewidth=1.5, alpha=0.8)\n",
    "ax1.fill_between(pca_result.index, \n",
    "                 pca_result['aggregate_vol_forecast'] - rmse,\n",
    "                 pca_result['aggregate_vol_forecast'] + rmse,\n",
    "                 alpha=0.2, color='#3498db', label='\u00b1RMSE band')\n",
    "ax1.set_title(f'{TICKER} - PCA Aggregate Volatility Model vs True Volatility', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = pca_result['aggregate_vol_forecast'].corr(true_vol_aligned)\n",
    "ax1.annotate(f'Correlation: {corr:.4f}\\nRMSE: {rmse:.4f}', \n",
    "             xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "             verticalalignment='top', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot 2: Principal Components\n",
    "ax2 = axes[1]\n",
    "for i in range(PCA_N_COMPONENTS):\n",
    "    ax2.plot(pca_result[f'PC{i+1}'], label=f'PC{i+1} ({pca_model.explained_variance_ratio_[i]*100:.1f}%)',\n",
    "             alpha=0.8)\n",
    "ax2.set_title('Principal Components Over Time', fontsize=14)\n",
    "ax2.set_ylabel('Standardized Value')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Scatter plot - Forecast vs Actual\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(true_vol_aligned, pca_result['aggregate_vol_forecast'], alpha=0.5, s=10)\n",
    "min_val = min(true_vol_aligned.min(), pca_result['aggregate_vol_forecast'].min())\n",
    "max_val = max(true_vol_aligned.max(), pca_result['aggregate_vol_forecast'].max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "ax3.set_xlabel('True Volatility')\n",
    "ax3.set_ylabel('Forecast Volatility')\n",
    "ax3.set_title('Forecast vs Actual Volatility', fontsize=14)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Explanation & Maintenance\n",
    "\n",
    "### 3.1 Understanding PCA (Principal Component Analysis)\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "Principal Component Analysis is a dimensionality reduction technique that identifies patterns of correlation across multiple variables. In our volatility modeling context:\n",
    "\n",
    "1. **Multiple Volatility Measures**: We calculate many volatility estimates (historical, EWMA, Parkinson, Garman-Klass, etc.) at different time windows. These measures are often highly correlated but capture slightly different aspects of volatility.\n",
    "\n",
    "2. **Common Variance Patterns**: PCA identifies the common underlying \"factors\" that drive these volatility measures. The first principal component (PC1) captures the most common variance pattern - essentially what all volatility measures agree on.\n",
    "\n",
    "3. **Noise Reduction**: By focusing on the top principal components (which explain most variance), we filter out measurement noise and idiosyncratic differences between methods.\n",
    "\n",
    "4. **Composite Signal**: The weighted combination of principal components creates a more robust volatility forecast than any single measure.\n",
    "\n",
    "**Interpretation of Principal Components:**\n",
    "- **PC1** (largest variance): Usually represents the \"level\" of volatility - whether vol is generally high or low\n",
    "- **PC2**: Often captures the \"slope\" - whether vol is rising or falling\n",
    "- **PC3**: May capture \"curvature\" or regime-specific patterns\n",
    "\n",
    "### 3.2 Why Lag Features Improve Volatility Forecasting\n",
    "\n",
    "**Volatility Clustering**: One of the most robust findings in financial markets is that volatility exhibits strong autocorrelation - high volatility tends to be followed by high volatility, and low by low. This is captured by including lagged features.\n",
    "\n",
    "**Optimal Lag Periods:**\n",
    "- **Lag 1-3 days**: Captures immediate momentum and short-term persistence\n",
    "- **Lag 5-10 days**: Captures weekly patterns and medium-term trends\n",
    "- **Lag 20 days**: Captures monthly cycles and mean-reversion signals\n",
    "\n",
    "**Why This Works:**\n",
    "- Volatility at time t is highly correlated with volatility at t-1, t-2, etc.\n",
    "- Changes in the relationship between current and lagged volatility signal regime changes\n",
    "- The PCA captures how these lag relationships evolve over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Maintenance Strategy\n",
    "\n",
    "#### Rolling Window Retraining Schedule\n",
    "\n",
    "The PCA model should be periodically retrained to adapt to changing market conditions:\n",
    "\n",
    "| Frequency | Action | Rationale |\n",
    "|-----------|--------|----------|\n",
    "| Weekly | Monitor explained variance | Early warning of model drift |\n",
    "| Monthly | Validate on recent data | Check out-of-sample performance |\n",
    "| Quarterly | Full retraining | Adapt to structural changes |\n",
    "| After major events | Emergency recalibration | Respond to regime shifts |\n",
    "\n",
    "#### Out-of-Sample Validation Approach\n",
    "\n",
    "```python\n",
    "# Recommended validation structure:\n",
    "# - Training: 70% of historical data\n",
    "# - Validation: 15% for hyperparameter tuning\n",
    "# - Test: 15% for final evaluation\n",
    "# - Rolling: Move window forward and repeat\n",
    "```\n",
    "\n",
    "#### Drift Detection Methods\n",
    "\n",
    "1. **Explained Variance Monitoring**: If total explained variance drops significantly, component structure may be changing\n",
    "2. **Correlation Decay**: Track rolling correlation between forecast and realized volatility\n",
    "3. **RMSE Tracking**: Monitor prediction error over rolling windows\n",
    "4. **Distribution Shift**: Compare feature distributions using KS tests\n",
    "\n",
    "#### Recalibration Triggers\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| 30-day rolling correlation | < 0.7 | Alert & investigate |\n",
    "| 30-day rolling RMSE | > 1.5\u00d7 baseline | Trigger retraining |\n",
    "| Explained variance (PC1-3) | < 70% | Review feature set |\n",
    "| Consecutive days of underperformance | > 10 | Emergency review |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: MODEL MAINTENANCE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_model_drift_metrics(true_vol: pd.Series, forecast_vol: pd.Series,\n",
    "                                  window: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rolling metrics to detect model drift.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_vol : pd.Series\n",
    "        Realized volatility\n",
    "    forecast_vol : pd.Series\n",
    "        Model forecast\n",
    "    window : int\n",
    "        Rolling window for metrics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Drift metrics over time\n",
    "    \"\"\"\n",
    "    # Align series\n",
    "    common_idx = true_vol.index.intersection(forecast_vol.index)\n",
    "    true_vol = true_vol.loc[common_idx]\n",
    "    forecast_vol = forecast_vol.loc[common_idx]\n",
    "    \n",
    "    drift_metrics = pd.DataFrame(index=common_idx)\n",
    "    \n",
    "    # Rolling correlation\n",
    "    drift_metrics['rolling_corr'] = true_vol.rolling(window).corr(forecast_vol)\n",
    "    \n",
    "    # Rolling RMSE\n",
    "    errors_sq = (true_vol - forecast_vol) ** 2\n",
    "    drift_metrics['rolling_rmse'] = np.sqrt(errors_sq.rolling(window).mean())\n",
    "    \n",
    "    # Rolling MAE\n",
    "    errors_abs = np.abs(true_vol - forecast_vol)\n",
    "    drift_metrics['rolling_mae'] = errors_abs.rolling(window).mean()\n",
    "    \n",
    "    # Directional accuracy (did we predict direction of change correctly?)\n",
    "    true_direction = np.sign(true_vol.diff())\n",
    "    forecast_direction = np.sign(forecast_vol.diff())\n",
    "    correct_direction = (true_direction == forecast_direction).astype(int)\n",
    "    drift_metrics['directional_accuracy'] = correct_direction.rolling(window).mean()\n",
    "    \n",
    "    return drift_metrics.dropna()\n",
    "\n",
    "\n",
    "def check_recalibration_needed(drift_metrics: pd.DataFrame, \n",
    "                               corr_threshold: float = 0.7,\n",
    "                               rmse_multiplier: float = 1.5) -> dict:\n",
    "    \"\"\"\n",
    "    Check if model needs recalibration based on drift metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Recalibration status and recommendations\n",
    "    \"\"\"\n",
    "    latest = drift_metrics.iloc[-1]\n",
    "    baseline_rmse = drift_metrics['rolling_rmse'].median()\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    if latest['rolling_corr'] < corr_threshold:\n",
    "        alerts.append(f\"Low correlation: {latest['rolling_corr']:.3f} < {corr_threshold}\")\n",
    "    \n",
    "    if latest['rolling_rmse'] > baseline_rmse * rmse_multiplier:\n",
    "        alerts.append(f\"High RMSE: {latest['rolling_rmse']:.4f} > {baseline_rmse * rmse_multiplier:.4f}\")\n",
    "    \n",
    "    if latest['directional_accuracy'] < 0.5:\n",
    "        alerts.append(f\"Poor directional accuracy: {latest['directional_accuracy']:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'needs_recalibration': len(alerts) > 0,\n",
    "        'alerts': alerts,\n",
    "        'latest_metrics': latest.to_dict()\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate drift metrics for our model\n",
    "drift_metrics = calculate_model_drift_metrics(\n",
    "    true_vol_aligned,\n",
    "    pca_result['aggregate_vol_forecast'],\n",
    "    window=30\n",
    ")\n",
    "\n",
    "# Check recalibration status\n",
    "recal_status = check_recalibration_needed(drift_metrics)\n",
    "\n",
    "print(\"Model Health Check:\")\n",
    "print(f\"  Needs Recalibration: {recal_status['needs_recalibration']}\")\n",
    "if recal_status['alerts']:\n",
    "    print(\"  Alerts:\")\n",
    "    for alert in recal_status['alerts']:\n",
    "        print(f\"    - {alert}\")\n",
    "print(f\"\\nLatest Metrics:\")\n",
    "for metric, value in recal_status['latest_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Mean Reversion Analysis\n",
    "\n",
    "This section analyzes volatility mean reversion characteristics to inform optimal options expiration selection.\n",
    "\n",
    "### 4.1 Volatility Z-Score and Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: MEAN REVERSION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility_zscore(vol_series: pd.Series, lookback: int = 252) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate rolling z-score of volatility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vol_series : pd.Series\n",
    "        Volatility series\n",
    "    lookback : int\n",
    "        Lookback window for mean and std calculation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Z-score series\n",
    "    \"\"\"\n",
    "    rolling_mean = vol_series.rolling(window=lookback).mean()\n",
    "    rolling_std = vol_series.rolling(window=lookback).std()\n",
    "    zscore = (vol_series - rolling_mean) / rolling_std\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def classify_volatility_regime(zscore: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Classify volatility into regimes based on z-score.\n",
    "    \n",
    "    Regimes:\n",
    "    - Low: z < -1\n",
    "    - Normal: -1 <= z <= 1\n",
    "    - Elevated: 1 < z <= 2\n",
    "    - Extreme: z > 2\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        zscore < -1,\n",
    "        (zscore >= -1) & (zscore <= 1),\n",
    "        (zscore > 1) & (zscore <= 2),\n",
    "        zscore > 2\n",
    "    ]\n",
    "    choices = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    return pd.Series(np.select(conditions, choices, default='Normal'), index=zscore.index)\n",
    "\n",
    "\n",
    "# Calculate z-scores\n",
    "vol_zscore = calculate_volatility_zscore(realized_vol['rv_true'].dropna())\n",
    "vol_regime = classify_volatility_regime(vol_zscore)\n",
    "\n",
    "# Combine into analysis DataFrame\n",
    "vol_analysis = pd.DataFrame({\n",
    "    'volatility': realized_vol['rv_true'],\n",
    "    'zscore': vol_zscore,\n",
    "    'regime': vol_regime\n",
    "}).dropna()\n",
    "\n",
    "# Regime statistics\n",
    "regime_stats = vol_analysis.groupby('regime').agg({\n",
    "    'volatility': ['mean', 'std', 'count'],\n",
    "    'zscore': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Volatility Regime Statistics:\")\n",
    "print(regime_stats)\n",
    "print(f\"\\nRegime Distribution:\")\n",
    "regime_counts = vol_analysis['regime'].value_counts()\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = count / len(vol_analysis) * 100\n",
    "    print(f\"  {regime}: {count} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mean Reversion Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# MEAN REVERSION SPEED ANALYSIS\n# ============================================================================\n\ndef calculate_mean_reversion_speed(zscore: pd.Series, regime_series: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Calculate mean reversion characteristics for each volatility regime.\n    Vectorized implementation.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Mean reversion metrics by regime\n    \"\"\"\n    results = []\n    \n    for regime in ['Low', 'Normal', 'Elevated', 'Extreme']:\n        # Find regime entry points\n        regime_mask = regime_series == regime\n        regime_start = regime_mask & ~regime_mask.shift(1, fill_value=False)\n        \n        # For each entry, measure time to revert\n        times_to_05 = []  # Time to reach \u00b10.5\u03c3\n        times_to_10 = []  # Time to reach \u00b11\u03c3\n        \n        entry_indices = zscore.index[regime_start]\n        \n        for entry_idx in entry_indices:\n            entry_zscore = zscore.loc[entry_idx]\n            future_zscore = zscore.loc[entry_idx:]\n            \n            # Time to \u00b10.5\u03c3\n            mask_05 = np.abs(future_zscore) <= 0.5\n            if mask_05.any():\n                idx_positions = np.where(mask_05.values)[0]\n                if len(idx_positions) > 0:\n                    first_05 = mask_05.index[idx_positions[0]]\n                    days_to_05 = (first_05 - entry_idx).days\n                    if days_to_05 > 0:\n                        times_to_05.append(days_to_05)\n            \n            # Time to \u00b11\u03c3 (only relevant for Elevated/Extreme)\n            if regime in ['Elevated', 'Extreme']:\n                mask_10 = np.abs(future_zscore) <= 1.0\n                if mask_10.any():\n                    idx_positions_10 = np.where(mask_10.values)[0]\n                        if len(idx_positions_10) > 0:\n                            first_10 = mask_10.index[idx_positions_10[0]]\n                    days_to_10 = (first_10 - entry_idx).days\n                    if days_to_10 > 0:\n                        times_to_10.append(days_to_10)\n        \n        # Calculate half-life (approximate from exponential decay)\n        if len(times_to_05) > 0:\n            avg_time_05 = np.mean(times_to_05)\n            median_time_05 = np.median(times_to_05)\n        else:\n            avg_time_05 = np.nan\n            median_time_05 = np.nan\n            \n        if len(times_to_10) > 0:\n            avg_time_10 = np.mean(times_to_10)\n            median_time_10 = np.median(times_to_10)\n        else:\n            avg_time_10 = np.nan\n            median_time_10 = np.nan\n        \n        results.append({\n            'Regime': regime,\n            'Entry_Count': len(entry_indices),\n            'Avg_Days_to_0.5\u03c3': avg_time_05,\n            'Median_Days_to_0.5\u03c3': median_time_05,\n            'Avg_Days_to_1\u03c3': avg_time_10,\n            'Median_Days_to_1\u03c3': median_time_10,\n        })\n    \n    return pd.DataFrame(results)\n\n\n# Calculate mean reversion metrics\nreversion_metrics = calculate_mean_reversion_speed(vol_zscore.dropna(), vol_regime.dropna())\n\nprint(\"Mean Reversion Speed by Volatility Regime:\")\nprint(reversion_metrics.to_string(index=False))\n\n# Recommended DTE by regime\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECOMMENDED OPTIONS EXPIRATION BY VOLATILITY REGIME:\")\nprint(\"=\"*60)\n\ndte_recommendations = {\n    'Extreme': '7-14 DTE (quick mean reversion expected)',\n    'Elevated': '14-21 DTE (moderate reversion timeframe)',\n    'Normal': '30-45 DTE (standard theta decay window)',\n    'Low': '45-60 DTE (wait for volatility expansion)'\n}\n\nfor regime, rec in dte_recommendations.items():\n    print(f\"  {regime}: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Volatility Regime Transition Matrix (PRIORITY VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY REGIME TRANSITION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_transitions(regime_series: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate regime transition probability matrix.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        Transition probability matrix and transition count matrix\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    \n",
    "    # Create transition count matrix\n",
    "    transition_counts = pd.DataFrame(0, index=regimes, columns=regimes)\n",
    "    \n",
    "    current_regime = regime_series.iloc[:-1].values\n",
    "    next_regime = regime_series.iloc[1:].values\n",
    "    \n",
    "    for curr, nxt in zip(current_regime, next_regime):\n",
    "        if curr in regimes and nxt in regimes:\n",
    "            transition_counts.loc[curr, nxt] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    row_sums = transition_counts.sum(axis=1)\n",
    "    transition_probs = transition_counts.div(row_sums, axis=0)\n",
    "    \n",
    "    return transition_probs, transition_counts\n",
    "\n",
    "\n",
    "def calculate_avg_regime_duration(regime_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate average duration spent in each regime.\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    durations = {regime: [] for regime in regimes}\n",
    "    \n",
    "    current_regime = None\n",
    "    current_duration = 0\n",
    "    \n",
    "    for regime in regime_series.values:\n",
    "        if regime == current_regime:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            if current_regime in durations:\n",
    "                durations[current_regime].append(current_duration)\n",
    "            current_regime = regime\n",
    "            current_duration = 1\n",
    "    \n",
    "    # Add final regime\n",
    "    if current_regime in durations:\n",
    "        durations[current_regime].append(current_duration)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    duration_stats = []\n",
    "    for regime in regimes:\n",
    "        if durations[regime]:\n",
    "            duration_stats.append({\n",
    "                'Regime': regime,\n",
    "                'Avg_Duration_Days': np.mean(durations[regime]),\n",
    "                'Median_Duration_Days': np.median(durations[regime]),\n",
    "                'Max_Duration_Days': np.max(durations[regime]),\n",
    "                'Num_Episodes': len(durations[regime])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(duration_stats)\n",
    "\n",
    "\n",
    "# Calculate transitions\n",
    "transition_probs, transition_counts = calculate_regime_transitions(vol_regime.dropna())\n",
    "duration_stats = calculate_avg_regime_duration(vol_regime.dropna())\n",
    "\n",
    "print(\"Regime Transition Probabilities:\")\n",
    "print(transition_probs.round(3))\n",
    "print(\"\\nAverage Regime Duration:\")\n",
    "print(duration_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION: Regime Transition Heatmap\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Transition probability heatmap\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(transition_probs, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "            ax=ax1, vmin=0, vmax=1, linewidths=0.5,\n",
    "            cbar_kws={'label': 'Transition Probability'})\n",
    "ax1.set_title('Volatility Regime Transition Probabilities\\n(Row = Current Regime, Column = Next Regime)', \n",
    "              fontsize=14)\n",
    "ax1.set_xlabel('Next Regime')\n",
    "ax1.set_ylabel('Current Regime')\n",
    "\n",
    "# Add average transition times as annotations\n",
    "for i, regime_from in enumerate(transition_probs.index):\n",
    "    duration = duration_stats[duration_stats['Regime'] == regime_from]['Avg_Duration_Days'].values\n",
    "    if len(duration) > 0:\n",
    "        ax1.annotate(f'Avg: {duration[0]:.1f}d', \n",
    "                    xy=(-0.3, i + 0.5), xycoords='data',\n",
    "                    fontsize=9, ha='right', va='center')\n",
    "\n",
    "# Bar chart of regime durations\n",
    "ax2 = axes[1]\n",
    "colors = [get_vol_regime_color(r) for r in duration_stats['Regime']]\n",
    "bars = ax2.bar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'], color=colors, alpha=0.8)\n",
    "ax2.errorbar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'],\n",
    "             yerr=[duration_stats['Avg_Duration_Days'] - duration_stats['Median_Duration_Days'],\n",
    "                   duration_stats['Max_Duration_Days'] - duration_stats['Avg_Duration_Days']],\n",
    "             fmt='none', color='black', capsize=5)\n",
    "ax2.set_title('Average Regime Duration (with min/max range)', fontsize=14)\n",
    "ax2.set_xlabel('Volatility Regime')\n",
    "ax2.set_ylabel('Days')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count annotations\n",
    "for bar, count in zip(bars, duration_stats['Num_Episodes']):\n",
    "    ax2.annotate(f'n={count}', \n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"- High diagonal values indicate regime persistence\")\n",
    "print(\"- Off-diagonal values show transition likelihoods\")\n",
    "print(\"- Extreme volatility typically reverts to Elevated/Normal\")\n",
    "print(\"- Use this to inform position sizing and DTE selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Options Strategy Development & Delta Analysis\n",
    "\n",
    "This section implements six options strategies with systematic delta testing and performance analysis.\n",
    "\n",
    "### Strategy Suite:\n",
    "1. **Calendar Spread** - Long back month, short front month, same strike\n",
    "2. **Double Diagonal** - Calendar spread on both calls and puts, different strikes\n",
    "3. **Long Straddle** - Long ATM call + long ATM put\n",
    "4. **Long Strangle** - Long OTM call + long OTM put\n",
    "5. **Bull Put Spread** - Short put + long lower strike put\n",
    "6. **Bull Call Spread** - Long call + short higher strike call\n",
    "\n",
    "### 5.1 Strategy Implementation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: OPTIONS STRATEGY DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy configurations\n",
    "STRATEGIES = [\n",
    "    'Calendar Spread',\n",
    "    'Double Diagonal',\n",
    "    'Straddle',\n",
    "    'Strangle',\n",
    "    'Bull Put Spread',\n",
    "    'Bull Call Spread'\n",
    "]\n",
    "\n",
    "def calculate_strategy_pnl(strategy_type: str, entry_price: float, exit_price: float,\n",
    "                           contracts: int = 1, multiplier: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate P&L for a strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy_type : str\n",
    "        Type of options strategy\n",
    "    entry_price : float\n",
    "        Net debit/credit at entry\n",
    "    exit_price : float\n",
    "        Net value at exit\n",
    "    contracts : int\n",
    "        Number of contracts\n",
    "    multiplier : int\n",
    "        Contract multiplier (usually 100)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Profit/Loss in dollars\n",
    "    \"\"\"\n",
    "    return (exit_price - entry_price) * contracts * multiplier\n",
    "\n",
    "\n",
    "def apply_stop_loss(pnl_series: pd.Series, stop_threshold: float = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply stop loss to a P&L series.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pnl_series : pd.Series\n",
    "        Series of P&L values (as returns or dollar amounts)\n",
    "    stop_threshold : float\n",
    "        Stop loss threshold (e.g., -0.20 for 20% loss). None = no stop.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Adjusted P&L series with stop loss applied\n",
    "    \"\"\"\n",
    "    if stop_threshold is None:\n",
    "        return pnl_series\n",
    "    \n",
    "    # Apply stop loss - cap losses at threshold\n",
    "    adjusted = pnl_series.copy()\n",
    "    adjusted = np.where(adjusted < stop_threshold, stop_threshold, adjusted)\n",
    "    return pd.Series(adjusted, index=pnl_series.index)\n",
    "\n",
    "\n",
    "def roll_position(current_expiry: datetime, days_to_roll: int = 5) -> datetime:\n",
    "    \"\"\"\n",
    "    Calculate next expiration date for position rolling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_expiry : datetime\n",
    "        Current position expiration\n",
    "    days_to_roll : int\n",
    "        Days before expiry to trigger roll\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    datetime\n",
    "        Next expiration date (approximately 30 days out)\n",
    "    \"\"\"\n",
    "    # Roll to next monthly expiration\n",
    "    next_expiry = current_expiry + timedelta(days=30)\n",
    "    # Adjust to third Friday (standard monthly expiration)\n",
    "    # Find first day of expiry month\n",
    "    first_day = next_expiry.replace(day=1)\n",
    "    # Find first Friday\n",
    "    days_until_friday = (4 - first_day.weekday()) % 7\n",
    "    first_friday = first_day + timedelta(days=days_until_friday)\n",
    "    # Third Friday\n",
    "    third_friday = first_friday + timedelta(days=14)\n",
    "    return third_friday\n",
    "\n",
    "\n",
    "print(\"Strategy functions defined.\")\n",
    "print(f\"Strategies: {STRATEGIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Strategy Returns Using Real QuantConnect Options Data\n\nThe notebook uses **real options data from QuantConnect** instead of simulations:\n\n- **Greeks** (Delta, Gamma, Theta, Vega, Rho) are fetched directly from the QuantConnect API\n- **Implied Volatility** is obtained from QuantConnect's option pricing model\n- **Contract selection** is limited to those needed for each specific strategy\n- **Bid/Ask prices** are used for realistic P&L calculations\n\nThe `get_strategy_contracts()` function filters the options chain to only the contracts required for each strategy type, reducing data overhead and ensuring accurate Greeks from the QuantConnect API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# STRATEGY RETURNS USING REAL QUANTCONNECT OPTIONS DATA\n# ============================================================================\n\ndef calculate_real_strategy_returns(ticker: str, dates: pd.DatetimeIndex,\n                                    underlying_prices: pd.Series,\n                                    strategy: str, delta_config: dict = None,\n                                    front_dte: int = 30, back_dte: int = 60) -> pd.Series:\n    \"\"\"\n    Calculate strategy returns using real QuantConnect options data.\n    Uses Greeks and IV from QuantConnect API.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker symbol\n    dates : pd.DatetimeIndex\n        Trading dates to calculate returns\n    underlying_prices : pd.Series\n        Underlying asset prices\n    strategy : str\n        Strategy type\n    delta_config : dict\n        Delta configuration for the strategy\n    front_dte : int\n        Front month days to expiration\n    back_dte : int\n        Back month days to expiration\n        \n    Returns:\n    --------\n    pd.Series\n        Strategy daily returns based on real options data\n    \"\"\"\n    returns = pd.Series(index=dates, dtype=float)\n    \n    # Sample dates periodically to avoid API overload (every 5 trading days)\n    sample_dates = dates[::5]\n    \n    current_position = None\n    entry_value = 0\n    \n    for i, date in enumerate(sample_dates):\n        try:\n            if date not in underlying_prices.index:\n                continue\n                \n            underlying_price = underlying_prices.loc[date]\n            \n            # Get contracts for this strategy using QuantConnect Greeks\n            contracts = get_strategy_contracts(\n                ticker=ticker,\n                date=date,\n                underlying_price=underlying_price,\n                strategy_type=strategy,\n                delta_config=delta_config,\n                front_dte=front_dte,\n                back_dte=back_dte\n            )\n            \n            if not contracts:\n                continue\n            \n            # Calculate position value based on strategy type\n            position_value = 0\n            \n            if strategy == 'Calendar Spread':\n                # Long back month, short front month (same strike)\n                if 'back_call' in contracts and 'front_call' in contracts:\n                    back_price = contracts['back_call'].get('mid', contracts['back_call'].get('last', 0))\n                    front_price = contracts['front_call'].get('mid', contracts['front_call'].get('last', 0))\n                    position_value = back_price - front_price  # Net debit\n                    \n            elif strategy == 'Double Diagonal':\n                # Calendar spreads on calls and puts\n                value = 0\n                if 'back_call' in contracts and 'front_call' in contracts:\n                    value += contracts['back_call'].get('mid', 0) - contracts['front_call'].get('mid', 0)\n                if 'back_put' in contracts and 'front_put' in contracts:\n                    value += contracts['back_put'].get('mid', 0) - contracts['front_put'].get('mid', 0)\n                position_value = value\n                \n            elif strategy == 'Straddle':\n                # Long ATM call + Long ATM put\n                if 'long_call' in contracts and 'long_put' in contracts:\n                    call_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    put_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = call_price + put_price  # Net debit\n                    \n            elif strategy == 'Strangle':\n                # Long OTM call + Long OTM put\n                if 'long_call' in contracts and 'long_put' in contracts:\n                    call_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    put_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = call_price + put_price  # Net debit\n                    \n            elif strategy == 'Bull Put Spread':\n                # Short put + Long lower strike put (credit spread)\n                if 'short_put' in contracts and 'long_put' in contracts:\n                    short_price = contracts['short_put'].get('mid', contracts['short_put'].get('last', 0))\n                    long_price = contracts['long_put'].get('mid', contracts['long_put'].get('last', 0))\n                    position_value = short_price - long_price  # Net credit (negative debit)\n                    \n            elif strategy == 'Bull Call Spread':\n                # Long call + Short higher strike call (debit spread)\n                if 'long_call' in contracts and 'short_call' in contracts:\n                    long_price = contracts['long_call'].get('mid', contracts['long_call'].get('last', 0))\n                    short_price = contracts['short_call'].get('mid', contracts['short_call'].get('last', 0))\n                    position_value = long_price - short_price  # Net debit\n            \n            # Calculate return if we have a previous position\n            if current_position is not None and entry_value != 0:\n                pnl = position_value - entry_value\n                daily_return = pnl / abs(entry_value) if entry_value != 0 else 0\n                \n                # Spread return across days since last sample\n                if i > 0:\n                    prev_date = sample_dates[i-1]\n                    days_between = (date - prev_date).days\n                    if days_between > 0:\n                        daily_return_spread = daily_return / days_between\n                        date_range = pd.date_range(prev_date, date, freq='D')[1:]\n                        for d in date_range:\n                            if d in returns.index:\n                                returns.loc[d] = daily_return_spread\n            \n            # Update position\n            current_position = contracts\n            entry_value = position_value\n            \n        except Exception as e:\n            continue\n    \n    return returns.fillna(0)\n\n\ndef calculate_strategy_pnl_from_greeks(contracts: dict, underlying_change: float,\n                                       days_passed: int = 1, vol_change: float = 0) -> float:\n    \"\"\"\n    Calculate strategy P&L using Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    contracts : dict\n        Dictionary of contract data with Greeks from QuantConnect\n    underlying_change : float\n        Change in underlying price\n    days_passed : int\n        Number of days passed (for theta)\n    vol_change : float\n        Change in implied volatility\n        \n    Returns:\n    --------\n    float\n        Estimated P&L based on Greeks\n    \"\"\"\n    total_pnl = 0\n    \n    for leg_name, contract in contracts.items():\n        if not isinstance(contract, dict):\n            continue\n            \n        # Get Greeks from QuantConnect API data\n        delta = contract.get('delta', 0) or 0\n        gamma = contract.get('gamma', 0) or 0\n        theta = contract.get('theta', 0) or 0\n        vega = contract.get('vega', 0) or 0\n        \n        # Determine position sign (long or short)\n        is_short = 'short' in leg_name.lower() or 'front' in leg_name.lower()\n        position_sign = -1 if is_short else 1\n        \n        # Calculate P&L components using QuantConnect Greeks\n        delta_pnl = delta * underlying_change * position_sign\n        gamma_pnl = 0.5 * gamma * (underlying_change ** 2) * position_sign\n        theta_pnl = theta * days_passed * position_sign\n        vega_pnl = vega * vol_change * position_sign\n        \n        leg_pnl = delta_pnl + gamma_pnl + theta_pnl + vega_pnl\n        total_pnl += leg_pnl\n    \n    return total_pnl\n\n\n# Generate returns for all strategies using real options data\n# Note: For full historical backtest, this would query QuantConnect for each date\n# For research purposes, we use a hybrid approach\n\nprint(\"Generating strategy returns using real QuantConnect options data...\")\nprint(\"Note: Full historical backtest requires options data access in QuantConnect Research environment\")\n\nstrategy_returns = {}\n\n# For demonstration, calculate returns using available data\n# In production, this would iterate through historical dates\nfor strategy in STRATEGIES:\n    print(f\"  Processing {strategy}...\")\n    \n    # Use real options data approach\n    # This calls QuantConnect API for Greeks and contract selection\n    returns = calculate_real_strategy_returns(\n        ticker=TICKER,\n        dates=df_asset.index,\n        underlying_prices=df_asset['close'],\n        strategy=strategy,\n        delta_config=None,\n        front_dte=30,\n        back_dte=60\n    )\n    \n    strategy_returns[strategy] = returns\n\n# Create DataFrame\nstrategy_returns_df = pd.DataFrame(strategy_returns)\n\nprint(f\"\\nStrategy returns generated: {len(strategy_returns_df)} days\")\nprint(\"\\nStrategy Return Statistics:\")\nprint(strategy_returns_df.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B. Delta Selection Analysis\n\nSystematic testing of multiple delta configurations for each strategy type.\n\n**Delta Configurations to Test:**\n| Strategy | Delta Configurations |\n|----------|---------------------|\n| Calendar Spreads | 0.50 (ATM), 0.40, 0.30, 0.20 |\n| Double Diagonals | (0.30, -0.30), (0.25, -0.25), (0.20, -0.20) |\n| Straddles | ATM (~0.50 for calls) |\n| Strangles | (0.30, -0.30), (0.25, -0.25), (0.20, -0.20), (0.16, -0.16) |\n| Bull Put Spreads | Short deltas: -0.30, -0.20, -0.16, -0.10 (5-10 pt width) |\n| Bull Call Spreads | Long deltas: 0.50, 0.40, 0.30 (5-10 pt width) |\n\n**Analysis Outputs:**\n- Performance metrics for each strategy \u00d7 delta configuration\n- Heatmap visualization of returns by strategy and delta\n- Optimal delta identification per strategy\n- How optimal delta changes across volatility regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5B: DELTA SELECTION ANALYSIS\n# ============================================================================\n\n# Define delta configurations for systematic testing\nDELTA_CONFIGS = {\n    'Calendar Spread': [\n        {'call_delta': 0.50, 'name': 'ATM (0.50)'},\n        {'call_delta': 0.40, 'name': 'Delta 0.40'},\n        {'call_delta': 0.30, 'name': 'Delta 0.30'},\n        {'call_delta': 0.20, 'name': 'Delta 0.20'},\n    ],\n    'Double Diagonal': [\n        {'call_delta': 0.30, 'put_delta': -0.30, 'name': '(0.30, -0.30)'},\n        {'call_delta': 0.25, 'put_delta': -0.25, 'name': '(0.25, -0.25)'},\n        {'call_delta': 0.20, 'put_delta': -0.20, 'name': '(0.20, -0.20)'},\n    ],\n    'Straddle': [\n        {'call_delta': 0.50, 'name': 'ATM (0.50)'},\n    ],\n    'Strangle': [\n        {'call_delta': 0.30, 'put_delta': -0.30, 'name': '(0.30, -0.30)'},\n        {'call_delta': 0.25, 'put_delta': -0.25, 'name': '(0.25, -0.25)'},\n        {'call_delta': 0.20, 'put_delta': -0.20, 'name': '(0.20, -0.20)'},\n        {'call_delta': 0.16, 'put_delta': -0.16, 'name': '(0.16, -0.16)'},\n    ],\n    'Bull Put Spread': [\n        {'put_delta': -0.30, 'spread_width': 5, 'name': 'Delta -0.30 (5pt)'},\n        {'put_delta': -0.30, 'spread_width': 10, 'name': 'Delta -0.30 (10pt)'},\n        {'put_delta': -0.20, 'spread_width': 5, 'name': 'Delta -0.20 (5pt)'},\n        {'put_delta': -0.20, 'spread_width': 10, 'name': 'Delta -0.20 (10pt)'},\n        {'put_delta': -0.16, 'spread_width': 5, 'name': 'Delta -0.16 (5pt)'},\n        {'put_delta': -0.10, 'spread_width': 5, 'name': 'Delta -0.10 (5pt)'},\n    ],\n    'Bull Call Spread': [\n        {'call_delta': 0.50, 'spread_width': 5, 'name': 'Delta 0.50 (5pt)'},\n        {'call_delta': 0.50, 'spread_width': 10, 'name': 'Delta 0.50 (10pt)'},\n        {'call_delta': 0.40, 'spread_width': 5, 'name': 'Delta 0.40 (5pt)'},\n        {'call_delta': 0.40, 'spread_width': 10, 'name': 'Delta 0.40 (10pt)'},\n        {'call_delta': 0.30, 'spread_width': 5, 'name': 'Delta 0.30 (5pt)'},\n    ],\n}\n\n\ndef backtest_strategy_with_real_data(ticker: str, strategy_type: str, \n                                      delta_config: dict, dates: pd.DatetimeIndex,\n                                      underlying_prices: pd.Series,\n                                      vol_regime: pd.Series = None,\n                                      include_transaction_costs: bool = True) -> dict:\n    \"\"\"\n    Backtest a strategy using real QuantConnect options data.\n    Uses Greeks from QuantConnect API.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    strategy_type : str\n        Type of options strategy\n    delta_config : dict\n        Delta configuration for the strategy\n    dates : pd.DatetimeIndex\n        Trading dates\n    underlying_prices : pd.Series\n        Underlying asset prices\n    vol_regime : pd.Series\n        Volatility regime classification\n    include_transaction_costs : bool\n        Include bid/ask spread in P&L\n        \n    Returns:\n    --------\n    dict\n        Performance metrics\n    \"\"\"\n    returns = []\n    regime_returns = {}  # Track returns by regime\n    \n    # Sample dates for backtest (every 5 trading days to avoid API overload)\n    sample_dates = dates[::5]\n    \n    current_position = None\n    entry_value = 0\n    entry_regime = None\n    \n    for i, date in enumerate(sample_dates):\n        try:\n            if date not in underlying_prices.index:\n                continue\n            \n            underlying_price = underlying_prices.loc[date]\n            \n            # Get contracts for this strategy using QuantConnect Greeks\n            contracts = get_strategy_contracts(\n                ticker=ticker,\n                date=date,\n                underlying_price=underlying_price,\n                strategy_type=strategy_type,\n                delta_config=delta_config\n            )\n            \n            if not contracts:\n                continue\n            \n            # Calculate position value\n            position_value = calculate_position_value(contracts, strategy_type, \n                                                       include_transaction_costs)\n            \n            # Calculate return if we have a previous position\n            if current_position is not None and entry_value != 0:\n                pnl = position_value - entry_value\n                daily_return = pnl / abs(entry_value) if entry_value != 0 else 0\n                returns.append({'date': date, 'return': daily_return, 'regime': entry_regime})\n                \n                # Track by regime\n                if entry_regime:\n                    if entry_regime not in regime_returns:\n                        regime_returns[entry_regime] = []\n                    regime_returns[entry_regime].append(daily_return)\n            \n            # Update position\n            current_position = contracts\n            entry_value = position_value\n            entry_regime = vol_regime.loc[date] if vol_regime is not None and date in vol_regime.index else None\n            \n        except Exception as e:\n            continue\n    \n    # Calculate metrics\n    if not returns:\n        return None\n    \n    returns_df = pd.DataFrame(returns)\n    returns_series = returns_df['return']\n    \n    total_return = (1 + returns_series).prod() - 1\n    n_periods = len(returns_series)\n    annual_factor = 252 / (n_periods * 5) if n_periods > 0 else 1  # Adjust for sampling\n    annual_return = (1 + total_return) ** annual_factor - 1 if n_periods > 0 else 0\n    volatility_ann = returns_series.std() * np.sqrt(252 / 5)\n    sharpe = annual_return / volatility_ann if volatility_ann > 0 else 0\n    \n    # Drawdown\n    cumulative = (1 + returns_series).cumprod()\n    running_max = cumulative.expanding().max()\n    drawdown = (cumulative - running_max) / running_max\n    max_drawdown = drawdown.min() if len(drawdown) > 0 else 0\n    \n    # Win rate\n    win_rate = (returns_series > 0).mean()\n    \n    # Regime-specific metrics\n    regime_metrics = {}\n    for regime, regime_rets in regime_returns.items():\n        if len(regime_rets) > 0:\n            regime_metrics[regime] = {\n                'avg_return': np.mean(regime_rets),\n                'win_rate': sum(1 for r in regime_rets if r > 0) / len(regime_rets),\n                'count': len(regime_rets)\n            }\n    \n    return {\n        'strategy': strategy_type,\n        'delta_config': delta_config.get('name', str(delta_config)),\n        'total_return': total_return,\n        'annual_return': annual_return,\n        'volatility': volatility_ann,\n        'sharpe': sharpe,\n        'max_drawdown': max_drawdown,\n        'win_rate': win_rate,\n        'n_trades': n_periods,\n        'regime_metrics': regime_metrics\n    }\n\n\ndef calculate_position_value(contracts: dict, strategy_type: str,\n                             include_transaction_costs: bool = True) -> float:\n    \"\"\"\n    Calculate position value from QuantConnect option contracts.\n    \n    Uses bid/ask spread for transaction cost estimation.\n    \"\"\"\n    value = 0\n    \n    for leg_name, contract in contracts.items():\n        if not isinstance(contract, dict):\n            continue\n        \n        # Use mid price, adjusted for transaction costs\n        bid = contract.get('bid', 0) or 0\n        ask = contract.get('ask', 0) or 0\n        mid = contract.get('mid', 0) or ((bid + ask) / 2 if bid and ask else 0)\n        \n        if include_transaction_costs:\n            # For buys, use ask; for sells, use bid\n            is_short = 'short' in leg_name.lower() or 'front' in leg_name.lower()\n            price = bid if is_short else ask\n        else:\n            price = mid\n        \n        # Determine sign based on leg type\n        if 'short' in leg_name.lower() or 'front' in leg_name.lower():\n            value -= price  # Credit for short positions\n        else:\n            value += price  # Debit for long positions\n    \n    return value\n\n\n# Run systematic delta testing for all strategies\nprint(\"Running systematic delta configuration testing...\")\nprint(\"=\" * 60)\n\ndelta_test_results = []\n\nfor strategy in STRATEGIES:\n    print(f\"\\nTesting {strategy}...\")\n    \n    configs = DELTA_CONFIGS.get(strategy, [{'name': 'default'}])\n    \n    for config in configs:\n        result = backtest_strategy_with_real_data(\n            ticker=TICKER,\n            strategy_type=strategy,\n            delta_config=config,\n            dates=df_asset.index,\n            underlying_prices=df_asset['close'],\n            vol_regime=vol_regime if 'vol_regime' in dir() else None,\n            include_transaction_costs=True\n        )\n        \n        if result:\n            delta_test_results.append(result)\n            print(f\"  {config.get('name', 'default')}: Sharpe={result['sharpe']:.2f}, \"\n                  f\"Return={result['total_return']:.2%}\")\n\n# Create results DataFrame\ndelta_results_df = pd.DataFrame(delta_test_results)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Delta Configuration Test Results Summary:\")\nprint(delta_results_df[['strategy', 'delta_config', 'sharpe', 'total_return', \n                        'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B.1 Delta Configuration Heatmap\n\nVisualization showing returns by strategy \u00d7 delta configuration to identify optimal deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# DELTA CONFIGURATION HEATMAP VISUALIZATION\n# ============================================================================\n\ndef plot_delta_heatmap(results_df: pd.DataFrame):\n    \"\"\"\n    Create heatmap showing returns by strategy and delta configuration.\n    \"\"\"\n    if results_df.empty:\n        print(\"No results to plot\")\n        return\n    \n    # Pivot for heatmap\n    pivot = results_df.pivot_table(\n        index='strategy',\n        columns='delta_config',\n        values='sharpe',\n        aggfunc='mean'\n    )\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Create heatmap with diverging colormap\n    vmax = max(abs(pivot.values.min()), abs(pivot.values.max())) if pivot.values.size > 0 else 1\n    \n    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n                ax=ax, linewidths=0.5, cbar_kws={'label': 'Sharpe Ratio'})\n    \n    ax.set_title(f'{TICKER} - Strategy Performance by Delta Configuration', fontsize=14)\n    ax.set_xlabel('Delta Configuration')\n    ax.set_ylabel('Strategy')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Identify optimal delta for each strategy\n    print(\"\\nOptimal Delta Configuration by Strategy:\")\n    print(\"-\" * 50)\n    for strategy in pivot.index:\n        row = pivot.loc[strategy].dropna()\n        if len(row) > 0:\n            best_delta = row.idxmax()\n            best_sharpe = row.max()\n            print(f\"  {strategy}: {best_delta} (Sharpe: {best_sharpe:.2f})\")\n\n\n# Plot delta configuration heatmap\nif len(delta_results_df) > 0:\n    plot_delta_heatmap(delta_results_df)\nelse:\n    print(\"No delta test results available for heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B.2 Optimal Delta by Volatility Regime\n\nAnalysis of how optimal delta changes across different volatility regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# OPTIMAL DELTA BY VOLATILITY REGIME\n# ============================================================================\n\ndef analyze_delta_by_regime(results_df: pd.DataFrame):\n    \"\"\"\n    Analyze how optimal delta changes across volatility regimes.\n    \"\"\"\n    regime_delta_analysis = []\n    \n    for _, row in results_df.iterrows():\n        strategy = row['strategy']\n        delta_config = row['delta_config']\n        regime_metrics = row.get('regime_metrics', {})\n        \n        if regime_metrics:\n            for regime, metrics in regime_metrics.items():\n                regime_delta_analysis.append({\n                    'strategy': strategy,\n                    'delta_config': delta_config,\n                    'regime': regime,\n                    'avg_return': metrics['avg_return'],\n                    'win_rate': metrics['win_rate'],\n                    'count': metrics['count']\n                })\n    \n    if not regime_delta_analysis:\n        print(\"No regime-specific data available\")\n        return pd.DataFrame()\n    \n    regime_df = pd.DataFrame(regime_delta_analysis)\n    \n    # For each strategy and regime, find optimal delta\n    print(\"Optimal Delta Configuration by Strategy and Regime:\")\n    print(\"=\" * 70)\n    \n    for strategy in regime_df['strategy'].unique():\n        print(f\"\\n{strategy}:\")\n        strategy_data = regime_df[regime_df['strategy'] == strategy]\n        \n        for regime in ['Low', 'Normal', 'Elevated', 'Extreme']:\n            regime_data = strategy_data[strategy_data['regime'] == regime]\n            if len(regime_data) > 0 and regime_data['count'].sum() >= 5:\n                # Find best delta by avg return\n                best_idx = regime_data['avg_return'].idxmax()\n                best_row = regime_data.loc[best_idx]\n                print(f\"  {regime}: {best_row['delta_config']} \"\n                      f\"(Avg Return: {best_row['avg_return']:.4f}, \"\n                      f\"Win Rate: {best_row['win_rate']:.0%}, \"\n                      f\"n={best_row['count']})\")\n    \n    return regime_df\n\n\n# Analyze delta by regime\nif len(delta_results_df) > 0:\n    regime_delta_df = analyze_delta_by_regime(delta_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5C. Strategy Execution Rules\n\n**Entry Rules:**\n- Use aggregate volatility model signals for timing\n- Use ensemble regime identification for strategy selection\n- Enter when regime confidence is above threshold (default: 70%)\n\n**Rolling Mechanism:**\n- At expiration (or 5 DTE), automatically roll positions forward\n- Maintain same delta target for rolled positions\n- Track roll costs/credits separately\n- Continue rolling to evaluate long-term regime-based performance\n\n**Position Sizing:**\n- Consistent notional sizing across strategies for fair comparison\n- Default: $10,000 notional per position\n- Adjust for delta-weighted exposure\n\n**Transaction Costs:**\n- Include bid/ask spread in all P&L calculations\n- Use mid price for mark-to-market, execution price for actual trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5C: STRATEGY EXECUTION RULES\n# ============================================================================\n\n# Execution parameters\nEXECUTION_CONFIG = {\n    'notional_size': 10000,           # Consistent notional per trade\n    'min_regime_confidence': 0.70,    # Minimum confidence to trade\n    'days_to_roll': 5,                # Days before expiry to roll\n    'include_transaction_costs': True,\n    'slippage_pct': 0.001,            # Additional slippage estimate\n}\n\n\ndef roll_position(current_contracts: dict, new_date: datetime, \n                  underlying_price: float, strategy_type: str,\n                  delta_config: dict) -> Tuple[dict, float]:\n    \"\"\"\n    Roll positions forward to next available expiration.\n    Maintains same delta target.\n    \n    Parameters:\n    -----------\n    current_contracts : dict\n        Current position contracts\n    new_date : datetime\n        Roll date\n    underlying_price : float\n        Current underlying price\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta configuration to maintain\n        \n    Returns:\n    --------\n    Tuple[dict, float]\n        New contracts and roll cost/credit\n    \"\"\"\n    # Close current position\n    close_value = calculate_position_value(\n        current_contracts, strategy_type, \n        include_transaction_costs=True\n    )\n    \n    # Open new position at same delta\n    new_contracts = get_strategy_contracts(\n        ticker=TICKER,\n        date=new_date,\n        underlying_price=underlying_price,\n        strategy_type=strategy_type,\n        delta_config=delta_config,\n        front_dte=30,\n        back_dte=60\n    )\n    \n    open_value = calculate_position_value(\n        new_contracts, strategy_type,\n        include_transaction_costs=True\n    )\n    \n    roll_cost = open_value - close_value\n    \n    return new_contracts, roll_cost\n\n\ndef execute_strategy_with_rolls(ticker: str, strategy_type: str,\n                                 delta_config: dict, dates: pd.DatetimeIndex,\n                                 underlying_prices: pd.Series,\n                                 vol_regime: pd.Series = None,\n                                 regime_filter: str = None) -> pd.DataFrame:\n    \"\"\"\n    Execute strategy with automatic position rolling.\n    Vectorized where possible.\n    \n    Parameters:\n    -----------\n    ticker : str\n        Underlying ticker\n    strategy_type : str\n        Strategy type\n    delta_config : dict\n        Delta configuration\n    dates : pd.DatetimeIndex\n        Trading dates\n    underlying_prices : pd.Series\n        Underlying prices\n    vol_regime : pd.Series\n        Volatility regime classification\n    regime_filter : str\n        Only trade in this regime (optional)\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Trade log with dates, returns, costs, regimes\n    \"\"\"\n    trades = []\n    current_contracts = None\n    entry_date = None\n    entry_value = 0\n    total_roll_costs = 0\n    \n    sample_dates = dates[::5]  # Sample every 5 trading days\n    \n    for date in sample_dates:\n        try:\n            if date not in underlying_prices.index:\n                continue\n            \n            underlying_price = underlying_prices.loc[date]\n            current_regime = vol_regime.loc[date] if vol_regime is not None and date in vol_regime.index else None\n            \n            # Check regime filter\n            if regime_filter and current_regime != regime_filter:\n                # Close position if we have one and regime changes\n                if current_contracts is not None:\n                    exit_value = calculate_position_value(current_contracts, strategy_type, True)\n                    pnl = exit_value - entry_value\n                    trades.append({\n                        'entry_date': entry_date,\n                        'exit_date': date,\n                        'pnl': pnl,\n                        'return': pnl / abs(entry_value) if entry_value != 0 else 0,\n                        'roll_costs': total_roll_costs,\n                        'regime': current_regime\n                    })\n                    current_contracts = None\n                    total_roll_costs = 0\n                continue\n            \n            # Open new position if needed\n            if current_contracts is None:\n                current_contracts = get_strategy_contracts(\n                    ticker=ticker,\n                    date=date,\n                    underlying_price=underlying_price,\n                    strategy_type=strategy_type,\n                    delta_config=delta_config\n                )\n                if current_contracts:\n                    entry_date = date\n                    entry_value = calculate_position_value(current_contracts, strategy_type, True)\n            else:\n                # Check if we need to roll (simplified: roll every 30 days)\n                days_held = (date - entry_date).days if entry_date else 0\n                if days_held >= 25:  # Roll around 25 days\n                    new_contracts, roll_cost = roll_position(\n                        current_contracts, date, underlying_price,\n                        strategy_type, delta_config\n                    )\n                    if new_contracts:\n                        total_roll_costs += roll_cost\n                        current_contracts = new_contracts\n                        entry_date = date\n                        entry_value = calculate_position_value(current_contracts, strategy_type, True)\n            \n        except Exception as e:\n            continue\n    \n    return pd.DataFrame(trades)\n\n\nprint(\"Strategy execution functions with rolling defined.\")\nprint(f\"Execution config: {EXECUTION_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5D. Cross-Regime Performance Analysis\n\n**CRITICAL:** Trade ALL strategies across ALL regime combinations and analyze performance.\n\nEach day/trade is tagged with:\n- Market regime (from ensemble clustering)\n- Sector regime (from ensemble clustering)\n- Asset regime (from ensemble clustering)\n- Volatility regime (Low/Normal/Elevated/Extreme)\n\nPerformance metrics calculated per regime:\n- Average return per regime\n- Win rate per regime\n- Sharpe ratio per regime\n- Maximum drawdown per regime\n- Average days to profit per regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n# SECTION 5D: CROSS-REGIME PERFORMANCE ANALYSIS\n# ============================================================================\n\ndef run_cross_regime_analysis(ticker: str, strategies: list,\n                               dates: pd.DatetimeIndex,\n                               underlying_prices: pd.Series,\n                               market_regimes: pd.Series = None,\n                               sector_regimes: pd.Series = None,\n                               asset_regimes: pd.Series = None,\n                               vol_regimes: pd.Series = None) -> pd.DataFrame:\n    \"\"\"\n    Run all strategies continuously and tag with all regime types.\n    Vectorized analysis across entire historical period.\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Complete performance data with regime tags\n    \"\"\"\n    all_results = []\n    \n    for strategy in strategies:\n        print(f\"Running {strategy} across all regimes...\")\n        \n        # Get default delta config\n        configs = DELTA_CONFIGS.get(strategy, [{'name': 'default'}])\n        default_config = configs[0] if configs else {'name': 'default'}\n        \n        # Calculate returns using real options data\n        returns = calculate_real_strategy_returns(\n            ticker=ticker,\n            dates=dates,\n            underlying_prices=underlying_prices,\n            strategy=strategy,\n            delta_config=default_config\n        )\n        \n        # Tag each return with regime information\n        for date in returns.index:\n            if returns.loc[date] == 0:\n                continue\n            \n            regime_tags = {\n                'date': date,\n                'strategy': strategy,\n                'return': returns.loc[date],\n            }\n            \n            # Add regime tags\n            if market_regimes is not None and date in market_regimes.index:\n                regime_tags['market_regime'] = market_regimes.loc[date]\n            if sector_regimes is not None and date in sector_regimes.index:\n                regime_tags['sector_regime'] = sector_regimes.loc[date]\n            if asset_regimes is not None and date in asset_regimes.index:\n                regime_tags['asset_regime'] = asset_regimes.loc[date]\n            if vol_regimes is not None and date in vol_regimes.index:\n                regime_tags['vol_regime'] = vol_regimes.loc[date]\n            \n            all_results.append(regime_tags)\n    \n    return pd.DataFrame(all_results)\n\n\ndef calculate_regime_specific_metrics(results_df: pd.DataFrame, \n                                       regime_col: str) -> pd.DataFrame:\n    \"\"\"\n    Calculate performance metrics grouped by strategy and regime.\n    \"\"\"\n    metrics = []\n    \n    for (strategy, regime), group in results_df.groupby(['strategy', regime_col]):\n        returns = group['return']\n        \n        if len(returns) < 5:\n            continue\n        \n        # Calculate metrics\n        total_return = (1 + returns).prod() - 1\n        avg_return = returns.mean()\n        win_rate = (returns > 0).mean()\n        \n        # Sharpe (annualized)\n        vol = returns.std() * np.sqrt(252 / 5)  # Adjust for sampling\n        sharpe = (avg_return * 252 / 5) / vol if vol > 0 else 0\n        \n        # Drawdown\n        cumulative = (1 + returns).cumprod()\n        running_max = cumulative.expanding().max()\n        max_dd = ((cumulative - running_max) / running_max).min()\n        \n        # Days to profit (simplified)\n        profitable_trades = returns[returns > 0]\n        avg_days_to_profit = 5 if len(profitable_trades) > 0 else np.nan\n        \n        metrics.append({\n            'strategy': strategy,\n            'regime': regime,\n            'n_trades': len(returns),\n            'total_return': total_return,\n            'avg_return': avg_return,\n            'win_rate': win_rate,\n            'sharpe': sharpe,\n            'max_drawdown': max_dd,\n            'avg_days_to_profit': avg_days_to_profit\n        })\n    \n    return pd.DataFrame(metrics)\n\n\n# Run cross-regime analysis\nprint(\"Running cross-regime performance analysis...\")\nprint(\"=\" * 60)\n\ncross_regime_results = run_cross_regime_analysis(\n    ticker=TICKER,\n    strategies=STRATEGIES,\n    dates=df_asset.index,\n    underlying_prices=df_asset['close'],\n    market_regimes=market_regimes if 'market_regimes' in dir() else None,\n    sector_regimes=sector_regimes if 'sector_regimes' in dir() else None,\n    asset_regimes=asset_regimes if 'asset_regimes' in dir() else None,\n    vol_regimes=vol_regime if 'vol_regime' in dir() else None\n)\n\nprint(f\"\\nTotal trade observations: {len(cross_regime_results)}\")\n\n# Calculate metrics by volatility regime\nif 'vol_regime' in cross_regime_results.columns:\n    vol_metrics = calculate_regime_specific_metrics(cross_regime_results, 'vol_regime')\n    print(\"\\nPerformance by Volatility Regime:\")\n    print(vol_metrics.to_string(index=False))\n\n# Calculate metrics by market regime\nif 'market_regime' in cross_regime_results.columns:\n    market_metrics = calculate_regime_specific_metrics(cross_regime_results, 'market_regime')\n    print(\"\\nPerformance by Market Regime:\")\n    print(market_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 PRIORITY VISUALIZATION #1: Strategy Equity Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #1: Strategy Equity Curves\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate cumulative returns for each strategy\n",
    "cumulative_returns = (1 + strategy_returns_df).cumprod()\n",
    "\n",
    "# Calculate buy-and-hold benchmark\n",
    "benchmark_returns = df_asset['returns'].loc[strategy_returns_df.index]\n",
    "benchmark_cumulative = (1 + benchmark_returns.fillna(0)).cumprod()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot each strategy with consistent colors\n",
    "for strategy in STRATEGIES:\n",
    "    color = get_strategy_color(strategy)\n",
    "    ax.plot(cumulative_returns[strategy], label=strategy, color=color, linewidth=1.5)\n",
    "\n",
    "# Plot benchmark\n",
    "ax.plot(benchmark_cumulative, label='Buy & Hold', color='gray', \n",
    "        linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Add regime shading\n",
    "if len(vol_regime.dropna()) > 0:\n",
    "    regime_aligned = vol_regime.reindex(cumulative_returns.index, method='ffill')\n",
    "    \n",
    "    # Shade by regime\n",
    "    prev_regime = None\n",
    "    regime_start = cumulative_returns.index[0]\n",
    "    \n",
    "    for idx, regime in regime_aligned.items():\n",
    "        if regime != prev_regime and prev_regime is not None:\n",
    "            color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, idx, alpha=0.1, color=color)\n",
    "            regime_start = idx\n",
    "        prev_regime = regime\n",
    "    \n",
    "    # Final regime\n",
    "    if prev_regime:\n",
    "        color = get_vol_regime_color(prev_regime)\n",
    "        ax.axvspan(regime_start, cumulative_returns.index[-1], alpha=0.1, color=color)\n",
    "\n",
    "ax.set_title(f'{TICKER} - Strategy Equity Curves Comparison', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return (1 = Starting Value)')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=1, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add regime legend\n",
    "regime_patches = [mpatches.Patch(color=get_vol_regime_color(r), alpha=0.3, label=r) \n",
    "                  for r in ['Low', 'Normal', 'Elevated', 'Extreme']]\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0] + regime_patches, \n",
    "          loc='upper left', fontsize=9, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final returns\n",
    "print(\"\\nFinal Cumulative Returns:\")\n",
    "for strategy in STRATEGIES:\n",
    "    final_return = cumulative_returns[strategy].iloc[-1] - 1\n",
    "    print(f\"  {strategy}: {final_return:.2%}\")\n",
    "print(f\"  Buy & Hold: {benchmark_cumulative.iloc[-1] - 1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio (Individual Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_rolling_sharpe(returns: pd.Series, window: int = 60) -> pd.Series:\n",
    "    \"\"\"Calculate rolling Sharpe ratio.\"\"\"\n",
    "    rolling_mean = returns.rolling(window=window).mean() * 252\n",
    "    rolling_std = returns.rolling(window=window).std() * np.sqrt(252)\n",
    "    return rolling_mean / rolling_std\n",
    "\n",
    "# Create separate plots for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Calculate rolling Sharpe\n",
    "    rolling_sharpe = calculate_rolling_sharpe(strategy_returns_df[strategy], ROLLING_SHARPE_WINDOW)\n",
    "    \n",
    "    # Plot smoothed line\n",
    "    smoothed = rolling_sharpe.rolling(10).mean()\n",
    "    ax.plot(smoothed, color=color, linewidth=2, label=f'{strategy} (60-day rolling)')\n",
    "    \n",
    "    # Add confidence bands (\u00b11 std)\n",
    "    rolling_std = rolling_sharpe.rolling(20).std()\n",
    "    ax.fill_between(smoothed.index, \n",
    "                    smoothed - rolling_std, \n",
    "                    smoothed + rolling_std,\n",
    "                    alpha=0.2, color=color)\n",
    "    \n",
    "    # Add regime background shading\n",
    "    regime_aligned = vol_regime.reindex(rolling_sharpe.index, method='ffill')\n",
    "    \n",
    "    prev_regime = None\n",
    "    regime_start = rolling_sharpe.dropna().index[0] if len(rolling_sharpe.dropna()) > 0 else None\n",
    "    \n",
    "    if regime_start is not None:\n",
    "        for idx in rolling_sharpe.dropna().index:\n",
    "            regime = regime_aligned.get(idx)\n",
    "            if regime != prev_regime and prev_regime is not None:\n",
    "                regime_color = get_vol_regime_color(prev_regime)\n",
    "                ax.axvspan(regime_start, idx, alpha=0.15, color=regime_color)\n",
    "                regime_start = idx\n",
    "            prev_regime = regime\n",
    "        \n",
    "        # Final segment\n",
    "        if prev_regime:\n",
    "            regime_color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, rolling_sharpe.dropna().index[-1], alpha=0.15, color=regime_color)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(y=1, color='green', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = 1')\n",
    "    ax.axhline(y=-1, color='red', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = -1')\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Rolling Sharpe Ratio ({ROLLING_SHARPE_WINDOW}-day window)', \n",
    "                 fontsize=14, color=color)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sharpe Ratio')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overall Sharpe annotation\n",
    "    overall_sharpe = (strategy_returns_df[strategy].mean() * 252) / (strategy_returns_df[strategy].std() * np.sqrt(252))\n",
    "    ax.annotate(f'Overall Sharpe: {overall_sharpe:.2f}', \n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Stop Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_strategy_parameters(strategy: str, underlying_returns: pd.Series,\n",
    "                                 volatility: pd.Series, vol_regime: pd.Series,\n",
    "                                 stop_loss_thresholds: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Test multiple stop loss thresholds for a strategy.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get base returns\n",
    "    base_returns = simulate_strategy_returns(\n",
    "        underlying_returns=underlying_returns,\n",
    "        volatility=volatility,\n",
    "        vol_regime=vol_regime,\n",
    "        strategy=strategy\n",
    "    )\n",
    "    \n",
    "    for stop in stop_loss_thresholds:\n",
    "        # Apply stop loss\n",
    "        adjusted_returns = apply_stop_loss(base_returns, stop)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = (1 + adjusted_returns).prod() - 1\n",
    "        annual_return = (1 + total_return) ** (252 / len(adjusted_returns)) - 1 if len(adjusted_returns) > 0 else 0\n",
    "        vol_ann = adjusted_returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / vol_ann if vol_ann > 0 else 0\n",
    "        \n",
    "        cumulative = (1 + adjusted_returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        win_rate = (adjusted_returns > 0).mean()\n",
    "        \n",
    "        # Count stopped trades\n",
    "        if stop is not None:\n",
    "            n_stopped = (base_returns < stop).sum()\n",
    "        else:\n",
    "            n_stopped = 0\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'stop_loss': f'{stop:.0%}' if stop else 'None',\n",
    "            'stop_value': stop if stop else 0,\n",
    "            'total_return': total_return,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'win_rate': win_rate,\n",
    "            'n_stopped': n_stopped\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run stop loss optimization for each strategy\n",
    "stop_loss_results = []\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    results = optimize_strategy_parameters(\n",
    "        strategy=strategy,\n",
    "        underlying_returns=df_asset['returns'],\n",
    "        volatility=realized_vol['rv_true'],\n",
    "        vol_regime=vol_regime,\n",
    "        stop_loss_thresholds=STOP_LOSS_THRESHOLDS\n",
    "    )\n",
    "    stop_loss_results.append(results)\n",
    "\n",
    "stop_loss_df = pd.concat(stop_loss_results, ignore_index=True)\n",
    "\n",
    "print(\"Stop Loss Optimization Results:\")\n",
    "print(stop_loss_df[['strategy', 'stop_loss', 'total_return', 'sharpe', 'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS VISUALIZATION - Separate plots per strategy\n",
    "# ============================================================================\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    strategy_data = stop_loss_df[stop_loss_df['strategy'] == strategy]\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Max Drawdown vs Total Return scatter\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        -strategy_data['max_drawdown'] * 100,  # Convert to positive %\n",
    "        strategy_data['total_return'] * 100,\n",
    "        c=range(len(strategy_data)),\n",
    "        cmap='viridis',\n",
    "        s=100,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "    \n",
    "    # Annotate points\n",
    "    for idx, row in strategy_data.iterrows():\n",
    "        ax1.annotate(row['stop_loss'],\n",
    "                    xy=(-row['max_drawdown']*100, row['total_return']*100),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel('Max Drawdown (%)')\n",
    "    ax1.set_ylabel('Total Return (%)')\n",
    "    ax1.set_title(f'{strategy} - Return vs Drawdown by Stop Loss', fontsize=12, color=color)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Bar chart of Sharpe by stop loss\n",
    "    ax2 = axes[1]\n",
    "    bars = ax2.bar(strategy_data['stop_loss'], strategy_data['sharpe'], color=color, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('Stop Loss Threshold')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.set_title(f'{strategy} - Sharpe Ratio by Stop Loss', fontsize=12, color=color)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify optimal stop loss\n",
    "    best_row = strategy_data.loc[strategy_data['sharpe'].idxmax()]\n",
    "    print(f\"{strategy} - Optimal Stop Loss: {best_row['stop_loss']} (Sharpe: {best_row['sharpe']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Market Regime Analysis Using Ensemble Clustering\n",
    "\n",
    "This section implements regime detection using an ensemble of:\n",
    "- **Gaussian Mixture Models (GMM)** - Probabilistic clustering\n",
    "- **K-Means Clustering** - Hard cluster assignments\n",
    "- **Agglomerative Clustering** - Hierarchical clustering\n",
    "- **Change-Point Detection (CPD)** - Structural break detection\n",
    "\n",
    "### Why Ensemble Approach?\n",
    "\n",
    "| Method | Strengths | Weaknesses |\n",
    "|--------|-----------|------------|\n",
    "| GMM | Probability distributions, overlapping clusters | Sensitive to initialization |\n",
    "| K-Means | Fast, simple, spherical clusters | Assumes equal cluster sizes |\n",
    "| Agglomerative | Hierarchical structure, no shape assumption | Computationally intensive |\n",
    "| CPD | Explicit transition detection | May miss gradual changes |\n",
    "\n",
    "The ensemble combines these methods to reduce individual weaknesses and provide confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Feature Engineering for Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: ENSEMBLE REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_regime_features(data: pd.DataFrame, feature_set: str = 'comprehensive') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for regime detection.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        OHLCV data with 'close', 'high', 'low', 'volume' columns\n",
    "    feature_set : str\n",
    "        'comprehensive', 'returns_only', 'volatility_only'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Feature DataFrame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Ensure we have returns\n",
    "    if 'returns' not in data.columns:\n",
    "        data = data.copy()\n",
    "        data['returns'] = data['close'].pct_change()\n",
    "    \n",
    "    # 1. Return-based features\n",
    "    features['returns_1d'] = data['returns']\n",
    "    features['returns_5d'] = data['close'].pct_change(5)\n",
    "    features['returns_10d'] = data['close'].pct_change(10)\n",
    "    features['returns_20d'] = data['close'].pct_change(20)\n",
    "    features['returns_50d'] = data['close'].pct_change(50)\n",
    "    \n",
    "    # Return volatility\n",
    "    features['vol_5d'] = data['returns'].rolling(5).std()\n",
    "    features['vol_10d'] = data['returns'].rolling(10).std()\n",
    "    features['vol_20d'] = data['returns'].rolling(20).std()\n",
    "    \n",
    "    # Skewness and kurtosis\n",
    "    features['skew_20d'] = data['returns'].rolling(20).skew()\n",
    "    features['kurt_20d'] = data['returns'].rolling(20).kurt()\n",
    "    \n",
    "    if feature_set in ['comprehensive', 'volatility_only']:\n",
    "        # 2. Volatility features\n",
    "        features['vol_of_vol'] = features['vol_20d'].rolling(20).std()\n",
    "        \n",
    "        if 'high' in data.columns and 'low' in data.columns:\n",
    "            features['hl_range'] = (data['high'] - data['low']) / data['close']\n",
    "            features['hl_range_20d'] = features['hl_range'].rolling(20).mean()\n",
    "    \n",
    "    if feature_set == 'comprehensive':\n",
    "        # 3. Trend features\n",
    "        features['sma_20'] = data['close'].rolling(20).mean()\n",
    "        features['sma_50'] = data['close'].rolling(50).mean()\n",
    "        features['sma_200'] = data['close'].rolling(200).mean()\n",
    "        \n",
    "        features['price_sma20_ratio'] = data['close'] / features['sma_20']\n",
    "        features['price_sma50_ratio'] = data['close'] / features['sma_50']\n",
    "        features['sma20_sma50_ratio'] = features['sma_20'] / features['sma_50']\n",
    "        \n",
    "        # Linear regression slope (trend strength)\n",
    "        def calc_slope(series, window=20):\n",
    "            def slope(x):\n",
    "                if len(x) < window:\n",
    "                    return np.nan\n",
    "                y = x.values\n",
    "                x_vals = np.arange(len(y))\n",
    "                slope, _ = np.polyfit(x_vals, y, 1)\n",
    "                return slope\n",
    "            return series.rolling(window).apply(slope, raw=False)\n",
    "        \n",
    "        features['trend_slope_20d'] = calc_slope(data['close'], 20)\n",
    "        \n",
    "        # 4. Momentum features\n",
    "        # RSI\n",
    "        delta = data['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        features['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        ema_12 = data['close'].ewm(span=12, adjust=False).mean()\n",
    "        ema_26 = data['close'].ewm(span=26, adjust=False).mean()\n",
    "        features['macd'] = ema_12 - ema_26\n",
    "        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()\n",
    "        features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "        \n",
    "        # Rate of change\n",
    "        features['roc_10'] = data['close'].pct_change(10)\n",
    "        features['roc_20'] = data['close'].pct_change(20)\n",
    "        \n",
    "        # 5. Volume features (if available)\n",
    "        if 'volume' in data.columns:\n",
    "            features['volume_sma_20'] = data['volume'].rolling(20).mean()\n",
    "            features['volume_ratio'] = data['volume'] / features['volume_sma_20']\n",
    "    \n",
    "    # Drop columns used for intermediate calculations\n",
    "    cols_to_keep = [c for c in features.columns if not c.startswith('sma_')]\n",
    "    features = features[cols_to_keep]\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "\n",
    "# Engineer features for market, sector, and asset\n",
    "print(\"Engineering features...\")\n",
    "\n",
    "market_features = engineer_regime_features(df_market, 'comprehensive')\n",
    "print(f\"Market features: {market_features.shape}\")\n",
    "\n",
    "sector_features = engineer_regime_features(df_sector, 'comprehensive')\n",
    "print(f\"Sector features: {sector_features.shape}\")\n",
    "\n",
    "asset_features = engineer_regime_features(df_asset, 'comprehensive')\n",
    "print(f\"Asset features: {asset_features.shape}\")\n",
    "\n",
    "print(f\"\\nFeature columns ({len(market_features.columns)}):\")\n",
    "for col in market_features.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Individual Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLUSTERING METHODS\n",
    "# ============================================================================\n",
    "\n",
    "def fit_gmm_regimes(features: pd.DataFrame, n_components_range: range = range(3, 7)) -> Tuple[np.ndarray, GaussianMixture, int]:\n",
    "    \"\"\"\n",
    "    Fit Gaussian Mixture Model for regime detection.\n",
    "    Uses BIC for optimal component selection.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, GaussianMixture, int]\n",
    "        Labels, fitted model, optimal n_components\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Find optimal n_components using BIC\n",
    "    bic_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for n in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n, random_state=42, n_init=5)\n",
    "        gmm.fit(X_scaled)\n",
    "        bic_scores.append(gmm.bic(X_scaled))\n",
    "        models.append(gmm)\n",
    "    \n",
    "    # Select model with lowest BIC\n",
    "    best_idx = np.argmin(bic_scores)\n",
    "    best_n = list(n_components_range)[best_idx]\n",
    "    best_model = models[best_idx]\n",
    "    \n",
    "    # Get labels and probabilities\n",
    "    labels = best_model.predict(X_scaled)\n",
    "    probabilities = best_model.predict_proba(X_scaled)\n",
    "    \n",
    "    print(f\"GMM: Optimal components = {best_n} (BIC = {bic_scores[best_idx]:.2f})\")\n",
    "    \n",
    "    return labels, best_model, best_n, probabilities\n",
    "\n",
    "\n",
    "def fit_kmeans_regimes(features: pd.DataFrame, k_range: range = range(3, 7)) -> Tuple[np.ndarray, KMeans, int]:\n",
    "    \"\"\"\n",
    "    Fit K-Means clustering for regime detection.\n",
    "    Uses silhouette score for optimal k selection.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        models.append(kmeans)\n",
    "    \n",
    "    best_idx = np.argmax(silhouette_scores)\n",
    "    best_k = list(k_range)[best_idx]\n",
    "    best_model = models[best_idx]\n",
    "    labels = best_model.predict(X_scaled)\n",
    "    \n",
    "    # Calculate distance to cluster centers\n",
    "    distances = best_model.transform(X_scaled)\n",
    "    min_distances = distances.min(axis=1)\n",
    "    \n",
    "    print(f\"K-Means: Optimal k = {best_k} (Silhouette = {silhouette_scores[best_idx]:.3f})\")\n",
    "    \n",
    "    return labels, best_model, best_k, min_distances\n",
    "\n",
    "\n",
    "def fit_agglomerative_regimes(features: pd.DataFrame, n_clusters_range: range = range(3, 7)) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Fit Agglomerative (Hierarchical) clustering.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for n in n_clusters_range:\n",
    "        agg = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
    "        labels = agg.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    best_idx = np.argmax(silhouette_scores)\n",
    "    best_n = list(n_clusters_range)[best_idx]\n",
    "    best_labels = all_labels[best_idx]\n",
    "    \n",
    "    print(f\"Agglomerative: Optimal clusters = {best_n} (Silhouette = {silhouette_scores[best_idx]:.3f})\")\n",
    "    \n",
    "    return best_labels, best_n\n",
    "\n",
    "\n",
    "def detect_change_points(features: pd.DataFrame, method: str = 'pelt', \n",
    "                         n_bkps: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect structural breaks using change-point detection.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Regime labels based on detected segments\n",
    "    \"\"\"\n",
    "    if not RUPTURES_AVAILABLE:\n",
    "        print(\"Warning: ruptures not available, returning uniform segments\")\n",
    "        return np.zeros(len(features))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    if method == 'pelt':\n",
    "        algo = rpt.Pelt(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(pen=3)\n",
    "    elif method == 'binseg':\n",
    "        algo = rpt.Binseg(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(n_bkps=n_bkps or 10)\n",
    "    else:\n",
    "        algo = rpt.BottomUp(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(n_bkps=n_bkps or 10)\n",
    "    \n",
    "    # Convert change points to labels\n",
    "    labels = np.zeros(len(features))\n",
    "    prev_bkp = 0\n",
    "    for i, bkp in enumerate(result):\n",
    "        labels[prev_bkp:bkp] = i\n",
    "        prev_bkp = bkp\n",
    "    \n",
    "    print(f\"CPD ({method}): {len(result)} change points detected\")\n",
    "    \n",
    "    return labels, result\n",
    "\n",
    "\n",
    "print(\"Clustering functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ensemble Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def ensemble_regime_classification(gmm_labels: np.ndarray, kmeans_labels: np.ndarray,\n",
    "                                   agg_labels: np.ndarray, cpd_labels: np.ndarray,\n",
    "                                   gmm_probs: np.ndarray = None,\n",
    "                                   kmeans_distances: np.ndarray = None,\n",
    "                                   weights: dict = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Combine multiple clustering methods using weighted voting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gmm_labels, kmeans_labels, agg_labels, cpd_labels : np.ndarray\n",
    "        Labels from each method\n",
    "    gmm_probs : np.ndarray\n",
    "        GMM probability matrix (for confidence weighting)\n",
    "    kmeans_distances : np.ndarray\n",
    "        K-means distances to cluster centers (for confidence weighting)\n",
    "    weights : dict\n",
    "        Base weights for each method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        Ensemble labels and confidence scores\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {'gmm': 0.3, 'kmeans': 0.25, 'agg': 0.25, 'cpd': 0.2}\n",
    "    \n",
    "    n_samples = len(gmm_labels)\n",
    "    \n",
    "    # Find number of unique regimes across all methods\n",
    "    all_unique = max(\n",
    "        len(np.unique(gmm_labels)),\n",
    "        len(np.unique(kmeans_labels)),\n",
    "        len(np.unique(agg_labels)),\n",
    "        len(np.unique(cpd_labels))\n",
    "    )\n",
    "    \n",
    "    # Create voting matrix\n",
    "    ensemble_labels = np.zeros(n_samples, dtype=int)\n",
    "    confidence_scores = np.zeros(n_samples)\n",
    "    \n",
    "    # For each sample, use majority voting\n",
    "    for i in range(n_samples):\n",
    "        votes = [\n",
    "            (gmm_labels[i], weights['gmm']),\n",
    "            (kmeans_labels[i], weights['kmeans']),\n",
    "            (agg_labels[i], weights['agg']),\n",
    "            (cpd_labels[i], weights['cpd'])\n",
    "        ]\n",
    "        \n",
    "        # Aggregate votes by label\n",
    "        vote_counts = {}\n",
    "        for label, weight in votes:\n",
    "            vote_counts[label] = vote_counts.get(label, 0) + weight\n",
    "        \n",
    "        # Select winner\n",
    "        winner = max(vote_counts.keys(), key=lambda x: vote_counts[x])\n",
    "        ensemble_labels[i] = winner\n",
    "        \n",
    "        # Confidence = fraction of weighted votes for winner\n",
    "        total_weight = sum(weights.values())\n",
    "        confidence_scores[i] = vote_counts[winner] / total_weight\n",
    "    \n",
    "    return ensemble_labels, confidence_scores\n",
    "\n",
    "\n",
    "def map_regime_labels(labels: np.ndarray, features: pd.DataFrame,\n",
    "                      feature_names: list = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map numeric cluster labels to meaningful regime names based on feature centroids.\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['returns_20d', 'vol_20d']\n",
    "    \n",
    "    # Calculate centroids\n",
    "    df = features.copy()\n",
    "    df['cluster'] = labels\n",
    "    \n",
    "    centroids = df.groupby('cluster')[feature_names].mean()\n",
    "    \n",
    "    # Define regime mapping based on centroid characteristics\n",
    "    regime_names = {}\n",
    "    \n",
    "    for cluster in centroids.index:\n",
    "        ret_col = [c for c in feature_names if 'return' in c.lower()]\n",
    "        vol_col = [c for c in feature_names if 'vol' in c.lower()]\n",
    "        \n",
    "        if ret_col and vol_col:\n",
    "            avg_return = centroids.loc[cluster, ret_col[0]]\n",
    "            avg_vol = centroids.loc[cluster, vol_col[0]]\n",
    "            \n",
    "            # Determine regime name\n",
    "            if avg_return > 0 and avg_vol < centroids[vol_col[0]].median():\n",
    "                regime_names[cluster] = 'Bull_Low_Vol'\n",
    "            elif avg_return > 0 and avg_vol >= centroids[vol_col[0]].median():\n",
    "                regime_names[cluster] = 'Bull_High_Vol'\n",
    "            elif avg_return < 0:\n",
    "                regime_names[cluster] = 'Bear'\n",
    "            else:\n",
    "                regime_names[cluster] = 'Choppy'\n",
    "        else:\n",
    "            regime_names[cluster] = f'Regime_{cluster}'\n",
    "    \n",
    "    return pd.Series([regime_names.get(l, 'Unknown') for l in labels], index=features.index)\n",
    "\n",
    "\n",
    "print(\"Ensemble functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Run Ensemble Regime Detection on Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - MARKET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MARKET REGIME DETECTION (SPY/QQQ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_mkt, gmm_model_mkt, gmm_n_mkt, gmm_probs_mkt = fit_gmm_regimes(market_features)\n",
    "kmeans_labels_mkt, kmeans_model_mkt, kmeans_k_mkt, kmeans_dist_mkt = fit_kmeans_regimes(market_features)\n",
    "agg_labels_mkt, agg_n_mkt = fit_agglomerative_regimes(market_features)\n",
    "cpd_labels_mkt, cpd_bkps_mkt = detect_change_points(market_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_mkt, confidence_mkt = ensemble_regime_classification(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    gmm_probs=gmm_probs_mkt, kmeans_distances=kmeans_dist_mkt\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "market_regimes = map_regime_labels(ensemble_labels_mkt, market_features, \n",
    "                                   ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nMarket Regime Distribution:\")\n",
    "print(market_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_mkt.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - SECTOR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"SECTOR REGIME DETECTION ({SECTOR_ETF})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_sec, gmm_model_sec, gmm_n_sec, gmm_probs_sec = fit_gmm_regimes(sector_features)\n",
    "kmeans_labels_sec, kmeans_model_sec, kmeans_k_sec, kmeans_dist_sec = fit_kmeans_regimes(sector_features)\n",
    "agg_labels_sec, agg_n_sec = fit_agglomerative_regimes(sector_features)\n",
    "cpd_labels_sec, cpd_bkps_sec = detect_change_points(sector_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_sec, confidence_sec = ensemble_regime_classification(\n",
    "    gmm_labels_sec, kmeans_labels_sec, agg_labels_sec, cpd_labels_sec,\n",
    "    gmm_probs=gmm_probs_sec, kmeans_distances=kmeans_dist_sec\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "sector_regimes = map_regime_labels(ensemble_labels_sec, sector_features,\n",
    "                                   ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nSector Regime Distribution:\")\n",
    "print(sector_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_sec.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - ASSET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ASSET REGIME DETECTION ({TICKER})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_ast, gmm_model_ast, gmm_n_ast, gmm_probs_ast = fit_gmm_regimes(asset_features)\n",
    "kmeans_labels_ast, kmeans_model_ast, kmeans_k_ast, kmeans_dist_ast = fit_kmeans_regimes(asset_features)\n",
    "agg_labels_ast, agg_n_ast = fit_agglomerative_regimes(asset_features)\n",
    "cpd_labels_ast, cpd_bkps_ast = detect_change_points(asset_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_ast, confidence_ast = ensemble_regime_classification(\n",
    "    gmm_labels_ast, kmeans_labels_ast, agg_labels_ast, cpd_labels_ast,\n",
    "    gmm_probs=gmm_probs_ast, kmeans_distances=kmeans_dist_ast\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "asset_regimes = map_regime_labels(ensemble_labels_ast, asset_features,\n",
    "                                  ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nAsset Regime Distribution:\")\n",
    "print(asset_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_ast.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Regime Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REGIME VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_regime_clustering(features: pd.DataFrame, labels: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Validate clustering quality using multiple metrics.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    metrics = {\n",
    "        'silhouette': silhouette_score(X_scaled, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(X_scaled, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels),\n",
    "        'n_clusters': len(np.unique(labels)),\n",
    "        'cluster_sizes': dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Validate all regime detections\n",
    "print(\"Regime Validation Metrics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "market_validation = validate_regime_clustering(market_features, ensemble_labels_mkt)\n",
    "print(f\"\\nMarket Regimes:\")\n",
    "print(f\"  Silhouette Score: {market_validation['silhouette']:.3f} (higher is better)\")\n",
    "print(f\"  Davies-Bouldin Index: {market_validation['davies_bouldin']:.3f} (lower is better)\")\n",
    "print(f\"  Calinski-Harabasz Index: {market_validation['calinski_harabasz']:.1f} (higher is better)\")\n",
    "\n",
    "sector_validation = validate_regime_clustering(sector_features, ensemble_labels_sec)\n",
    "print(f\"\\nSector Regimes:\")\n",
    "print(f\"  Silhouette Score: {sector_validation['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {sector_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {sector_validation['calinski_harabasz']:.1f}\")\n",
    "\n",
    "asset_validation = validate_regime_clustering(asset_features, ensemble_labels_ast)\n",
    "print(f\"\\nAsset Regimes:\")\n",
    "print(f\"  Silhouette Score: {asset_validation['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {asset_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {asset_validation['calinski_harabasz']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 PRIORITY VISUALIZATION #3: Regime Probability Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3a: Market Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_probabilities(price_data: pd.DataFrame, regime_series: pd.Series,\n",
    "                              confidence_scores: np.ndarray, cpd_breakpoints: list,\n",
    "                              title: str, figsize: tuple = (14, 10)):\n",
    "    \"\"\"\n",
    "    Create regime visualization with price, regimes, and confidence.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize, gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "    \n",
    "    # Align data\n",
    "    common_idx = price_data.index.intersection(regime_series.index)\n",
    "    price = price_data.loc[common_idx, 'close']\n",
    "    regimes = regime_series.loc[common_idx]\n",
    "    conf = pd.Series(confidence_scores, index=regime_series.index).loc[common_idx]\n",
    "    \n",
    "    # Panel 1: Price with regime shading\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(price, color='black', linewidth=1)\n",
    "    \n",
    "    # Add regime shading\n",
    "    prev_regime = None\n",
    "    segment_start = common_idx[0]\n",
    "    \n",
    "    for idx in common_idx:\n",
    "        current_regime = regimes.loc[idx]\n",
    "        if current_regime != prev_regime and prev_regime is not None:\n",
    "            color = get_regime_color(prev_regime)\n",
    "            ax1.axvspan(segment_start, idx, alpha=0.3, color=color, label=prev_regime)\n",
    "            segment_start = idx\n",
    "        prev_regime = current_regime\n",
    "    \n",
    "    # Final segment\n",
    "    if prev_regime:\n",
    "        color = get_regime_color(prev_regime)\n",
    "        ax1.axvspan(segment_start, common_idx[-1], alpha=0.3, color=color)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for regimes\n",
    "    unique_regimes = regimes.unique()\n",
    "    patches = [mpatches.Patch(color=get_regime_color(r), alpha=0.5, label=r) \n",
    "               for r in unique_regimes]\n",
    "    ax1.legend(handles=patches, loc='upper left', fontsize=9)\n",
    "    \n",
    "    # Panel 2: Regime classification\n",
    "    ax2 = axes[1]\n",
    "    regime_numeric = pd.Categorical(regimes).codes\n",
    "    ax2.plot(common_idx, regime_numeric, drawstyle='steps-post', color='navy', linewidth=1)\n",
    "    ax2.set_ylabel('Regime')\n",
    "    ax2.set_yticks(range(len(unique_regimes)))\n",
    "    ax2.set_yticklabels(unique_regimes)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add CPD breakpoints\n",
    "    if cpd_breakpoints:\n",
    "        for bkp in cpd_breakpoints[:-1]:  # Exclude last (end of data)\n",
    "            if bkp < len(common_idx):\n",
    "                ax2.axvline(x=common_idx[bkp], color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Panel 3: Confidence score\n",
    "    ax3 = axes[2]\n",
    "    ax3.fill_between(common_idx, conf, alpha=0.5, color='green')\n",
    "    ax3.plot(common_idx, conf, color='darkgreen', linewidth=1)\n",
    "    ax3.axhline(y=0.7, color='orange', linestyle='--', label='Min confidence (0.7)')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.set_xlabel('Date')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.legend(loc='lower right', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot Market Regimes\n",
    "plot_regime_probabilities(\n",
    "    df_market, market_regimes, confidence_mkt, cpd_bkps_mkt,\n",
    "    f'{MARKET_PROXY} - Market Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3b: Sector Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "plot_regime_probabilities(\n",
    "    df_sector, sector_regimes, confidence_sec, cpd_bkps_sec,\n",
    "    f'{SECTOR_ETF} - Sector Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3c: Asset Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "plot_regime_probabilities(\n",
    "    df_asset, asset_regimes, confidence_ast, cpd_bkps_ast,\n",
    "    f'{TICKER} - Asset Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Method Agreement Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD AGREEMENT VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_comparison(gmm_labels, kmeans_labels, agg_labels, cpd_labels,\n",
    "                           ensemble_labels, index, title):\n",
    "    \"\"\"\n",
    "    Compare regime assignments from each individual method.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    methods = [\n",
    "        ('GMM', gmm_labels),\n",
    "        ('K-Means', kmeans_labels),\n",
    "        ('Agglomerative', agg_labels),\n",
    "        ('CPD', cpd_labels),\n",
    "        ('Ensemble', ensemble_labels)\n",
    "    ]\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    \n",
    "    for ax, (method, labels), color in zip(axes, methods, colors):\n",
    "        ax.plot(index, labels, drawstyle='steps-post', color=color, linewidth=1)\n",
    "        ax.fill_between(index, labels, alpha=0.3, step='post', color=color)\n",
    "        ax.set_ylabel(method, fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight ensemble row\n",
    "        if method == 'Ensemble':\n",
    "            ax.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    axes[0].set_title(title, fontsize=14)\n",
    "    axes[-1].set_xlabel('Date')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot method comparison for market\n",
    "plot_regime_comparison(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    ensemble_labels_mkt, market_features.index,\n",
    "    f'{MARKET_PROXY} - Regime Method Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONSENSUS HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "def plot_ensemble_consensus(gmm_labels, kmeans_labels, agg_labels, cpd_labels, index, title):\n",
    "    \"\"\"\n",
    "    Show agreement level between methods over time.\n",
    "    \"\"\"\n",
    "    # Calculate agreement matrix\n",
    "    n_samples = len(gmm_labels)\n",
    "    agreement = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        labels = [gmm_labels[i], kmeans_labels[i], agg_labels[i], cpd_labels[i]]\n",
    "        # Count most common label\n",
    "        most_common_count = max(labels.count(l) for l in set(labels))\n",
    "        agreement[i] = most_common_count / 4  # 4 methods\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Color by agreement level\n",
    "    colors = np.where(agreement >= 0.75, '#2ecc71',  # High agreement\n",
    "              np.where(agreement >= 0.5, '#f39c12',  # Moderate\n",
    "                       '#e74c3c'))  # Low agreement\n",
    "    \n",
    "    ax.scatter(index, agreement, c=colors, s=10, alpha=0.7)\n",
    "    ax.axhline(y=0.75, color='green', linestyle='--', label='High agreement (75%+)')\n",
    "    ax.axhline(y=0.5, color='orange', linestyle='--', label='Moderate (50%)')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Agreement Level')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Agreement Statistics:\")\n",
    "    print(f\"  High agreement (75%+): {(agreement >= 0.75).mean():.1%}\")\n",
    "    print(f\"  Moderate (50-75%): {((agreement >= 0.5) & (agreement < 0.75)).mean():.1%}\")\n",
    "    print(f\"  Low (<50%): {(agreement < 0.5).mean():.1%}\")\n",
    "\n",
    "\n",
    "plot_ensemble_consensus(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    market_features.index,\n",
    "    f'{MARKET_PROXY} - Ensemble Method Agreement Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Optimal Strategy Selection & Output\n",
    "\n",
    "This section synthesizes all analysis to determine optimal strategy allocation by regime.\n",
    "\n",
    "### 7.1 Strategy Performance by Regime Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: OPTIMAL STRATEGY SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create combined regime DataFrame\n",
    "common_idx = market_regimes.index.intersection(sector_regimes.index).intersection(asset_regimes.index)\n",
    "common_idx = common_idx.intersection(vol_regime.dropna().index)\n",
    "common_idx = common_idx.intersection(strategy_returns_df.index)\n",
    "\n",
    "regime_df = pd.DataFrame({\n",
    "    'market_regime': market_regimes.loc[common_idx],\n",
    "    'sector_regime': sector_regimes.loc[common_idx],\n",
    "    'asset_regime': asset_regimes.loc[common_idx],\n",
    "    'vol_regime': vol_regime.loc[common_idx],\n",
    "    'market_confidence': pd.Series(confidence_mkt, index=market_regimes.index).loc[common_idx],\n",
    "    'sector_confidence': pd.Series(confidence_sec, index=sector_regimes.index).loc[common_idx],\n",
    "    'asset_confidence': pd.Series(confidence_ast, index=asset_regimes.index).loc[common_idx]\n",
    "}, index=common_idx)\n",
    "\n",
    "# Add strategy returns\n",
    "for strategy in STRATEGIES:\n",
    "    regime_df[f'{strategy}_return'] = strategy_returns_df[strategy].loc[common_idx]\n",
    "\n",
    "print(f\"Combined regime data: {len(regime_df)} observations\")\n",
    "print(f\"\\nRegime combinations:\")\n",
    "combo_counts = regime_df.groupby(['market_regime', 'sector_regime']).size()\n",
    "print(combo_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY PERFORMANCE BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_performance(regime_df: pd.DataFrame, strategies: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate strategy performance metrics by regime combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by market and sector regime\n",
    "    groups = regime_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt_regime, sec_regime), group in groups:\n",
    "        for strategy in strategies:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in group.columns:\n",
    "                returns = group[col].dropna()\n",
    "                \n",
    "                if len(returns) >= 10:  # Minimum sample size\n",
    "                    total_return = (1 + returns).prod() - 1\n",
    "                    avg_return = returns.mean()\n",
    "                    vol = returns.std()\n",
    "                    sharpe = (avg_return * 252) / (vol * np.sqrt(252)) if vol > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    # Drawdown\n",
    "                    cumulative = (1 + returns).cumprod()\n",
    "                    running_max = cumulative.expanding().max()\n",
    "                    max_dd = ((cumulative - running_max) / running_max).min()\n",
    "                    \n",
    "                    # Average confidence\n",
    "                    avg_conf = (group['market_confidence'] + group['sector_confidence']).mean() / 2\n",
    "                    \n",
    "                    results.append({\n",
    "                        'market_regime': mkt_regime,\n",
    "                        'sector_regime': sec_regime,\n",
    "                        'strategy': strategy,\n",
    "                        'avg_return': avg_return,\n",
    "                        'total_return': total_return,\n",
    "                        'sharpe': sharpe,\n",
    "                        'win_rate': win_rate,\n",
    "                        'max_drawdown': max_dd,\n",
    "                        'sample_size': len(returns),\n",
    "                        'confidence': avg_conf\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "regime_performance = calculate_regime_performance(regime_df, STRATEGIES)\n",
    "\n",
    "print(\"Strategy Performance by Regime:\")\n",
    "print(regime_performance.sort_values('sharpe', ascending=False).head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 PRIORITY VISUALIZATION #4: Strategy Performance Heatmaps by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #4: Performance Heatmaps by Strategy\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_performance_heatmap(perf_df: pd.DataFrame, strategy: str):\n",
    "    \"\"\"\n",
    "    Create heatmap showing strategy performance across regime combinations.\n",
    "    \"\"\"\n",
    "    strategy_data = perf_df[perf_df['strategy'] == strategy]\n",
    "    \n",
    "    if strategy_data.empty:\n",
    "        print(f\"No data for {strategy}\")\n",
    "        return\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    pivot_return = strategy_data.pivot(index='market_regime', columns='sector_regime', \n",
    "                                        values='avg_return').fillna(0)\n",
    "    pivot_winrate = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                         values='win_rate').fillna(0)\n",
    "    pivot_sample = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                        values='sample_size').fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIG_SIZE_HEATMAP)\n",
    "    \n",
    "    # Main heatmap with returns\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Use diverging colormap\n",
    "    vmax = max(abs(pivot_return.values.min()), abs(pivot_return.values.max()))\n",
    "    vmin = -vmax\n",
    "    \n",
    "    sns.heatmap(pivot_return * 100, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, center=0, vmin=vmin*100, vmax=vmax*100,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Avg Daily Return (%)'})\n",
    "    \n",
    "    # Add sample size annotations\n",
    "    for i in range(len(pivot_return.index)):\n",
    "        for j in range(len(pivot_return.columns)):\n",
    "            n = pivot_sample.iloc[i, j]\n",
    "            wr = pivot_winrate.iloc[i, j]\n",
    "            if n > 0:\n",
    "                ax.annotate(f'n={int(n)}\\nWR={wr:.0%}',\n",
    "                           xy=(j + 0.5, i + 0.7),\n",
    "                           ha='center', va='center',\n",
    "                           fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Performance by Regime Combination', \n",
    "                 fontsize=14, color=color, fontweight='bold')\n",
    "    ax.set_xlabel('Sector Regime')\n",
    "    ax.set_ylabel('Market Regime')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create separate heatmaps for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    plot_regime_performance_heatmap(regime_performance, strategy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Optimal Strategy Selection by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMAL STRATEGY BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_strategy(perf_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the optimal strategy for each regime combination.\n",
    "    \"\"\"\n",
    "    # For each regime combination, find best strategy by Sharpe\n",
    "    optimal = []\n",
    "    \n",
    "    groups = perf_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt, sec), group in groups:\n",
    "        if len(group) > 0:\n",
    "            best = group.loc[group['sharpe'].idxmax()]\n",
    "            \n",
    "            # Also find optimal stop loss (simplified - use best overall for now)\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best['strategy']]\n",
    "            if len(stop_data) > 0:\n",
    "                best_stop = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                best_stop = 'None'\n",
    "            \n",
    "            optimal.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Recommended_Strategy': best['strategy'],\n",
    "                'Expected_Return': best['avg_return'],\n",
    "                'Sharpe': best['sharpe'],\n",
    "                'Max_DD': best['max_drawdown'],\n",
    "                'Win_Rate': best['win_rate'],\n",
    "                'Sample_Size': best['sample_size'],\n",
    "                'Ensemble_Confidence': best['confidence'],\n",
    "                'Stop_Loss_%': best_stop\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(optimal)\n",
    "\n",
    "\n",
    "optimal_strategies = find_optimal_strategy(regime_performance)\n",
    "\n",
    "print(\"OPTIMAL STRATEGY ALLOCATION BY REGIME:\")\n",
    "print(\"=\"*80)\n",
    "print(optimal_strategies.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE OUTPUT TABLE\n",
    "# ============================================================================\n",
    "\n",
    "# Add volatility regime and asset regime for complete picture\n",
    "def create_comprehensive_recommendations() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive strategy recommendations table.\n",
    "    \"\"\"\n",
    "    # Get unique regime combinations\n",
    "    combos = regime_df.groupby(['market_regime', 'sector_regime', 'asset_regime', 'vol_regime']).size()\n",
    "    combos = combos[combos >= 5].reset_index(name='sample_size')  # Min 5 days\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for _, row in combos.iterrows():\n",
    "        mkt, sec, ast, vol = row['market_regime'], row['sector_regime'], row['asset_regime'], row['vol_regime']\n",
    "        \n",
    "        # Filter data for this regime combination\n",
    "        mask = (\n",
    "            (regime_df['market_regime'] == mkt) &\n",
    "            (regime_df['sector_regime'] == sec) &\n",
    "            (regime_df['asset_regime'] == ast) &\n",
    "            (regime_df['vol_regime'] == vol)\n",
    "        )\n",
    "        subset = regime_df[mask]\n",
    "        \n",
    "        if len(subset) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Find best strategy\n",
    "        best_strategy = None\n",
    "        best_sharpe = -np.inf\n",
    "        best_return = 0\n",
    "        best_winrate = 0\n",
    "        \n",
    "        for strategy in STRATEGIES:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in subset.columns:\n",
    "                returns = subset[col].dropna()\n",
    "                if len(returns) >= 5:\n",
    "                    avg_ret = returns.mean()\n",
    "                    vol_ret = returns.std()\n",
    "                    sharpe = (avg_ret * 252) / (vol_ret * np.sqrt(252)) if vol_ret > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    if sharpe > best_sharpe:\n",
    "                        best_sharpe = sharpe\n",
    "                        best_strategy = strategy\n",
    "                        best_return = avg_ret\n",
    "                        best_winrate = win_rate\n",
    "        \n",
    "        if best_strategy:\n",
    "            # Determine delta based on strategy type\n",
    "            call_delta = 'N/A'\n",
    "            put_delta = 'N/A'\n",
    "            \n",
    "            if 'Call' in best_strategy:\n",
    "                call_delta = '0.40'\n",
    "            if 'Put' in best_strategy:\n",
    "                put_delta = '-0.20'\n",
    "            if best_strategy in ['Straddle', 'Strangle']:\n",
    "                call_delta = '0.30'\n",
    "                put_delta = '-0.30'\n",
    "            if 'Calendar' in best_strategy or 'Diagonal' in best_strategy:\n",
    "                call_delta = '0.50'\n",
    "            \n",
    "            # Get recommended stop loss\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best_strategy]\n",
    "            if len(stop_data) > 0:\n",
    "                stop_loss = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                stop_loss = 'None'\n",
    "            \n",
    "            # Average confidence\n",
    "            avg_conf = subset[['market_confidence', 'sector_confidence', 'asset_confidence']].mean().mean()\n",
    "            \n",
    "            recommendations.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Regime_Asset': ast,\n",
    "                'Volatility_Regime': vol,\n",
    "                'Ensemble_Confidence': f'{avg_conf:.2f}',\n",
    "                'Recommended_Strategy': best_strategy,\n",
    "                'Call_Delta': call_delta,\n",
    "                'Put_Delta': put_delta,\n",
    "                'Stop_Loss_%': stop_loss,\n",
    "                'Expected_Return': f'{best_return*100:.2f}%',\n",
    "                'Sharpe': f'{best_sharpe:.2f}',\n",
    "                'Win_Rate': f'{best_winrate:.0%}',\n",
    "                'Sample_Size': len(subset)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "\n",
    "comprehensive_recommendations = create_comprehensive_recommendations()\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE STRATEGY RECOMMENDATIONS:\")\n",
    "print(\"=\"*120)\n",
    "print(comprehensive_recommendations.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# comprehensive_recommendations.to_csv('strategy_recommendations.csv', index=False)\n",
    "# print(\"\\nRecommendations saved to strategy_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Current Regime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CURRENT REGIME ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Get current (most recent) regime states\n",
    "current_market = market_regimes.iloc[-1]\n",
    "current_sector = sector_regimes.iloc[-1]\n",
    "current_asset = asset_regimes.iloc[-1]\n",
    "current_vol = vol_regime.iloc[-1] if len(vol_regime) > 0 else 'Unknown'\n",
    "\n",
    "current_market_conf = confidence_mkt[-1] if len(confidence_mkt) > 0 else 0\n",
    "current_sector_conf = confidence_sec[-1] if len(confidence_sec) > 0 else 0\n",
    "current_asset_conf = confidence_ast[-1] if len(confidence_ast) > 0 else 0\n",
    "\n",
    "print(\"CURRENT REGIME STATES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {regime_df.index[-1]}\")\n",
    "print(f\"\\nMarket Regime: {current_market} (Confidence: {current_market_conf:.2%})\")\n",
    "print(f\"Sector Regime: {current_sector} (Confidence: {current_sector_conf:.2%})\")\n",
    "print(f\"Asset Regime:  {current_asset} (Confidence: {current_asset_conf:.2%})\")\n",
    "print(f\"Volatility:    {current_vol}\")\n",
    "\n",
    "# Find recommended strategy for current regime\n",
    "current_rec = comprehensive_recommendations[\n",
    "    (comprehensive_recommendations['Regime_Market'] == current_market) &\n",
    "    (comprehensive_recommendations['Regime_Sector'] == current_sector)\n",
    "]\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    print(\"\\nRECOMMENDED STRATEGY FOR CURRENT REGIME:\")\n",
    "    print(\"-\"*60)\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Call Delta: {rec['Call_Delta']}\")\n",
    "    print(f\"Put Delta: {rec['Put_Delta']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "    print(f\"Expected Return: {rec['Expected_Return']}\")\n",
    "    print(f\"Sharpe Ratio: {rec['Sharpe']}\")\n",
    "    print(f\"Win Rate: {rec['Win_Rate']}\")\n",
    "else:\n",
    "    print(\"\\nNo historical data for current regime combination.\")\n",
    "    print(\"Consider using the most similar regime or conservative positioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 PRIORITY VISUALIZATION #5: Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #5: Summary Dashboard\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=FIG_SIZE_DASHBOARD)\n",
    "\n",
    "# Create 3x3 grid\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Current regime states\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "regimes = ['Market', 'Sector', 'Asset']\n",
    "regime_values = [current_market, current_sector, current_asset]\n",
    "confidence_values = [current_market_conf, current_sector_conf, current_asset_conf]\n",
    "colors = [get_regime_color(r) for r in regime_values]\n",
    "\n",
    "bars = ax1.barh(regimes, confidence_values, color=colors, alpha=0.7)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_xlabel('Confidence')\n",
    "ax1.set_title('Current Regime States', fontsize=12)\n",
    "\n",
    "for bar, val, regime in zip(bars, confidence_values, regime_values):\n",
    "    ax1.annotate(f'{regime}\\n{val:.0%}', \n",
    "                xy=(val + 0.02, bar.get_y() + bar.get_height()/2),\n",
    "                va='center', fontsize=9)\n",
    "\n",
    "# Panel 2: Volatility distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "vol_hist = realized_vol['rv_true'].dropna()\n",
    "current_vol_value = vol_hist.iloc[-1] if len(vol_hist) > 0 else 0\n",
    "\n",
    "ax2.hist(vol_hist, bins=30, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "ax2.axvline(x=current_vol_value, color='red', linewidth=2, label=f'Current: {current_vol_value:.2%}')\n",
    "ax2.axvline(x=vol_hist.mean(), color='green', linestyle='--', label=f'Mean: {vol_hist.mean():.2%}')\n",
    "ax2.set_title('Volatility Distribution', fontsize=12)\n",
    "ax2.set_xlabel('Volatility')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Panel 3: Recommended strategy\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    strategy_color = get_strategy_color(rec['Recommended_Strategy'])\n",
    "    \n",
    "    text = f\"RECOMMENDED STRATEGY\\n\\n\"\n",
    "    text += f\"{rec['Recommended_Strategy']}\\n\\n\"\n",
    "    text += f\"Stop Loss: {rec['Stop_Loss_%']}\\n\"\n",
    "    text += f\"Expected Return: {rec['Expected_Return']}\\n\"\n",
    "    text += f\"Sharpe: {rec['Sharpe']}\"\n",
    "    \n",
    "    ax3.text(0.5, 0.5, text, ha='center', va='center', fontsize=11,\n",
    "             transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor=strategy_color, alpha=0.3))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No recommendation\\nfor current regime', \n",
    "             ha='center', va='center', fontsize=11, transform=ax3.transAxes)\n",
    "\n",
    "ax3.set_title('Strategy Recommendation', fontsize=12)\n",
    "\n",
    "# Panel 4: Performance metrics table\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    table_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Win Rate', rec['Win_Rate']],\n",
    "        ['Sharpe', rec['Sharpe']],\n",
    "        ['Sample Size', str(rec['Sample_Size'])],\n",
    "        ['Confidence', rec['Ensemble_Confidence']]\n",
    "    ]\n",
    "    \n",
    "    table = ax4.table(cellText=table_data, loc='center', cellLoc='center',\n",
    "                      colWidths=[0.4, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "\n",
    "ax4.set_title('Expected Performance', fontsize=12)\n",
    "\n",
    "# Panel 5: Historical performance of recommended strategy\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec_strategy = current_rec.iloc[0]['Recommended_Strategy']\n",
    "    strategy_returns_series = strategy_returns_df[rec_strategy]\n",
    "    cumulative = (1 + strategy_returns_series).cumprod()\n",
    "    \n",
    "    color = get_strategy_color(rec_strategy)\n",
    "    ax5.plot(cumulative, color=color, linewidth=1.5)\n",
    "    ax5.set_title(f'{rec_strategy} Historical Performance', fontsize=12)\n",
    "    ax5.set_ylabel('Cumulative Return')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 6: Risk warnings\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.axis('off')\n",
    "\n",
    "# Check for warnings\n",
    "warnings = []\n",
    "avg_conf = (current_market_conf + current_sector_conf + current_asset_conf) / 3\n",
    "\n",
    "if avg_conf < 0.6:\n",
    "    warnings.append(\"\u26a0\ufe0f Low regime confidence\")\n",
    "if current_vol == 'Extreme':\n",
    "    warnings.append(\"\u26a0\ufe0f Extreme volatility detected\")\n",
    "if len(current_rec) == 0:\n",
    "    warnings.append(\"\u26a0\ufe0f Novel regime combination\")\n",
    "if len(current_rec) > 0 and int(current_rec.iloc[0]['Sample_Size']) < 20:\n",
    "    warnings.append(\"\u26a0\ufe0f Limited historical data\")\n",
    "\n",
    "if warnings:\n",
    "    warning_text = \"RISK WARNINGS\\n\\n\" + \"\\n\".join(warnings)\n",
    "    ax6.text(0.5, 0.5, warning_text, ha='center', va='center', fontsize=10,\n",
    "             transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.5))\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, '\u2705 No significant warnings\\n\\nRegime stable with\\nhigh confidence',\n",
    "             ha='center', va='center', fontsize=10, transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.5))\n",
    "\n",
    "ax6.set_title('Risk Alerts', fontsize=12)\n",
    "\n",
    "# Panel 7: Method agreement pie chart\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "# Calculate method agreement\n",
    "agreement_levels = {\n",
    "    'High (75%+)': (confidence_mkt >= 0.75).mean(),\n",
    "    'Moderate (50-75%)': ((confidence_mkt >= 0.5) & (confidence_mkt < 0.75)).mean(),\n",
    "    'Low (<50%)': (confidence_mkt < 0.5).mean()\n",
    "}\n",
    "\n",
    "colors_pie = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax7.pie([v for v in agreement_levels.values()], labels=agreement_levels.keys(),\n",
    "        autopct='%1.0f%%', colors=colors_pie, startangle=90)\n",
    "ax7.set_title('Ensemble Agreement Distribution', fontsize=12)\n",
    "\n",
    "# Panel 8: PCA feature space (simplified 2D)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "# Quick PCA for visualization\n",
    "scaler_viz = StandardScaler()\n",
    "X_viz = scaler_viz.fit_transform(asset_features.dropna())\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca = pca_viz.fit_transform(X_viz)\n",
    "\n",
    "# Color by regime\n",
    "colors_scatter = [get_regime_color(r) for r in asset_regimes.loc[asset_features.dropna().index]]\n",
    "ax8.scatter(X_pca[:, 0], X_pca[:, 1], c=colors_scatter, alpha=0.5, s=10)\n",
    "ax8.scatter(X_pca[-1, 0], X_pca[-1, 1], c='black', s=100, marker='*', \n",
    "            label='Current', edgecolors='white', linewidth=1)\n",
    "ax8.set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.0%})')\n",
    "ax8.set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.0%})')\n",
    "ax8.set_title('Feature Space Position', fontsize=12)\n",
    "ax8.legend(loc='upper right', fontsize=9)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 9: Regime transition probabilities\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "# Get transition probabilities from current regime\n",
    "if current_vol in transition_probs.index:\n",
    "    trans = transition_probs.loc[current_vol]\n",
    "    colors_bar = [get_vol_regime_color(r) for r in trans.index]\n",
    "    ax9.bar(trans.index, trans.values, color=colors_bar, alpha=0.7)\n",
    "    ax9.set_ylim(0, 1)\n",
    "    ax9.set_ylabel('Probability')\n",
    "    ax9.set_title(f'Next Regime Probability\\n(from {current_vol})', fontsize=12)\n",
    "    ax9.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax9.text(0.5, 0.5, 'Transition data\\nnot available', \n",
    "             ha='center', va='center', transform=ax9.transAxes)\n",
    "    ax9.set_title('Regime Transition', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'{TICKER} - Volatility Trading Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Portfolio Extension Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PORTFOLIO EXTENSION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "# This section provides structure for future multi-asset portfolio optimization\n",
    "\n",
    "class VolatilityTradingPortfolio:\n",
    "    \"\"\"\n",
    "    Framework for multi-asset volatility trading portfolio.\n",
    "    \n",
    "    Designed to be extended for portfolio-level optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tickers: List[str]):\n",
    "        self.tickers = tickers\n",
    "        self.asset_data = {}\n",
    "        self.regime_data = {}\n",
    "        self.strategy_allocations = {}\n",
    "        \n",
    "    def add_asset(self, ticker: str, data: pd.DataFrame, regimes: pd.Series):\n",
    "        \"\"\"Add asset data to portfolio.\"\"\"\n",
    "        self.asset_data[ticker] = data\n",
    "        self.regime_data[ticker] = regimes\n",
    "        \n",
    "    def calculate_cross_asset_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate correlation matrix across assets.\"\"\"\n",
    "        returns_df = pd.DataFrame()\n",
    "        for ticker, data in self.asset_data.items():\n",
    "            returns_df[ticker] = data['returns'] if 'returns' in data.columns else data['close'].pct_change()\n",
    "        return returns_df.corr()\n",
    "    \n",
    "    def calculate_portfolio_greeks(self, positions: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate aggregate portfolio Greeks.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        positions : dict\n",
    "            Dictionary of position sizes by ticker and strategy\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Aggregate delta, gamma, theta, vega\n",
    "        \"\"\"\n",
    "        # Placeholder for portfolio-level Greek calculations\n",
    "        return {\n",
    "            'portfolio_delta': 0,\n",
    "            'portfolio_gamma': 0,\n",
    "            'portfolio_theta': 0,\n",
    "            'portfolio_vega': 0\n",
    "        }\n",
    "    \n",
    "    def check_regime_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Check if assets share regimes simultaneously.\"\"\"\n",
    "        regime_df = pd.DataFrame()\n",
    "        for ticker, regimes in self.regime_data.items():\n",
    "            regime_df[ticker] = pd.Categorical(regimes).codes\n",
    "        return regime_df.corr()\n",
    "    \n",
    "    def optimize_allocation(self, constraints: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Optimize strategy allocation across assets.\n",
    "        \n",
    "        Placeholder for future implementation of:\n",
    "        - Mean-variance optimization\n",
    "        - Risk parity\n",
    "        - Regime-based allocation\n",
    "        \"\"\"\n",
    "        # Placeholder - to be implemented\n",
    "        return {ticker: {'strategy': None, 'weight': 0} for ticker in self.tickers}\n",
    "\n",
    "\n",
    "# Example usage (for future expansion)\n",
    "print(\"Portfolio Extension Framework initialized.\")\n",
    "print(\"\")\n",
    "print(\"Key methods for future implementation:\")\n",
    "print(\"  - add_asset(): Add multiple assets to portfolio\")\n",
    "print(\"  - calculate_cross_asset_correlation(): Diversification analysis\")\n",
    "print(\"  - calculate_portfolio_greeks(): Aggregate risk exposure\")\n",
    "print(\"  - check_regime_correlation(): Regime synchronization\")\n",
    "print(\"  - optimize_allocation(): Multi-asset strategy allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions\n",
    "\n",
    "This notebook has implemented a comprehensive volatility trading research framework including:\n",
    "\n",
    "### Key Components:\n",
    "1. **Volatility Modeling**: Multiple models (Historical, EWMA, Parkinson, Garman-Klass, Rogers-Satchell, ATR) with PCA-based aggregate forecasting\n",
    "\n",
    "2. **Mean Reversion Analysis**: Volatility regime classification with transition probabilities and DTE recommendations\n",
    "\n",
    "3. **Options Strategies**: Six strategies (Calendar Spread, Double Diagonal, Straddle, Strangle, Bull Put Spread, Bull Call Spread) with delta optimization\n",
    "\n",
    "4. **Ensemble Regime Detection**: Combined GMM, K-Means, Agglomerative Clustering, and Change-Point Detection for robust regime identification at Market, Sector, and Asset levels\n",
    "\n",
    "5. **Stop Loss Optimization**: Systematic testing of stop loss thresholds by strategy\n",
    "\n",
    "6. **Comprehensive Output**: Machine-readable recommendations table with regime-specific strategy allocation\n",
    "\n",
    "### Technical Implementation:\n",
    "- All computations use vectorized operations (NumPy/Pandas)\n",
    "- No multiprocessing (QuantConnect compatible)\n",
    "- Ticker-agnostic design for easy portfolio extension\n",
    "- Consistent color palettes across all visualizations\n",
    "\n",
    "### Next Steps:\n",
    "1. Connect to live QuantConnect options data\n",
    "2. Implement actual options pricing models\n",
    "3. Extend to multi-asset portfolio optimization\n",
    "4. Add real-time regime monitoring\n",
    "5. Backtest with actual transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# END OF NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VOLATILITY TRADING RESEARCH NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Ticker analyzed: {TICKER}\")\n",
    "print(f\"Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"\")\n",
    "print(f\"Current Regimes:\")\n",
    "print(f\"  Market: {current_market}\")\n",
    "print(f\"  Sector: {current_sector}\")\n",
    "print(f\"  Asset:  {current_asset}\")\n",
    "print(f\"  Volatility: {current_vol}\")\n",
    "print(f\"\")\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Recommended Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "else:\n",
    "    print(\"No specific recommendation for current regime.\")\n",
    "\n",
    "print(f\"\")\n",
    "print(\"See comprehensive_recommendations DataFrame for full regime-strategy mapping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}