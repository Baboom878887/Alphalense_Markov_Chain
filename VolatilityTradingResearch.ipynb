{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>\n",
    "\n",
    "# Volatility Trading Research Notebook\n",
    "\n",
    "## Options Strategies with Ensemble Regime Detection\n",
    "\n",
    "This notebook implements a comprehensive volatility trading research framework for QuantConnect.\n",
    "\n",
    "**Key Features:**\n",
    "- Multiple volatility models with PCA-based aggregate forecasting\n",
    "- Ensemble regime detection (GMM, K-Means, Agglomerative Clustering, Change-Point Detection)\n",
    "- Six options strategies with delta optimization\n",
    "- Stop loss optimization per strategy and regime\n",
    "- Vectorized computations (no multiprocessing)\n",
    "\n",
    "**Test Ticker:** HIMS (ticker-agnostic design for future portfolio expansion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Initial Setup & Structure\n",
    "\n",
    "### 1.1 Import Libraries and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: INITIAL SETUP & STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "# Standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import json\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Machine Learning - Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Change-Point Detection\n",
    "try:\n",
    "    import ruptures as rpt\n",
    "    RUPTURES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RUPTURES_AVAILABLE = False\n",
    "    print(\"Warning: ruptures library not available. Install with: pip install ruptures\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"Warning: plotly not available for interactive charts\")\n",
    "\n",
    "# Set plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# QuantConnect Research Environment\n",
    "qb = QuantBook()\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Ruptures available: {RUPTURES_AVAILABLE}\")\n",
    "print(f\"Plotly available: {PLOTLY_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Color Palette Definitions\n",
    "\n",
    "**CRITICAL:** These color mappings must be used consistently across ALL visualizations throughout the notebook.\n",
    "\n",
    "#### Strategy Colors\n",
    "| Strategy | Color | Hex Code |\n",
    "|----------|-------|----------|\n",
    "| Calendar Spread | Blue | #1f77b4 |\n",
    "| Double Diagonal | Orange | #ff7f0e |\n",
    "| Straddle | Green | #2ca02c |\n",
    "| Strangle | Red | #d62728 |\n",
    "| Bull Put Spread | Purple | #9467bd |\n",
    "| Bull Call Spread | Brown | #8c564b |\n",
    "\n",
    "#### Regime Colors\n",
    "| Regime | Color | Hex Code |\n",
    "|--------|-------|----------|\n",
    "| Bull_Low_Vol | Green | #2ecc71 |\n",
    "| Bull_High_Vol | Orange | #f39c12 |\n",
    "| Bear | Red | #e74c3c |\n",
    "| Choppy | Gray | #95a5a6 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLOR PALETTE DEFINITIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "STRATEGY_COLORS = {\n",
    "    'Calendar Spread': '#1f77b4',    # Blue\n",
    "    'Double Diagonal': '#ff7f0e',    # Orange\n",
    "    'Straddle': '#2ca02c',           # Green\n",
    "    'Strangle': '#d62728',           # Red\n",
    "    'Bull Put Spread': '#9467bd',    # Purple\n",
    "    'Bull Call Spread': '#8c564b',   # Brown\n",
    "}\n",
    "\n",
    "# Regime color mapping - USE CONSISTENTLY ACROSS ALL VISUALIZATIONS\n",
    "REGIME_COLORS = {\n",
    "    'Bull_Low_Vol': '#2ecc71',       # Green\n",
    "    'Bull_High_Vol': '#f39c12',      # Orange\n",
    "    'Bear': '#e74c3c',               # Red\n",
    "    'Choppy': '#95a5a6',             # Gray\n",
    "    'Sideways': '#95a5a6',           # Gray (alias)\n",
    "    'Unknown': '#bdc3c7',            # Light Gray\n",
    "}\n",
    "\n",
    "# Volatility regime colors\n",
    "VOL_REGIME_COLORS = {\n",
    "    'Low': '#3498db',                # Light Blue\n",
    "    'Normal': '#2ecc71',             # Green\n",
    "    'Elevated': '#f39c12',           # Orange\n",
    "    'Extreme': '#e74c3c',            # Red\n",
    "}\n",
    "\n",
    "# Helper functions for consistent color access\n",
    "def get_strategy_color(strategy_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a strategy.\"\"\"\n",
    "    return STRATEGY_COLORS.get(strategy_name, '#7f7f7f')  # Default gray\n",
    "\n",
    "def get_regime_color(regime_name: str) -> str:\n",
    "    \"\"\"Get consistent color for a regime.\"\"\"\n",
    "    return REGIME_COLORS.get(regime_name, '#bdc3c7')  # Default light gray\n",
    "\n",
    "def get_vol_regime_color(vol_regime: str) -> str:\n",
    "    \"\"\"Get consistent color for a volatility regime.\"\"\"\n",
    "    return VOL_REGIME_COLORS.get(vol_regime, '#bdc3c7')\n",
    "\n",
    "# Display color palettes\n",
    "print(\"Strategy Colors:\")\n",
    "for strategy, color in STRATEGY_COLORS.items():\n",
    "    print(f\"  {strategy}: {color}\")\n",
    "\n",
    "print(\"\\nRegime Colors:\")\n",
    "for regime, color in REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")\n",
    "\n",
    "print(\"\\nVolatility Regime Colors:\")\n",
    "for regime, color in VOL_REGIME_COLORS.items():\n",
    "    print(f\"  {regime}: {color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Primary ticker for analysis (ticker-agnostic design)\n",
    "TICKER = 'HIMS'\n",
    "\n",
    "# Market and sector proxies\n",
    "MARKET_PROXY = 'SPY'\n",
    "SECTOR_MAPPING = {\n",
    "    'Technology': 'XLK',\n",
    "    'Healthcare': 'XLV',\n",
    "    'Financial': 'XLF',\n",
    "    'Consumer Discretionary': 'XLY',\n",
    "    'Consumer Staples': 'XLP',\n",
    "    'Energy': 'XLE',\n",
    "    'Utilities': 'XLU',\n",
    "    'Materials': 'XLB',\n",
    "    'Industrials': 'XLI',\n",
    "    'Real Estate': 'XLRE',\n",
    "    'Communication Services': 'XLC',\n",
    "}\n",
    "\n",
    "# Default sector ETF (HIMS is healthcare)\n",
    "SECTOR_ETF = 'XLV'\n",
    "\n",
    "# Date range for analysis (minimum 2-3 years recommended)\n",
    "START_DATE = datetime(2022, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "\n",
    "# Volatility calculation windows\n",
    "VOL_WINDOWS = [5, 10, 20, 30]\n",
    "\n",
    "# PCA configuration\n",
    "PCA_LAG_PERIODS = [1, 2, 3, 5, 10, 20]\n",
    "PCA_N_COMPONENTS = 3\n",
    "\n",
    "# Regime detection configuration\n",
    "N_REGIMES_RANGE = range(3, 7)  # Test 3-6 regimes\n",
    "DEFAULT_N_REGIMES = 4\n",
    "\n",
    "# Options strategy configurations\n",
    "CALENDAR_DELTAS = [0.50, 0.40, 0.30, 0.20]\n",
    "DIAGONAL_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20)]\n",
    "STRANGLE_DELTA_PAIRS = [(0.30, -0.30), (0.25, -0.25), (0.20, -0.20), (0.16, -0.16)]\n",
    "BULL_PUT_SHORT_DELTAS = [-0.30, -0.20, -0.16, -0.10]\n",
    "BULL_CALL_LONG_DELTAS = [0.50, 0.40, 0.30]\n",
    "SPREAD_WIDTHS = [5, 10]  # Strike width for spreads\n",
    "\n",
    "# Stop loss thresholds to test\n",
    "STOP_LOSS_THRESHOLDS = [-0.10, -0.15, -0.20, -0.25, -0.30, -0.40, -0.50, None]  # None = no stop\n",
    "\n",
    "# Rolling window for metrics\n",
    "ROLLING_SHARPE_WINDOW = 60\n",
    "\n",
    "# Minimum figure sizes\n",
    "FIG_SIZE_TIMESERIES = (14, 6)\n",
    "FIG_SIZE_REGIME = (14, 10)\n",
    "FIG_SIZE_COMPARISON = (14, 8)\n",
    "FIG_SIZE_HEATMAP = (10, 8)\n",
    "FIG_SIZE_DASHBOARD = (22, 16)\n",
    "FIG_SIZE_ROLLING = (12, 5)\n",
    "\n",
    "print(f\"Configuration loaded for ticker: {TICKER}\")\n",
    "print(f\"Analysis period: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"Volatility windows: {VOL_WINDOWS}\")\n",
    "print(f\"Testing {len(STOP_LOSS_THRESHOLDS)} stop loss thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA FETCHING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def fetch_equity_data(ticker: str, start_date: datetime, end_date: datetime, \n",
    "                      resolution: str = 'Daily') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch historical OHLCV data for an equity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : datetime\n",
    "        Start date for data fetch\n",
    "    end_date : datetime\n",
    "        End date for data fetch\n",
    "    resolution : str\n",
    "        Data resolution ('Daily', 'Hour', 'Minute')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Historical OHLCV data\n",
    "    \"\"\"\n",
    "    # Add equity to universe\n",
    "    symbol = qb.AddEquity(ticker, Resolution.Daily).Symbol\n",
    "    \n",
    "    # Fetch historical data\n",
    "    history = qb.History(symbol, start_date, end_date, Resolution.Daily)\n",
    "    \n",
    "    if history.empty:\n",
    "        print(f\"Warning: No data available for {ticker}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Reset index and clean up\n",
    "    df = history.reset_index()\n",
    "    df = df.rename(columns={'time': 'Date'})\n",
    "    df = df.set_index('Date')\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_options_chain(ticker: str, date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch options chain data for a specific date.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Underlying stock ticker\n",
    "    date : datetime\n",
    "        Date to fetch options chain\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Options chain with strikes, prices, Greeks, etc.\n",
    "    \"\"\"\n",
    "    # Add option universe\n",
    "    equity = qb.AddEquity(ticker, Resolution.Daily)\n",
    "    option = qb.AddOption(ticker, Resolution.Daily)\n",
    "    \n",
    "    # Set option filter\n",
    "    option.SetFilter(-10, 10, timedelta(days=0), timedelta(days=90))\n",
    "    \n",
    "    # Get option chain\n",
    "    chain = qb.OptionChainProvider.GetOptionContractList(equity.Symbol, date)\n",
    "    \n",
    "    if not chain:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Build options dataframe\n",
    "    options_data = []\n",
    "    for contract in chain:\n",
    "        try:\n",
    "            # Get contract details\n",
    "            history = qb.History(contract, date, date + timedelta(days=1), Resolution.Daily)\n",
    "            if not history.empty:\n",
    "                row = {\n",
    "                    'symbol': str(contract),\n",
    "                    'strike': contract.ID.StrikePrice,\n",
    "                    'expiry': contract.ID.Date,\n",
    "                    'option_type': 'Call' if contract.ID.OptionRight == OptionRight.Call else 'Put',\n",
    "                    'bid': history['bidprice'].iloc[-1] if 'bidprice' in history.columns else np.nan,\n",
    "                    'ask': history['askprice'].iloc[-1] if 'askprice' in history.columns else np.nan,\n",
    "                    'last': history['close'].iloc[-1] if 'close' in history.columns else np.nan,\n",
    "                    'volume': history['volume'].iloc[-1] if 'volume' in history.columns else np.nan,\n",
    "                }\n",
    "                options_data.append(row)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(options_data)\n",
    "\n",
    "\n",
    "def get_options_chain(ticker: str, date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Alias for fetch_options_chain for API consistency.\"\"\"\n",
    "    return fetch_options_chain(ticker, date)\n",
    "\n",
    "\n",
    "def select_options_by_delta(chain: pd.DataFrame, target_delta: float, \n",
    "                            option_type: str = 'Call') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select options contracts closest to target delta.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    chain : pd.DataFrame\n",
    "        Options chain data\n",
    "    target_delta : float\n",
    "        Target delta value (positive for calls, negative for puts)\n",
    "    option_type : str\n",
    "        'Call' or 'Put'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Filtered options closest to target delta\n",
    "    \"\"\"\n",
    "    if chain.empty or 'delta' not in chain.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter by option type\n",
    "    filtered = chain[chain['option_type'] == option_type].copy()\n",
    "    \n",
    "    if filtered.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate distance from target delta\n",
    "    filtered['delta_distance'] = np.abs(filtered['delta'] - target_delta)\n",
    "    \n",
    "    # Sort by delta distance and return closest\n",
    "    return filtered.nsmallest(1, 'delta_distance')\n",
    "\n",
    "\n",
    "print(\"Data fetching functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Fetch Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FETCH HISTORICAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Fetching data for {TICKER}...\")\n",
    "\n",
    "# Fetch underlying equity data\n",
    "df_asset = fetch_equity_data(TICKER, START_DATE, END_DATE)\n",
    "print(f\"  {TICKER}: {len(df_asset)} days of data\")\n",
    "\n",
    "# Fetch market proxy data\n",
    "df_market = fetch_equity_data(MARKET_PROXY, START_DATE, END_DATE)\n",
    "print(f\"  {MARKET_PROXY}: {len(df_market)} days of data\")\n",
    "\n",
    "# Fetch sector ETF data\n",
    "df_sector = fetch_equity_data(SECTOR_ETF, START_DATE, END_DATE)\n",
    "print(f\"  {SECTOR_ETF}: {len(df_sector)} days of data\")\n",
    "\n",
    "# Align all dataframes to common dates\n",
    "common_dates = df_asset.index.intersection(df_market.index).intersection(df_sector.index)\n",
    "df_asset = df_asset.loc[common_dates]\n",
    "df_market = df_market.loc[common_dates]\n",
    "df_sector = df_sector.loc[common_dates]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nData fetch completed in {elapsed:.2f} seconds\")\n",
    "print(f\"Common trading days: {len(common_dates)}\")\n",
    "print(f\"Date range: {common_dates[0]} to {common_dates[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Volatility Modeling & Comparison\n",
    "\n",
    "This section implements multiple volatility models and creates a PCA-based aggregate forecast.\n",
    "\n",
    "### 2.1 True Realized Volatility Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: VOLATILITY MODELING & COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility(prices: pd.Series, returns: pd.Series = None, \n",
    "                         method: str = 'historical', window: int = 20,\n",
    "                         annualize: bool = True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate volatility using various methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices : pd.Series\n",
    "        Price series (for range-based methods)\n",
    "    returns : pd.Series\n",
    "        Return series (for return-based methods)\n",
    "    method : str\n",
    "        Volatility calculation method\n",
    "    window : int\n",
    "        Rolling window size\n",
    "    annualize : bool\n",
    "        Whether to annualize volatility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Volatility series\n",
    "    \"\"\"\n",
    "    annualization_factor = np.sqrt(252) if annualize else 1\n",
    "    \n",
    "    if returns is None:\n",
    "        returns = prices.pct_change()\n",
    "    \n",
    "    if method == 'historical':\n",
    "        # Simple rolling standard deviation\n",
    "        vol = returns.rolling(window=window).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'ewma':\n",
    "        # Exponentially Weighted Moving Average\n",
    "        vol = returns.ewm(span=window, adjust=False).std() * annualization_factor\n",
    "        \n",
    "    elif method == 'parkinson':\n",
    "        # Parkinson volatility (uses high-low range)\n",
    "        # Requires high/low prices in a DataFrame\n",
    "        if isinstance(prices, pd.DataFrame) and 'high' in prices.columns and 'low' in prices.columns:\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            vol = np.sqrt((log_hl ** 2).rolling(window=window).mean() / (4 * np.log(2))) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'garman_klass':\n",
    "        # Garman-Klass volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hl = np.log(prices['high'] / prices['low'])\n",
    "            log_co = np.log(prices['close'] / prices['open'])\n",
    "            gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "            vol = np.sqrt(gk.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'rogers_satchell':\n",
    "        # Rogers-Satchell volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'open', 'close']):\n",
    "            log_hc = np.log(prices['high'] / prices['close'])\n",
    "            log_ho = np.log(prices['high'] / prices['open'])\n",
    "            log_lc = np.log(prices['low'] / prices['close'])\n",
    "            log_lo = np.log(prices['low'] / prices['open'])\n",
    "            rs = log_hc * log_ho + log_lc * log_lo\n",
    "            vol = np.sqrt(rs.rolling(window=window).mean()) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "            \n",
    "    elif method == 'atr':\n",
    "        # Average True Range based volatility\n",
    "        if isinstance(prices, pd.DataFrame) and all(c in prices.columns for c in ['high', 'low', 'close']):\n",
    "            prev_close = prices['close'].shift(1)\n",
    "            tr = np.maximum(\n",
    "                prices['high'] - prices['low'],\n",
    "                np.maximum(\n",
    "                    np.abs(prices['high'] - prev_close),\n",
    "                    np.abs(prices['low'] - prev_close)\n",
    "                )\n",
    "            )\n",
    "            atr = tr.rolling(window=window).mean()\n",
    "            vol = (atr / prices['close']) * annualization_factor\n",
    "        else:\n",
    "            vol = pd.Series(np.nan, index=prices.index)\n",
    "    else:\n",
    "        vol = pd.Series(np.nan, index=prices.index if hasattr(prices, 'index') else None)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "\n",
    "# Calculate true realized volatility (multiple windows)\n",
    "realized_vol = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "for window in VOL_WINDOWS:\n",
    "    realized_vol[f'rv_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Primary realized volatility (20-day is standard)\n",
    "realized_vol['rv_true'] = realized_vol['rv_20d']\n",
    "\n",
    "print(\"Realized Volatility Statistics:\")\n",
    "print(realized_vol.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Volatility Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTIPLE VOLATILITY MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate all volatility measures\n",
    "vol_models = pd.DataFrame(index=df_asset.index)\n",
    "\n",
    "# 1. Historical volatility (various windows)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'hist_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='historical',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 2. EWMA volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'ewma_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset['close'],\n",
    "        returns=df_asset['returns'],\n",
    "        method='ewma',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 3. Parkinson volatility (high-low based)\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'parkinson_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='parkinson',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 4. Garman-Klass volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'garman_klass_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='garman_klass',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 5. Rogers-Satchell volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'rogers_satchell_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='rogers_satchell',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# 6. ATR-based volatility\n",
    "for window in VOL_WINDOWS:\n",
    "    vol_models[f'atr_vol_{window}d'] = calculate_volatility(\n",
    "        prices=df_asset,\n",
    "        method='atr',\n",
    "        window=window\n",
    "    )\n",
    "\n",
    "# Drop NaN rows\n",
    "vol_models = vol_models.dropna()\n",
    "\n",
    "print(f\"Volatility models calculated: {vol_models.shape[1]} features\")\n",
    "print(f\"Valid observations: {len(vol_models)}\")\n",
    "print(f\"\\nVolatility Model Columns:\")\n",
    "for col in vol_models.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY MODEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate correlation and RMSE for each model vs true volatility\n",
    "true_vol = realized_vol['rv_true'].dropna()\n",
    "common_idx = true_vol.index.intersection(vol_models.index)\n",
    "\n",
    "model_metrics = []\n",
    "\n",
    "for col in vol_models.columns:\n",
    "    model_vol = vol_models[col].loc[common_idx]\n",
    "    true_vol_aligned = true_vol.loc[common_idx]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correlation = model_vol.corr(true_vol_aligned)\n",
    "    rmse = np.sqrt(((model_vol - true_vol_aligned) ** 2).mean())\n",
    "    mae = np.abs(model_vol - true_vol_aligned).mean()\n",
    "    \n",
    "    model_metrics.append({\n",
    "        'Model': col,\n",
    "        'Correlation': correlation,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(model_metrics).sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"Model Performance vs True Volatility (20d realized):\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualization: Model comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=FIG_SIZE_COMPARISON)\n",
    "\n",
    "# Top 5 models by correlation\n",
    "top_models = metrics_df.head(5)['Model'].tolist()\n",
    "\n",
    "# Plot 1: Time series comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol.loc[common_idx], label='True Volatility (20d)', color='black', linewidth=2)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(top_models)))\n",
    "for i, model in enumerate(top_models):\n",
    "    ax1.plot(vol_models[model].loc[common_idx], label=model, alpha=0.7, color=colors[i])\n",
    "ax1.set_title(f'{TICKER} - Volatility Models Comparison', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Correlation bar chart\n",
    "ax2 = axes[1]\n",
    "colors = ['#2ecc71' if x > 0.9 else '#f39c12' if x > 0.8 else '#e74c3c' \n",
    "          for x in metrics_df['Correlation'].head(10)]\n",
    "ax2.barh(metrics_df['Model'].head(10)[::-1], metrics_df['Correlation'].head(10)[::-1], color=colors)\n",
    "ax2.set_xlabel('Correlation with True Volatility')\n",
    "ax2.set_title('Top 10 Models by Correlation', fontsize=14)\n",
    "ax2.axvline(x=0.9, color='green', linestyle='--', label='High correlation (0.9)')\n",
    "ax2.axvline(x=0.8, color='orange', linestyle='--', label='Good correlation (0.8)')\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PCA-Based Aggregate Volatility Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA-BASED AGGREGATE VOLATILITY MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def run_pca_model(features: pd.DataFrame, n_components: int = 3, \n",
    "                  lag_periods: list = None) -> Tuple[pd.DataFrame, PCA, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Create PCA-based aggregate volatility forecast using lagged features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : pd.DataFrame\n",
    "        Volatility features\n",
    "    n_components : int\n",
    "        Number of PCA components\n",
    "    lag_periods : list\n",
    "        Lag periods to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, PCA, StandardScaler]\n",
    "        Lagged features with PCA components, fitted PCA model, fitted scaler\n",
    "    \"\"\"\n",
    "    if lag_periods is None:\n",
    "        lag_periods = [1, 2, 3, 5, 10, 20]\n",
    "    \n",
    "    # Create lagged features\n",
    "    lagged_features = features.copy()\n",
    "    \n",
    "    for col in features.columns:\n",
    "        for lag in lag_periods:\n",
    "            lagged_features[f'{col}_lag{lag}'] = features[col].shift(lag)\n",
    "    \n",
    "    # Drop NaN rows\n",
    "    lagged_features = lagged_features.dropna()\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(lagged_features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    result = pd.DataFrame(\n",
    "        pca_components,\n",
    "        index=lagged_features.index,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Add explained variance info\n",
    "    print(f\"PCA Explained Variance Ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.1f}%)\")\n",
    "    print(f\"  Total: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.1f}%)\")\n",
    "    \n",
    "    return result, pca, scaler\n",
    "\n",
    "\n",
    "# Run PCA on volatility features\n",
    "pca_result, pca_model, vol_scaler = run_pca_model(\n",
    "    features=vol_models,\n",
    "    n_components=PCA_N_COMPONENTS,\n",
    "    lag_periods=PCA_LAG_PERIODS\n",
    ")\n",
    "\n",
    "# Create aggregate volatility forecast (weighted combination of PCs)\n",
    "# Weight by explained variance ratio\n",
    "weights = pca_model.explained_variance_ratio_\n",
    "pca_result['aggregate_vol_signal'] = sum(\n",
    "    pca_result[f'PC{i+1}'] * weights[i] for i in range(PCA_N_COMPONENTS)\n",
    ")\n",
    "\n",
    "# Normalize to same scale as true volatility\n",
    "true_vol_aligned = realized_vol['rv_true'].loc[pca_result.index]\n",
    "signal_mean = pca_result['aggregate_vol_signal'].mean()\n",
    "signal_std = pca_result['aggregate_vol_signal'].std()\n",
    "true_mean = true_vol_aligned.mean()\n",
    "true_std = true_vol_aligned.std()\n",
    "\n",
    "pca_result['aggregate_vol_forecast'] = (\n",
    "    (pca_result['aggregate_vol_signal'] - signal_mean) / signal_std * true_std + true_mean\n",
    ")\n",
    "\n",
    "print(f\"\\nAggregate Model Statistics:\")\n",
    "print(f\"  Correlation with True Vol: {pca_result['aggregate_vol_forecast'].corr(true_vol_aligned):.4f}\")\n",
    "rmse = np.sqrt(((pca_result['aggregate_vol_forecast'] - true_vol_aligned) ** 2).mean())\n",
    "print(f\"  RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Aggregate Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE MODEL VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Aggregate forecast vs True volatility\n",
    "ax1 = axes[0]\n",
    "ax1.plot(true_vol_aligned, label='True Volatility (20d)', color='black', linewidth=2)\n",
    "ax1.plot(pca_result['aggregate_vol_forecast'], label='PCA Aggregate Forecast', \n",
    "         color='#3498db', linewidth=1.5, alpha=0.8)\n",
    "ax1.fill_between(pca_result.index, \n",
    "                 pca_result['aggregate_vol_forecast'] - rmse,\n",
    "                 pca_result['aggregate_vol_forecast'] + rmse,\n",
    "                 alpha=0.2, color='#3498db', label='\u00b1RMSE band')\n",
    "ax1.set_title(f'{TICKER} - PCA Aggregate Volatility Model vs True Volatility', fontsize=14)\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = pca_result['aggregate_vol_forecast'].corr(true_vol_aligned)\n",
    "ax1.annotate(f'Correlation: {corr:.4f}\\nRMSE: {rmse:.4f}', \n",
    "             xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "             verticalalignment='top', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot 2: Principal Components\n",
    "ax2 = axes[1]\n",
    "for i in range(PCA_N_COMPONENTS):\n",
    "    ax2.plot(pca_result[f'PC{i+1}'], label=f'PC{i+1} ({pca_model.explained_variance_ratio_[i]*100:.1f}%)',\n",
    "             alpha=0.8)\n",
    "ax2.set_title('Principal Components Over Time', fontsize=14)\n",
    "ax2.set_ylabel('Standardized Value')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Scatter plot - Forecast vs Actual\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(true_vol_aligned, pca_result['aggregate_vol_forecast'], alpha=0.5, s=10)\n",
    "min_val = min(true_vol_aligned.min(), pca_result['aggregate_vol_forecast'].min())\n",
    "max_val = max(true_vol_aligned.max(), pca_result['aggregate_vol_forecast'].max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "ax3.set_xlabel('True Volatility')\n",
    "ax3.set_ylabel('Forecast Volatility')\n",
    "ax3.set_title('Forecast vs Actual Volatility', fontsize=14)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Explanation & Maintenance\n",
    "\n",
    "### 3.1 Understanding PCA (Principal Component Analysis)\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "Principal Component Analysis is a dimensionality reduction technique that identifies patterns of correlation across multiple variables. In our volatility modeling context:\n",
    "\n",
    "1. **Multiple Volatility Measures**: We calculate many volatility estimates (historical, EWMA, Parkinson, Garman-Klass, etc.) at different time windows. These measures are often highly correlated but capture slightly different aspects of volatility.\n",
    "\n",
    "2. **Common Variance Patterns**: PCA identifies the common underlying \"factors\" that drive these volatility measures. The first principal component (PC1) captures the most common variance pattern - essentially what all volatility measures agree on.\n",
    "\n",
    "3. **Noise Reduction**: By focusing on the top principal components (which explain most variance), we filter out measurement noise and idiosyncratic differences between methods.\n",
    "\n",
    "4. **Composite Signal**: The weighted combination of principal components creates a more robust volatility forecast than any single measure.\n",
    "\n",
    "**Interpretation of Principal Components:**\n",
    "- **PC1** (largest variance): Usually represents the \"level\" of volatility - whether vol is generally high or low\n",
    "- **PC2**: Often captures the \"slope\" - whether vol is rising or falling\n",
    "- **PC3**: May capture \"curvature\" or regime-specific patterns\n",
    "\n",
    "### 3.2 Why Lag Features Improve Volatility Forecasting\n",
    "\n",
    "**Volatility Clustering**: One of the most robust findings in financial markets is that volatility exhibits strong autocorrelation - high volatility tends to be followed by high volatility, and low by low. This is captured by including lagged features.\n",
    "\n",
    "**Optimal Lag Periods:**\n",
    "- **Lag 1-3 days**: Captures immediate momentum and short-term persistence\n",
    "- **Lag 5-10 days**: Captures weekly patterns and medium-term trends\n",
    "- **Lag 20 days**: Captures monthly cycles and mean-reversion signals\n",
    "\n",
    "**Why This Works:**\n",
    "- Volatility at time t is highly correlated with volatility at t-1, t-2, etc.\n",
    "- Changes in the relationship between current and lagged volatility signal regime changes\n",
    "- The PCA captures how these lag relationships evolve over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Maintenance Strategy\n",
    "\n",
    "#### Rolling Window Retraining Schedule\n",
    "\n",
    "The PCA model should be periodically retrained to adapt to changing market conditions:\n",
    "\n",
    "| Frequency | Action | Rationale |\n",
    "|-----------|--------|----------|\n",
    "| Weekly | Monitor explained variance | Early warning of model drift |\n",
    "| Monthly | Validate on recent data | Check out-of-sample performance |\n",
    "| Quarterly | Full retraining | Adapt to structural changes |\n",
    "| After major events | Emergency recalibration | Respond to regime shifts |\n",
    "\n",
    "#### Out-of-Sample Validation Approach\n",
    "\n",
    "```python\n",
    "# Recommended validation structure:\n",
    "# - Training: 70% of historical data\n",
    "# - Validation: 15% for hyperparameter tuning\n",
    "# - Test: 15% for final evaluation\n",
    "# - Rolling: Move window forward and repeat\n",
    "```\n",
    "\n",
    "#### Drift Detection Methods\n",
    "\n",
    "1. **Explained Variance Monitoring**: If total explained variance drops significantly, component structure may be changing\n",
    "2. **Correlation Decay**: Track rolling correlation between forecast and realized volatility\n",
    "3. **RMSE Tracking**: Monitor prediction error over rolling windows\n",
    "4. **Distribution Shift**: Compare feature distributions using KS tests\n",
    "\n",
    "#### Recalibration Triggers\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| 30-day rolling correlation | < 0.7 | Alert & investigate |\n",
    "| 30-day rolling RMSE | > 1.5\u00d7 baseline | Trigger retraining |\n",
    "| Explained variance (PC1-3) | < 70% | Review feature set |\n",
    "| Consecutive days of underperformance | > 10 | Emergency review |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: MODEL MAINTENANCE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_model_drift_metrics(true_vol: pd.Series, forecast_vol: pd.Series,\n",
    "                                  window: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rolling metrics to detect model drift.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_vol : pd.Series\n",
    "        Realized volatility\n",
    "    forecast_vol : pd.Series\n",
    "        Model forecast\n",
    "    window : int\n",
    "        Rolling window for metrics\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Drift metrics over time\n",
    "    \"\"\"\n",
    "    # Align series\n",
    "    common_idx = true_vol.index.intersection(forecast_vol.index)\n",
    "    true_vol = true_vol.loc[common_idx]\n",
    "    forecast_vol = forecast_vol.loc[common_idx]\n",
    "    \n",
    "    drift_metrics = pd.DataFrame(index=common_idx)\n",
    "    \n",
    "    # Rolling correlation\n",
    "    drift_metrics['rolling_corr'] = true_vol.rolling(window).corr(forecast_vol)\n",
    "    \n",
    "    # Rolling RMSE\n",
    "    errors_sq = (true_vol - forecast_vol) ** 2\n",
    "    drift_metrics['rolling_rmse'] = np.sqrt(errors_sq.rolling(window).mean())\n",
    "    \n",
    "    # Rolling MAE\n",
    "    errors_abs = np.abs(true_vol - forecast_vol)\n",
    "    drift_metrics['rolling_mae'] = errors_abs.rolling(window).mean()\n",
    "    \n",
    "    # Directional accuracy (did we predict direction of change correctly?)\n",
    "    true_direction = np.sign(true_vol.diff())\n",
    "    forecast_direction = np.sign(forecast_vol.diff())\n",
    "    correct_direction = (true_direction == forecast_direction).astype(int)\n",
    "    drift_metrics['directional_accuracy'] = correct_direction.rolling(window).mean()\n",
    "    \n",
    "    return drift_metrics.dropna()\n",
    "\n",
    "\n",
    "def check_recalibration_needed(drift_metrics: pd.DataFrame, \n",
    "                               corr_threshold: float = 0.7,\n",
    "                               rmse_multiplier: float = 1.5) -> dict:\n",
    "    \"\"\"\n",
    "    Check if model needs recalibration based on drift metrics.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Recalibration status and recommendations\n",
    "    \"\"\"\n",
    "    latest = drift_metrics.iloc[-1]\n",
    "    baseline_rmse = drift_metrics['rolling_rmse'].median()\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    if latest['rolling_corr'] < corr_threshold:\n",
    "        alerts.append(f\"Low correlation: {latest['rolling_corr']:.3f} < {corr_threshold}\")\n",
    "    \n",
    "    if latest['rolling_rmse'] > baseline_rmse * rmse_multiplier:\n",
    "        alerts.append(f\"High RMSE: {latest['rolling_rmse']:.4f} > {baseline_rmse * rmse_multiplier:.4f}\")\n",
    "    \n",
    "    if latest['directional_accuracy'] < 0.5:\n",
    "        alerts.append(f\"Poor directional accuracy: {latest['directional_accuracy']:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'needs_recalibration': len(alerts) > 0,\n",
    "        'alerts': alerts,\n",
    "        'latest_metrics': latest.to_dict()\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate drift metrics for our model\n",
    "drift_metrics = calculate_model_drift_metrics(\n",
    "    true_vol_aligned,\n",
    "    pca_result['aggregate_vol_forecast'],\n",
    "    window=30\n",
    ")\n",
    "\n",
    "# Check recalibration status\n",
    "recal_status = check_recalibration_needed(drift_metrics)\n",
    "\n",
    "print(\"Model Health Check:\")\n",
    "print(f\"  Needs Recalibration: {recal_status['needs_recalibration']}\")\n",
    "if recal_status['alerts']:\n",
    "    print(\"  Alerts:\")\n",
    "    for alert in recal_status['alerts']:\n",
    "        print(f\"    - {alert}\")\n",
    "print(f\"\\nLatest Metrics:\")\n",
    "for metric, value in recal_status['latest_metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Mean Reversion Analysis\n",
    "\n",
    "This section analyzes volatility mean reversion characteristics to inform optimal options expiration selection.\n",
    "\n",
    "### 4.1 Volatility Z-Score and Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: MEAN REVERSION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_volatility_zscore(vol_series: pd.Series, lookback: int = 252) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate rolling z-score of volatility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vol_series : pd.Series\n",
    "        Volatility series\n",
    "    lookback : int\n",
    "        Lookback window for mean and std calculation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Z-score series\n",
    "    \"\"\"\n",
    "    rolling_mean = vol_series.rolling(window=lookback).mean()\n",
    "    rolling_std = vol_series.rolling(window=lookback).std()\n",
    "    zscore = (vol_series - rolling_mean) / rolling_std\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def classify_volatility_regime(zscore: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Classify volatility into regimes based on z-score.\n",
    "    \n",
    "    Regimes:\n",
    "    - Low: z < -1\n",
    "    - Normal: -1 <= z <= 1\n",
    "    - Elevated: 1 < z <= 2\n",
    "    - Extreme: z > 2\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        zscore < -1,\n",
    "        (zscore >= -1) & (zscore <= 1),\n",
    "        (zscore > 1) & (zscore <= 2),\n",
    "        zscore > 2\n",
    "    ]\n",
    "    choices = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    return pd.Series(np.select(conditions, choices, default='Normal'), index=zscore.index)\n",
    "\n",
    "\n",
    "# Calculate z-scores\n",
    "vol_zscore = calculate_volatility_zscore(realized_vol['rv_true'].dropna())\n",
    "vol_regime = classify_volatility_regime(vol_zscore)\n",
    "\n",
    "# Combine into analysis DataFrame\n",
    "vol_analysis = pd.DataFrame({\n",
    "    'volatility': realized_vol['rv_true'],\n",
    "    'zscore': vol_zscore,\n",
    "    'regime': vol_regime\n",
    "}).dropna()\n",
    "\n",
    "# Regime statistics\n",
    "regime_stats = vol_analysis.groupby('regime').agg({\n",
    "    'volatility': ['mean', 'std', 'count'],\n",
    "    'zscore': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"Volatility Regime Statistics:\")\n",
    "print(regime_stats)\n",
    "print(f\"\\nRegime Distribution:\")\n",
    "regime_counts = vol_analysis['regime'].value_counts()\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = count / len(vol_analysis) * 100\n",
    "    print(f\"  {regime}: {count} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mean Reversion Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEAN REVERSION SPEED ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_mean_reversion_speed(zscore: pd.Series, regime_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate mean reversion characteristics for each volatility regime.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Mean reversion metrics by regime\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for regime in ['Low', 'Normal', 'Elevated', 'Extreme']:\n",
    "        # Find regime entry points\n",
    "        regime_mask = regime_series == regime\n",
    "        regime_start = regime_mask & ~regime_mask.shift(1, fill_value=False)\n",
    "        \n",
    "        # For each entry, measure time to revert\n",
    "        times_to_05 = []  # Time to reach \u00b10.5\u03c3\n",
    "        times_to_10 = []  # Time to reach \u00b11\u03c3\n",
    "        \n",
    "        entry_indices = zscore.index[regime_start]\n",
    "        \n",
    "        for entry_idx in entry_indices:\n",
    "            entry_zscore = zscore.loc[entry_idx]\n",
    "            future_zscore = zscore.loc[entry_idx:]\n",
    "            \n",
    "            # Time to \u00b10.5\u03c3\n",
    "            mask_05 = np.abs(future_zscore) <= 0.5\n",
    "            if mask_05.any():\n",
    "                first_05 = mask_05.idxmax()\n",
    "                days_to_05 = (first_05 - entry_idx).days\n",
    "                if days_to_05 > 0:\n",
    "                    times_to_05.append(days_to_05)\n",
    "            \n",
    "            # Time to \u00b11\u03c3 (only relevant for Elevated/Extreme)\n",
    "            if regime in ['Elevated', 'Extreme']:\n",
    "                mask_10 = np.abs(future_zscore) <= 1.0\n",
    "                if mask_10.any():\n",
    "                    first_10 = mask_10.idxmax()\n",
    "                    days_to_10 = (first_10 - entry_idx).days\n",
    "                    if days_to_10 > 0:\n",
    "                        times_to_10.append(days_to_10)\n",
    "        \n",
    "        # Calculate half-life (approximate from exponential decay)\n",
    "        if len(times_to_05) > 0:\n",
    "            avg_time_05 = np.mean(times_to_05)\n",
    "            median_time_05 = np.median(times_to_05)\n",
    "        else:\n",
    "            avg_time_05 = np.nan\n",
    "            median_time_05 = np.nan\n",
    "            \n",
    "        if len(times_to_10) > 0:\n",
    "            avg_time_10 = np.mean(times_to_10)\n",
    "            median_time_10 = np.median(times_to_10)\n",
    "        else:\n",
    "            avg_time_10 = np.nan\n",
    "            median_time_10 = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Regime': regime,\n",
    "            'Entry_Count': len(entry_indices),\n",
    "            'Avg_Days_to_0.5\u03c3': avg_time_05,\n",
    "            'Median_Days_to_0.5\u03c3': median_time_05,\n",
    "            'Avg_Days_to_1\u03c3': avg_time_10,\n",
    "            'Median_Days_to_1\u03c3': median_time_10,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Calculate mean reversion metrics\n",
    "reversion_metrics = calculate_mean_reversion_speed(vol_zscore.dropna(), vol_regime.dropna())\n",
    "\n",
    "print(\"Mean Reversion Speed by Volatility Regime:\")\n",
    "print(reversion_metrics.to_string(index=False))\n",
    "\n",
    "# Recommended DTE by regime\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED OPTIONS EXPIRATION BY VOLATILITY REGIME:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dte_recommendations = {\n",
    "    'Extreme': '7-14 DTE (quick mean reversion expected)',\n",
    "    'Elevated': '14-21 DTE (moderate reversion timeframe)',\n",
    "    'Normal': '30-45 DTE (standard theta decay window)',\n",
    "    'Low': '45-60 DTE (wait for volatility expansion)'\n",
    "}\n",
    "\n",
    "for regime, rec in dte_recommendations.items():\n",
    "    print(f\"  {regime}: {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Volatility Regime Transition Matrix (PRIORITY VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VOLATILITY REGIME TRANSITION MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_transitions(regime_series: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate regime transition probability matrix.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        Transition probability matrix and transition count matrix\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    \n",
    "    # Create transition count matrix\n",
    "    transition_counts = pd.DataFrame(0, index=regimes, columns=regimes)\n",
    "    \n",
    "    current_regime = regime_series.iloc[:-1].values\n",
    "    next_regime = regime_series.iloc[1:].values\n",
    "    \n",
    "    for curr, nxt in zip(current_regime, next_regime):\n",
    "        if curr in regimes and nxt in regimes:\n",
    "            transition_counts.loc[curr, nxt] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    row_sums = transition_counts.sum(axis=1)\n",
    "    transition_probs = transition_counts.div(row_sums, axis=0)\n",
    "    \n",
    "    return transition_probs, transition_counts\n",
    "\n",
    "\n",
    "def calculate_avg_regime_duration(regime_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate average duration spent in each regime.\n",
    "    \"\"\"\n",
    "    regimes = ['Low', 'Normal', 'Elevated', 'Extreme']\n",
    "    durations = {regime: [] for regime in regimes}\n",
    "    \n",
    "    current_regime = None\n",
    "    current_duration = 0\n",
    "    \n",
    "    for regime in regime_series.values:\n",
    "        if regime == current_regime:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            if current_regime in durations:\n",
    "                durations[current_regime].append(current_duration)\n",
    "            current_regime = regime\n",
    "            current_duration = 1\n",
    "    \n",
    "    # Add final regime\n",
    "    if current_regime in durations:\n",
    "        durations[current_regime].append(current_duration)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    duration_stats = []\n",
    "    for regime in regimes:\n",
    "        if durations[regime]:\n",
    "            duration_stats.append({\n",
    "                'Regime': regime,\n",
    "                'Avg_Duration_Days': np.mean(durations[regime]),\n",
    "                'Median_Duration_Days': np.median(durations[regime]),\n",
    "                'Max_Duration_Days': np.max(durations[regime]),\n",
    "                'Num_Episodes': len(durations[regime])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(duration_stats)\n",
    "\n",
    "\n",
    "# Calculate transitions\n",
    "transition_probs, transition_counts = calculate_regime_transitions(vol_regime.dropna())\n",
    "duration_stats = calculate_avg_regime_duration(vol_regime.dropna())\n",
    "\n",
    "print(\"Regime Transition Probabilities:\")\n",
    "print(transition_probs.round(3))\n",
    "print(\"\\nAverage Regime Duration:\")\n",
    "print(duration_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION: Regime Transition Heatmap\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Transition probability heatmap\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(transition_probs, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "            ax=ax1, vmin=0, vmax=1, linewidths=0.5,\n",
    "            cbar_kws={'label': 'Transition Probability'})\n",
    "ax1.set_title('Volatility Regime Transition Probabilities\\n(Row = Current Regime, Column = Next Regime)', \n",
    "              fontsize=14)\n",
    "ax1.set_xlabel('Next Regime')\n",
    "ax1.set_ylabel('Current Regime')\n",
    "\n",
    "# Add average transition times as annotations\n",
    "for i, regime_from in enumerate(transition_probs.index):\n",
    "    duration = duration_stats[duration_stats['Regime'] == regime_from]['Avg_Duration_Days'].values\n",
    "    if len(duration) > 0:\n",
    "        ax1.annotate(f'Avg: {duration[0]:.1f}d', \n",
    "                    xy=(-0.3, i + 0.5), xycoords='data',\n",
    "                    fontsize=9, ha='right', va='center')\n",
    "\n",
    "# Bar chart of regime durations\n",
    "ax2 = axes[1]\n",
    "colors = [get_vol_regime_color(r) for r in duration_stats['Regime']]\n",
    "bars = ax2.bar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'], color=colors, alpha=0.8)\n",
    "ax2.errorbar(duration_stats['Regime'], duration_stats['Avg_Duration_Days'],\n",
    "             yerr=[duration_stats['Avg_Duration_Days'] - duration_stats['Median_Duration_Days'],\n",
    "                   duration_stats['Max_Duration_Days'] - duration_stats['Avg_Duration_Days']],\n",
    "             fmt='none', color='black', capsize=5)\n",
    "ax2.set_title('Average Regime Duration (with min/max range)', fontsize=14)\n",
    "ax2.set_xlabel('Volatility Regime')\n",
    "ax2.set_ylabel('Days')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count annotations\n",
    "for bar, count in zip(bars, duration_stats['Num_Episodes']):\n",
    "    ax2.annotate(f'n={count}', \n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"- High diagonal values indicate regime persistence\")\n",
    "print(\"- Off-diagonal values show transition likelihoods\")\n",
    "print(\"- Extreme volatility typically reverts to Elevated/Normal\")\n",
    "print(\"- Use this to inform position sizing and DTE selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Options Strategy Development & Delta Analysis\n",
    "\n",
    "This section implements six options strategies with systematic delta testing and performance analysis.\n",
    "\n",
    "### Strategy Suite:\n",
    "1. **Calendar Spread** - Long back month, short front month, same strike\n",
    "2. **Double Diagonal** - Calendar spread on both calls and puts, different strikes\n",
    "3. **Long Straddle** - Long ATM call + long ATM put\n",
    "4. **Long Strangle** - Long OTM call + long OTM put\n",
    "5. **Bull Put Spread** - Short put + long lower strike put\n",
    "6. **Bull Call Spread** - Long call + short higher strike call\n",
    "\n",
    "### 5.1 Strategy Implementation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: OPTIONS STRATEGY DEVELOPMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Strategy configurations\n",
    "STRATEGIES = [\n",
    "    'Calendar Spread',\n",
    "    'Double Diagonal',\n",
    "    'Straddle',\n",
    "    'Strangle',\n",
    "    'Bull Put Spread',\n",
    "    'Bull Call Spread'\n",
    "]\n",
    "\n",
    "def calculate_strategy_pnl(strategy_type: str, entry_price: float, exit_price: float,\n",
    "                           contracts: int = 1, multiplier: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Calculate P&L for a strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy_type : str\n",
    "        Type of options strategy\n",
    "    entry_price : float\n",
    "        Net debit/credit at entry\n",
    "    exit_price : float\n",
    "        Net value at exit\n",
    "    contracts : int\n",
    "        Number of contracts\n",
    "    multiplier : int\n",
    "        Contract multiplier (usually 100)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Profit/Loss in dollars\n",
    "    \"\"\"\n",
    "    return (exit_price - entry_price) * contracts * multiplier\n",
    "\n",
    "\n",
    "def apply_stop_loss(pnl_series: pd.Series, stop_threshold: float = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply stop loss to a P&L series.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pnl_series : pd.Series\n",
    "        Series of P&L values (as returns or dollar amounts)\n",
    "    stop_threshold : float\n",
    "        Stop loss threshold (e.g., -0.20 for 20% loss). None = no stop.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Adjusted P&L series with stop loss applied\n",
    "    \"\"\"\n",
    "    if stop_threshold is None:\n",
    "        return pnl_series\n",
    "    \n",
    "    # Apply stop loss - cap losses at threshold\n",
    "    adjusted = pnl_series.copy()\n",
    "    adjusted = np.where(adjusted < stop_threshold, stop_threshold, adjusted)\n",
    "    return pd.Series(adjusted, index=pnl_series.index)\n",
    "\n",
    "\n",
    "def roll_position(current_expiry: datetime, days_to_roll: int = 5) -> datetime:\n",
    "    \"\"\"\n",
    "    Calculate next expiration date for position rolling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_expiry : datetime\n",
    "        Current position expiration\n",
    "    days_to_roll : int\n",
    "        Days before expiry to trigger roll\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    datetime\n",
    "        Next expiration date (approximately 30 days out)\n",
    "    \"\"\"\n",
    "    # Roll to next monthly expiration\n",
    "    next_expiry = current_expiry + timedelta(days=30)\n",
    "    # Adjust to third Friday (standard monthly expiration)\n",
    "    # Find first day of expiry month\n",
    "    first_day = next_expiry.replace(day=1)\n",
    "    # Find first Friday\n",
    "    days_until_friday = (4 - first_day.weekday()) % 7\n",
    "    first_friday = first_day + timedelta(days=days_until_friday)\n",
    "    # Third Friday\n",
    "    third_friday = first_friday + timedelta(days=14)\n",
    "    return third_friday\n",
    "\n",
    "\n",
    "print(\"Strategy functions defined.\")\n",
    "print(f\"Strategies: {STRATEGIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Simulated Strategy Returns\n",
    "\n",
    "Since actual options data may not be available for all historical dates, we simulate strategy returns based on:\n",
    "- Underlying price movements\n",
    "- Volatility regime\n",
    "- Strategy-specific sensitivities (delta, vega, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SIMULATED STRATEGY RETURNS BASED ON UNDERLYING DYNAMICS\n",
    "# ============================================================================\n",
    "\n",
    "def simulate_strategy_returns(underlying_returns: pd.Series, volatility: pd.Series,\n",
    "                              vol_regime: pd.Series, strategy: str,\n",
    "                              delta_config: dict = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Simulate strategy returns based on underlying dynamics.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    This is a simplified model - actual returns would use full options pricing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    underlying_returns : pd.Series\n",
    "        Daily returns of underlying\n",
    "    volatility : pd.Series\n",
    "        Realized volatility\n",
    "    vol_regime : pd.Series\n",
    "        Volatility regime classification\n",
    "    strategy : str\n",
    "        Strategy type\n",
    "    delta_config : dict\n",
    "        Delta configuration for the strategy\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Simulated strategy returns\n",
    "    \"\"\"\n",
    "    # Align all series\n",
    "    common_idx = underlying_returns.dropna().index\n",
    "    common_idx = common_idx.intersection(volatility.dropna().index)\n",
    "    \n",
    "    ret = underlying_returns.loc[common_idx]\n",
    "    vol = volatility.loc[common_idx]\n",
    "    \n",
    "    # Normalized volatility for scaling\n",
    "    vol_normalized = (vol - vol.mean()) / vol.std()\n",
    "    \n",
    "    # Strategy-specific return simulation\n",
    "    if strategy == 'Calendar Spread':\n",
    "        # Calendar spreads profit from stable prices and vol crush\n",
    "        # Positive vega exposure, minimal delta\n",
    "        theta_component = 0.001  # Daily theta decay benefit\n",
    "        vega_component = -vol_normalized.diff() * 0.02  # Profit from vol decrease\n",
    "        gamma_component = -np.abs(ret) * 0.5  # Loss from large moves\n",
    "        strat_returns = theta_component + vega_component + gamma_component\n",
    "        \n",
    "    elif strategy == 'Double Diagonal':\n",
    "        # Similar to calendar but wider profit zone\n",
    "        theta_component = 0.0008\n",
    "        vega_component = -vol_normalized.diff() * 0.015\n",
    "        gamma_component = -np.abs(ret) * 0.3\n",
    "        strat_returns = theta_component + vega_component + gamma_component\n",
    "        \n",
    "    elif strategy == 'Straddle':\n",
    "        # Long straddle profits from large moves, loses theta\n",
    "        theta_component = -0.002  # Daily theta decay cost\n",
    "        gamma_component = np.abs(ret) * 2.0  # Profit from large moves\n",
    "        vega_component = vol_normalized.diff() * 0.03  # Profit from vol increase\n",
    "        strat_returns = theta_component + gamma_component + vega_component\n",
    "        \n",
    "    elif strategy == 'Strangle':\n",
    "        # Similar to straddle but needs larger moves\n",
    "        theta_component = -0.0015\n",
    "        gamma_component = np.where(np.abs(ret) > 0.02, np.abs(ret) * 1.5, -0.001)\n",
    "        vega_component = vol_normalized.diff() * 0.025\n",
    "        strat_returns = theta_component + gamma_component + vega_component\n",
    "        \n",
    "    elif strategy == 'Bull Put Spread':\n",
    "        # Credit spread - profits from stable/rising prices\n",
    "        # Short delta exposure\n",
    "        delta = delta_config.get('put_delta', -0.20) if delta_config else -0.20\n",
    "        delta_component = ret * np.abs(delta) * 0.5\n",
    "        theta_component = 0.0015\n",
    "        gamma_component = np.where(ret < -0.02, ret * 2, 0)  # Loss from large drops\n",
    "        strat_returns = delta_component + theta_component + gamma_component\n",
    "        \n",
    "    elif strategy == 'Bull Call Spread':\n",
    "        # Debit spread - profits from rising prices\n",
    "        delta = delta_config.get('call_delta', 0.40) if delta_config else 0.40\n",
    "        delta_component = ret * delta * 0.8\n",
    "        theta_component = -0.0008\n",
    "        strat_returns = delta_component + theta_component\n",
    "        \n",
    "    else:\n",
    "        strat_returns = pd.Series(0, index=common_idx)\n",
    "    \n",
    "    # Add noise for realism\n",
    "    noise = np.random.normal(0, 0.002, len(strat_returns))\n",
    "    strat_returns = strat_returns + noise\n",
    "    \n",
    "    return pd.Series(strat_returns, index=common_idx)\n",
    "\n",
    "\n",
    "# Generate returns for all strategies\n",
    "strategy_returns = {}\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    returns = simulate_strategy_returns(\n",
    "        underlying_returns=df_asset['returns'],\n",
    "        volatility=realized_vol['rv_true'],\n",
    "        vol_regime=vol_regime,\n",
    "        strategy=strategy\n",
    "    )\n",
    "    strategy_returns[strategy] = returns\n",
    "    \n",
    "# Create DataFrame\n",
    "strategy_returns_df = pd.DataFrame(strategy_returns)\n",
    "\n",
    "print(f\"Strategy returns generated: {len(strategy_returns_df)} days\")\n",
    "print(\"\\nStrategy Return Statistics:\")\n",
    "print(strategy_returns_df.describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Delta Configuration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DELTA CONFIGURATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def backtest_strategy(strategy_type: str, delta_config: dict, \n",
    "                      underlying_returns: pd.Series, volatility: pd.Series,\n",
    "                      vol_regime: pd.Series, stop_loss: float = None) -> dict:\n",
    "    \"\"\"\n",
    "    Backtest a strategy with specific delta configuration.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Performance metrics\n",
    "    \"\"\"\n",
    "    # Generate returns\n",
    "    returns = simulate_strategy_returns(\n",
    "        underlying_returns=underlying_returns,\n",
    "        volatility=volatility,\n",
    "        vol_regime=vol_regime,\n",
    "        strategy=strategy_type,\n",
    "        delta_config=delta_config\n",
    "    )\n",
    "    \n",
    "    # Apply stop loss\n",
    "    returns = apply_stop_loss(returns, stop_loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_return = (1 + returns).prod() - 1\n",
    "    annual_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "    volatility_ann = returns.std() * np.sqrt(252)\n",
    "    sharpe = annual_return / volatility_ann if volatility_ann > 0 else 0\n",
    "    \n",
    "    # Drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Win rate\n",
    "    win_rate = (returns > 0).mean()\n",
    "    \n",
    "    return {\n",
    "        'strategy': strategy_type,\n",
    "        'delta_config': str(delta_config),\n",
    "        'stop_loss': stop_loss,\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'volatility': volatility_ann,\n",
    "        'sharpe': sharpe,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'win_rate': win_rate,\n",
    "        'n_days': len(returns)\n",
    "    }\n",
    "\n",
    "\n",
    "# Test different delta configurations for each strategy\n",
    "delta_test_results = []\n",
    "\n",
    "# Calendar Spread deltas\n",
    "for delta in CALENDAR_DELTAS:\n",
    "    config = {'call_delta': delta}\n",
    "    result = backtest_strategy('Calendar Spread', config, \n",
    "                               df_asset['returns'], realized_vol['rv_true'], vol_regime)\n",
    "    delta_test_results.append(result)\n",
    "\n",
    "# Strangle delta pairs\n",
    "for call_d, put_d in STRANGLE_DELTA_PAIRS:\n",
    "    config = {'call_delta': call_d, 'put_delta': put_d}\n",
    "    result = backtest_strategy('Strangle', config,\n",
    "                               df_asset['returns'], realized_vol['rv_true'], vol_regime)\n",
    "    delta_test_results.append(result)\n",
    "\n",
    "# Bull Put Spread deltas\n",
    "for put_d in BULL_PUT_SHORT_DELTAS:\n",
    "    config = {'put_delta': put_d}\n",
    "    result = backtest_strategy('Bull Put Spread', config,\n",
    "                               df_asset['returns'], realized_vol['rv_true'], vol_regime)\n",
    "    delta_test_results.append(result)\n",
    "\n",
    "# Bull Call Spread deltas\n",
    "for call_d in BULL_CALL_LONG_DELTAS:\n",
    "    config = {'call_delta': call_d}\n",
    "    result = backtest_strategy('Bull Call Spread', config,\n",
    "                               df_asset['returns'], realized_vol['rv_true'], vol_regime)\n",
    "    delta_test_results.append(result)\n",
    "\n",
    "delta_results_df = pd.DataFrame(delta_test_results)\n",
    "\n",
    "print(\"Delta Configuration Test Results:\")\n",
    "print(delta_results_df[['strategy', 'delta_config', 'sharpe', 'total_return', 'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 PRIORITY VISUALIZATION #1: Strategy Equity Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #1: Strategy Equity Curves\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate cumulative returns for each strategy\n",
    "cumulative_returns = (1 + strategy_returns_df).cumprod()\n",
    "\n",
    "# Calculate buy-and-hold benchmark\n",
    "benchmark_returns = df_asset['returns'].loc[strategy_returns_df.index]\n",
    "benchmark_cumulative = (1 + benchmark_returns.fillna(0)).cumprod()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot each strategy with consistent colors\n",
    "for strategy in STRATEGIES:\n",
    "    color = get_strategy_color(strategy)\n",
    "    ax.plot(cumulative_returns[strategy], label=strategy, color=color, linewidth=1.5)\n",
    "\n",
    "# Plot benchmark\n",
    "ax.plot(benchmark_cumulative, label='Buy & Hold', color='gray', \n",
    "        linestyle='--', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Add regime shading\n",
    "if len(vol_regime.dropna()) > 0:\n",
    "    regime_aligned = vol_regime.reindex(cumulative_returns.index, method='ffill')\n",
    "    \n",
    "    # Shade by regime\n",
    "    prev_regime = None\n",
    "    regime_start = cumulative_returns.index[0]\n",
    "    \n",
    "    for idx, regime in regime_aligned.items():\n",
    "        if regime != prev_regime and prev_regime is not None:\n",
    "            color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, idx, alpha=0.1, color=color)\n",
    "            regime_start = idx\n",
    "        prev_regime = regime\n",
    "    \n",
    "    # Final regime\n",
    "    if prev_regime:\n",
    "        color = get_vol_regime_color(prev_regime)\n",
    "        ax.axvspan(regime_start, cumulative_returns.index[-1], alpha=0.1, color=color)\n",
    "\n",
    "ax.set_title(f'{TICKER} - Strategy Equity Curves Comparison', fontsize=14)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return (1 = Starting Value)')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=1, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add regime legend\n",
    "regime_patches = [mpatches.Patch(color=get_vol_regime_color(r), alpha=0.3, label=r) \n",
    "                  for r in ['Low', 'Normal', 'Elevated', 'Extreme']]\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0] + regime_patches, \n",
    "          loc='upper left', fontsize=9, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final returns\n",
    "print(\"\\nFinal Cumulative Returns:\")\n",
    "for strategy in STRATEGIES:\n",
    "    final_return = cumulative_returns[strategy].iloc[-1] - 1\n",
    "    print(f\"  {strategy}: {final_return:.2%}\")\n",
    "print(f\"  Buy & Hold: {benchmark_cumulative.iloc[-1] - 1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio (Individual Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #2: Rolling Sharpe Ratio\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_rolling_sharpe(returns: pd.Series, window: int = 60) -> pd.Series:\n",
    "    \"\"\"Calculate rolling Sharpe ratio.\"\"\"\n",
    "    rolling_mean = returns.rolling(window=window).mean() * 252\n",
    "    rolling_std = returns.rolling(window=window).std() * np.sqrt(252)\n",
    "    return rolling_mean / rolling_std\n",
    "\n",
    "# Create separate plots for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Calculate rolling Sharpe\n",
    "    rolling_sharpe = calculate_rolling_sharpe(strategy_returns_df[strategy], ROLLING_SHARPE_WINDOW)\n",
    "    \n",
    "    # Plot smoothed line\n",
    "    smoothed = rolling_sharpe.rolling(10).mean()\n",
    "    ax.plot(smoothed, color=color, linewidth=2, label=f'{strategy} (60-day rolling)')\n",
    "    \n",
    "    # Add confidence bands (\u00b11 std)\n",
    "    rolling_std = rolling_sharpe.rolling(20).std()\n",
    "    ax.fill_between(smoothed.index, \n",
    "                    smoothed - rolling_std, \n",
    "                    smoothed + rolling_std,\n",
    "                    alpha=0.2, color=color)\n",
    "    \n",
    "    # Add regime background shading\n",
    "    regime_aligned = vol_regime.reindex(rolling_sharpe.index, method='ffill')\n",
    "    \n",
    "    prev_regime = None\n",
    "    regime_start = rolling_sharpe.dropna().index[0] if len(rolling_sharpe.dropna()) > 0 else None\n",
    "    \n",
    "    if regime_start is not None:\n",
    "        for idx in rolling_sharpe.dropna().index:\n",
    "            regime = regime_aligned.get(idx)\n",
    "            if regime != prev_regime and prev_regime is not None:\n",
    "                regime_color = get_vol_regime_color(prev_regime)\n",
    "                ax.axvspan(regime_start, idx, alpha=0.15, color=regime_color)\n",
    "                regime_start = idx\n",
    "            prev_regime = regime\n",
    "        \n",
    "        # Final segment\n",
    "        if prev_regime:\n",
    "            regime_color = get_vol_regime_color(prev_regime)\n",
    "            ax.axvspan(regime_start, rolling_sharpe.dropna().index[-1], alpha=0.15, color=regime_color)\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.axhline(y=1, color='green', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = 1')\n",
    "    ax.axhline(y=-1, color='red', linestyle='--', linewidth=0.5, alpha=0.5, label='Sharpe = -1')\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Rolling Sharpe Ratio ({ROLLING_SHARPE_WINDOW}-day window)', \n",
    "                 fontsize=14, color=color)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sharpe Ratio')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add overall Sharpe annotation\n",
    "    overall_sharpe = (strategy_returns_df[strategy].mean() * 252) / (strategy_returns_df[strategy].std() * np.sqrt(252))\n",
    "    ax.annotate(f'Overall Sharpe: {overall_sharpe:.2f}', \n",
    "                xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Stop Loss Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_strategy_parameters(strategy: str, underlying_returns: pd.Series,\n",
    "                                 volatility: pd.Series, vol_regime: pd.Series,\n",
    "                                 stop_loss_thresholds: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Test multiple stop loss thresholds for a strategy.\n",
    "    Vectorized implementation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Get base returns\n",
    "    base_returns = simulate_strategy_returns(\n",
    "        underlying_returns=underlying_returns,\n",
    "        volatility=volatility,\n",
    "        vol_regime=vol_regime,\n",
    "        strategy=strategy\n",
    "    )\n",
    "    \n",
    "    for stop in stop_loss_thresholds:\n",
    "        # Apply stop loss\n",
    "        adjusted_returns = apply_stop_loss(base_returns, stop)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_return = (1 + adjusted_returns).prod() - 1\n",
    "        annual_return = (1 + total_return) ** (252 / len(adjusted_returns)) - 1 if len(adjusted_returns) > 0 else 0\n",
    "        vol_ann = adjusted_returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / vol_ann if vol_ann > 0 else 0\n",
    "        \n",
    "        cumulative = (1 + adjusted_returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        win_rate = (adjusted_returns > 0).mean()\n",
    "        \n",
    "        # Count stopped trades\n",
    "        if stop is not None:\n",
    "            n_stopped = (base_returns < stop).sum()\n",
    "        else:\n",
    "            n_stopped = 0\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': strategy,\n",
    "            'stop_loss': f'{stop:.0%}' if stop else 'None',\n",
    "            'stop_value': stop if stop else 0,\n",
    "            'total_return': total_return,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'win_rate': win_rate,\n",
    "            'n_stopped': n_stopped\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run stop loss optimization for each strategy\n",
    "stop_loss_results = []\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    results = optimize_strategy_parameters(\n",
    "        strategy=strategy,\n",
    "        underlying_returns=df_asset['returns'],\n",
    "        volatility=realized_vol['rv_true'],\n",
    "        vol_regime=vol_regime,\n",
    "        stop_loss_thresholds=STOP_LOSS_THRESHOLDS\n",
    "    )\n",
    "    stop_loss_results.append(results)\n",
    "\n",
    "stop_loss_df = pd.concat(stop_loss_results, ignore_index=True)\n",
    "\n",
    "print(\"Stop Loss Optimization Results:\")\n",
    "print(stop_loss_df[['strategy', 'stop_loss', 'total_return', 'sharpe', 'max_drawdown', 'win_rate']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STOP LOSS VISUALIZATION - Separate plots per strategy\n",
    "# ============================================================================\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    strategy_data = stop_loss_df[stop_loss_df['strategy'] == strategy]\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Max Drawdown vs Total Return scatter\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(\n",
    "        -strategy_data['max_drawdown'] * 100,  # Convert to positive %\n",
    "        strategy_data['total_return'] * 100,\n",
    "        c=range(len(strategy_data)),\n",
    "        cmap='viridis',\n",
    "        s=100,\n",
    "        edgecolors='black'\n",
    "    )\n",
    "    \n",
    "    # Annotate points\n",
    "    for idx, row in strategy_data.iterrows():\n",
    "        ax1.annotate(row['stop_loss'],\n",
    "                    xy=(-row['max_drawdown']*100, row['total_return']*100),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel('Max Drawdown (%)')\n",
    "    ax1.set_ylabel('Total Return (%)')\n",
    "    ax1.set_title(f'{strategy} - Return vs Drawdown by Stop Loss', fontsize=12, color=color)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right: Bar chart of Sharpe by stop loss\n",
    "    ax2 = axes[1]\n",
    "    bars = ax2.bar(strategy_data['stop_loss'], strategy_data['sharpe'], color=color, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax2.set_xlabel('Stop Loss Threshold')\n",
    "    ax2.set_ylabel('Sharpe Ratio')\n",
    "    ax2.set_title(f'{strategy} - Sharpe Ratio by Stop Loss', fontsize=12, color=color)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify optimal stop loss\n",
    "    best_row = strategy_data.loc[strategy_data['sharpe'].idxmax()]\n",
    "    print(f\"{strategy} - Optimal Stop Loss: {best_row['stop_loss']} (Sharpe: {best_row['sharpe']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Market Regime Analysis Using Ensemble Clustering\n",
    "\n",
    "This section implements regime detection using an ensemble of:\n",
    "- **Gaussian Mixture Models (GMM)** - Probabilistic clustering\n",
    "- **K-Means Clustering** - Hard cluster assignments\n",
    "- **Agglomerative Clustering** - Hierarchical clustering\n",
    "- **Change-Point Detection (CPD)** - Structural break detection\n",
    "\n",
    "### Why Ensemble Approach?\n",
    "\n",
    "| Method | Strengths | Weaknesses |\n",
    "|--------|-----------|------------|\n",
    "| GMM | Probability distributions, overlapping clusters | Sensitive to initialization |\n",
    "| K-Means | Fast, simple, spherical clusters | Assumes equal cluster sizes |\n",
    "| Agglomerative | Hierarchical structure, no shape assumption | Computationally intensive |\n",
    "| CPD | Explicit transition detection | May miss gradual changes |\n",
    "\n",
    "The ensemble combines these methods to reduce individual weaknesses and provide confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Feature Engineering for Regime Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: ENSEMBLE REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def engineer_regime_features(data: pd.DataFrame, feature_set: str = 'comprehensive') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for regime detection.\n",
    "    Vectorized implementation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        OHLCV data with 'close', 'high', 'low', 'volume' columns\n",
    "    feature_set : str\n",
    "        'comprehensive', 'returns_only', 'volatility_only'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Feature DataFrame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Ensure we have returns\n",
    "    if 'returns' not in data.columns:\n",
    "        data = data.copy()\n",
    "        data['returns'] = data['close'].pct_change()\n",
    "    \n",
    "    # 1. Return-based features\n",
    "    features['returns_1d'] = data['returns']\n",
    "    features['returns_5d'] = data['close'].pct_change(5)\n",
    "    features['returns_10d'] = data['close'].pct_change(10)\n",
    "    features['returns_20d'] = data['close'].pct_change(20)\n",
    "    features['returns_50d'] = data['close'].pct_change(50)\n",
    "    \n",
    "    # Return volatility\n",
    "    features['vol_5d'] = data['returns'].rolling(5).std()\n",
    "    features['vol_10d'] = data['returns'].rolling(10).std()\n",
    "    features['vol_20d'] = data['returns'].rolling(20).std()\n",
    "    \n",
    "    # Skewness and kurtosis\n",
    "    features['skew_20d'] = data['returns'].rolling(20).skew()\n",
    "    features['kurt_20d'] = data['returns'].rolling(20).kurt()\n",
    "    \n",
    "    if feature_set in ['comprehensive', 'volatility_only']:\n",
    "        # 2. Volatility features\n",
    "        features['vol_of_vol'] = features['vol_20d'].rolling(20).std()\n",
    "        \n",
    "        if 'high' in data.columns and 'low' in data.columns:\n",
    "            features['hl_range'] = (data['high'] - data['low']) / data['close']\n",
    "            features['hl_range_20d'] = features['hl_range'].rolling(20).mean()\n",
    "    \n",
    "    if feature_set == 'comprehensive':\n",
    "        # 3. Trend features\n",
    "        features['sma_20'] = data['close'].rolling(20).mean()\n",
    "        features['sma_50'] = data['close'].rolling(50).mean()\n",
    "        features['sma_200'] = data['close'].rolling(200).mean()\n",
    "        \n",
    "        features['price_sma20_ratio'] = data['close'] / features['sma_20']\n",
    "        features['price_sma50_ratio'] = data['close'] / features['sma_50']\n",
    "        features['sma20_sma50_ratio'] = features['sma_20'] / features['sma_50']\n",
    "        \n",
    "        # Linear regression slope (trend strength)\n",
    "        def calc_slope(series, window=20):\n",
    "            def slope(x):\n",
    "                if len(x) < window:\n",
    "                    return np.nan\n",
    "                y = x.values\n",
    "                x_vals = np.arange(len(y))\n",
    "                slope, _ = np.polyfit(x_vals, y, 1)\n",
    "                return slope\n",
    "            return series.rolling(window).apply(slope, raw=False)\n",
    "        \n",
    "        features['trend_slope_20d'] = calc_slope(data['close'], 20)\n",
    "        \n",
    "        # 4. Momentum features\n",
    "        # RSI\n",
    "        delta = data['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        features['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        ema_12 = data['close'].ewm(span=12, adjust=False).mean()\n",
    "        ema_26 = data['close'].ewm(span=26, adjust=False).mean()\n",
    "        features['macd'] = ema_12 - ema_26\n",
    "        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()\n",
    "        features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "        \n",
    "        # Rate of change\n",
    "        features['roc_10'] = data['close'].pct_change(10)\n",
    "        features['roc_20'] = data['close'].pct_change(20)\n",
    "        \n",
    "        # 5. Volume features (if available)\n",
    "        if 'volume' in data.columns:\n",
    "            features['volume_sma_20'] = data['volume'].rolling(20).mean()\n",
    "            features['volume_ratio'] = data['volume'] / features['volume_sma_20']\n",
    "    \n",
    "    # Drop columns used for intermediate calculations\n",
    "    cols_to_keep = [c for c in features.columns if not c.startswith('sma_')]\n",
    "    features = features[cols_to_keep]\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "\n",
    "# Engineer features for market, sector, and asset\n",
    "print(\"Engineering features...\")\n",
    "\n",
    "market_features = engineer_regime_features(df_market, 'comprehensive')\n",
    "print(f\"Market features: {market_features.shape}\")\n",
    "\n",
    "sector_features = engineer_regime_features(df_sector, 'comprehensive')\n",
    "print(f\"Sector features: {sector_features.shape}\")\n",
    "\n",
    "asset_features = engineer_regime_features(df_asset, 'comprehensive')\n",
    "print(f\"Asset features: {asset_features.shape}\")\n",
    "\n",
    "print(f\"\\nFeature columns ({len(market_features.columns)}):\")\n",
    "for col in market_features.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Individual Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLUSTERING METHODS\n",
    "# ============================================================================\n",
    "\n",
    "def fit_gmm_regimes(features: pd.DataFrame, n_components_range: range = range(3, 7)) -> Tuple[np.ndarray, GaussianMixture, int]:\n",
    "    \"\"\"\n",
    "    Fit Gaussian Mixture Model for regime detection.\n",
    "    Uses BIC for optimal component selection.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, GaussianMixture, int]\n",
    "        Labels, fitted model, optimal n_components\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Find optimal n_components using BIC\n",
    "    bic_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for n in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n, random_state=42, n_init=5)\n",
    "        gmm.fit(X_scaled)\n",
    "        bic_scores.append(gmm.bic(X_scaled))\n",
    "        models.append(gmm)\n",
    "    \n",
    "    # Select model with lowest BIC\n",
    "    best_idx = np.argmin(bic_scores)\n",
    "    best_n = list(n_components_range)[best_idx]\n",
    "    best_model = models[best_idx]\n",
    "    \n",
    "    # Get labels and probabilities\n",
    "    labels = best_model.predict(X_scaled)\n",
    "    probabilities = best_model.predict_proba(X_scaled)\n",
    "    \n",
    "    print(f\"GMM: Optimal components = {best_n} (BIC = {bic_scores[best_idx]:.2f})\")\n",
    "    \n",
    "    return labels, best_model, best_n, probabilities\n",
    "\n",
    "\n",
    "def fit_kmeans_regimes(features: pd.DataFrame, k_range: range = range(3, 7)) -> Tuple[np.ndarray, KMeans, int]:\n",
    "    \"\"\"\n",
    "    Fit K-Means clustering for regime detection.\n",
    "    Uses silhouette score for optimal k selection.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        models.append(kmeans)\n",
    "    \n",
    "    best_idx = np.argmax(silhouette_scores)\n",
    "    best_k = list(k_range)[best_idx]\n",
    "    best_model = models[best_idx]\n",
    "    labels = best_model.predict(X_scaled)\n",
    "    \n",
    "    # Calculate distance to cluster centers\n",
    "    distances = best_model.transform(X_scaled)\n",
    "    min_distances = distances.min(axis=1)\n",
    "    \n",
    "    print(f\"K-Means: Optimal k = {best_k} (Silhouette = {silhouette_scores[best_idx]:.3f})\")\n",
    "    \n",
    "    return labels, best_model, best_k, min_distances\n",
    "\n",
    "\n",
    "def fit_agglomerative_regimes(features: pd.DataFrame, n_clusters_range: range = range(3, 7)) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Fit Agglomerative (Hierarchical) clustering.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for n in n_clusters_range:\n",
    "        agg = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
    "        labels = agg.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    best_idx = np.argmax(silhouette_scores)\n",
    "    best_n = list(n_clusters_range)[best_idx]\n",
    "    best_labels = all_labels[best_idx]\n",
    "    \n",
    "    print(f\"Agglomerative: Optimal clusters = {best_n} (Silhouette = {silhouette_scores[best_idx]:.3f})\")\n",
    "    \n",
    "    return best_labels, best_n\n",
    "\n",
    "\n",
    "def detect_change_points(features: pd.DataFrame, method: str = 'pelt', \n",
    "                         n_bkps: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect structural breaks using change-point detection.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Regime labels based on detected segments\n",
    "    \"\"\"\n",
    "    if not RUPTURES_AVAILABLE:\n",
    "        print(\"Warning: ruptures not available, returning uniform segments\")\n",
    "        return np.zeros(len(features))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    if method == 'pelt':\n",
    "        algo = rpt.Pelt(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(pen=3)\n",
    "    elif method == 'binseg':\n",
    "        algo = rpt.Binseg(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(n_bkps=n_bkps or 10)\n",
    "    else:\n",
    "        algo = rpt.BottomUp(model='rbf').fit(X_scaled)\n",
    "        result = algo.predict(n_bkps=n_bkps or 10)\n",
    "    \n",
    "    # Convert change points to labels\n",
    "    labels = np.zeros(len(features))\n",
    "    prev_bkp = 0\n",
    "    for i, bkp in enumerate(result):\n",
    "        labels[prev_bkp:bkp] = i\n",
    "        prev_bkp = bkp\n",
    "    \n",
    "    print(f\"CPD ({method}): {len(result)} change points detected\")\n",
    "    \n",
    "    return labels, result\n",
    "\n",
    "\n",
    "print(\"Clustering functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ensemble Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "def ensemble_regime_classification(gmm_labels: np.ndarray, kmeans_labels: np.ndarray,\n",
    "                                   agg_labels: np.ndarray, cpd_labels: np.ndarray,\n",
    "                                   gmm_probs: np.ndarray = None,\n",
    "                                   kmeans_distances: np.ndarray = None,\n",
    "                                   weights: dict = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Combine multiple clustering methods using weighted voting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gmm_labels, kmeans_labels, agg_labels, cpd_labels : np.ndarray\n",
    "        Labels from each method\n",
    "    gmm_probs : np.ndarray\n",
    "        GMM probability matrix (for confidence weighting)\n",
    "    kmeans_distances : np.ndarray\n",
    "        K-means distances to cluster centers (for confidence weighting)\n",
    "    weights : dict\n",
    "        Base weights for each method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[np.ndarray, np.ndarray]\n",
    "        Ensemble labels and confidence scores\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {'gmm': 0.3, 'kmeans': 0.25, 'agg': 0.25, 'cpd': 0.2}\n",
    "    \n",
    "    n_samples = len(gmm_labels)\n",
    "    \n",
    "    # Find number of unique regimes across all methods\n",
    "    all_unique = max(\n",
    "        len(np.unique(gmm_labels)),\n",
    "        len(np.unique(kmeans_labels)),\n",
    "        len(np.unique(agg_labels)),\n",
    "        len(np.unique(cpd_labels))\n",
    "    )\n",
    "    \n",
    "    # Create voting matrix\n",
    "    ensemble_labels = np.zeros(n_samples, dtype=int)\n",
    "    confidence_scores = np.zeros(n_samples)\n",
    "    \n",
    "    # For each sample, use majority voting\n",
    "    for i in range(n_samples):\n",
    "        votes = [\n",
    "            (gmm_labels[i], weights['gmm']),\n",
    "            (kmeans_labels[i], weights['kmeans']),\n",
    "            (agg_labels[i], weights['agg']),\n",
    "            (cpd_labels[i], weights['cpd'])\n",
    "        ]\n",
    "        \n",
    "        # Aggregate votes by label\n",
    "        vote_counts = {}\n",
    "        for label, weight in votes:\n",
    "            vote_counts[label] = vote_counts.get(label, 0) + weight\n",
    "        \n",
    "        # Select winner\n",
    "        winner = max(vote_counts.keys(), key=lambda x: vote_counts[x])\n",
    "        ensemble_labels[i] = winner\n",
    "        \n",
    "        # Confidence = fraction of weighted votes for winner\n",
    "        total_weight = sum(weights.values())\n",
    "        confidence_scores[i] = vote_counts[winner] / total_weight\n",
    "    \n",
    "    return ensemble_labels, confidence_scores\n",
    "\n",
    "\n",
    "def map_regime_labels(labels: np.ndarray, features: pd.DataFrame,\n",
    "                      feature_names: list = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map numeric cluster labels to meaningful regime names based on feature centroids.\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = ['returns_20d', 'vol_20d']\n",
    "    \n",
    "    # Calculate centroids\n",
    "    df = features.copy()\n",
    "    df['cluster'] = labels\n",
    "    \n",
    "    centroids = df.groupby('cluster')[feature_names].mean()\n",
    "    \n",
    "    # Define regime mapping based on centroid characteristics\n",
    "    regime_names = {}\n",
    "    \n",
    "    for cluster in centroids.index:\n",
    "        ret_col = [c for c in feature_names if 'return' in c.lower()]\n",
    "        vol_col = [c for c in feature_names if 'vol' in c.lower()]\n",
    "        \n",
    "        if ret_col and vol_col:\n",
    "            avg_return = centroids.loc[cluster, ret_col[0]]\n",
    "            avg_vol = centroids.loc[cluster, vol_col[0]]\n",
    "            \n",
    "            # Determine regime name\n",
    "            if avg_return > 0 and avg_vol < centroids[vol_col[0]].median():\n",
    "                regime_names[cluster] = 'Bull_Low_Vol'\n",
    "            elif avg_return > 0 and avg_vol >= centroids[vol_col[0]].median():\n",
    "                regime_names[cluster] = 'Bull_High_Vol'\n",
    "            elif avg_return < 0:\n",
    "                regime_names[cluster] = 'Bear'\n",
    "            else:\n",
    "                regime_names[cluster] = 'Choppy'\n",
    "        else:\n",
    "            regime_names[cluster] = f'Regime_{cluster}'\n",
    "    \n",
    "    return pd.Series([regime_names.get(l, 'Unknown') for l in labels], index=features.index)\n",
    "\n",
    "\n",
    "print(\"Ensemble functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Run Ensemble Regime Detection on Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - MARKET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MARKET REGIME DETECTION (SPY/QQQ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_mkt, gmm_model_mkt, gmm_n_mkt, gmm_probs_mkt = fit_gmm_regimes(market_features)\n",
    "kmeans_labels_mkt, kmeans_model_mkt, kmeans_k_mkt, kmeans_dist_mkt = fit_kmeans_regimes(market_features)\n",
    "agg_labels_mkt, agg_n_mkt = fit_agglomerative_regimes(market_features)\n",
    "cpd_labels_mkt, cpd_bkps_mkt = detect_change_points(market_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_mkt, confidence_mkt = ensemble_regime_classification(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    gmm_probs=gmm_probs_mkt, kmeans_distances=kmeans_dist_mkt\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "market_regimes = map_regime_labels(ensemble_labels_mkt, market_features, \n",
    "                                   ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nMarket Regime Distribution:\")\n",
    "print(market_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_mkt.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - SECTOR\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"SECTOR REGIME DETECTION ({SECTOR_ETF})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_sec, gmm_model_sec, gmm_n_sec, gmm_probs_sec = fit_gmm_regimes(sector_features)\n",
    "kmeans_labels_sec, kmeans_model_sec, kmeans_k_sec, kmeans_dist_sec = fit_kmeans_regimes(sector_features)\n",
    "agg_labels_sec, agg_n_sec = fit_agglomerative_regimes(sector_features)\n",
    "cpd_labels_sec, cpd_bkps_sec = detect_change_points(sector_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_sec, confidence_sec = ensemble_regime_classification(\n",
    "    gmm_labels_sec, kmeans_labels_sec, agg_labels_sec, cpd_labels_sec,\n",
    "    gmm_probs=gmm_probs_sec, kmeans_distances=kmeans_dist_sec\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "sector_regimes = map_regime_labels(ensemble_labels_sec, sector_features,\n",
    "                                   ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nSector Regime Distribution:\")\n",
    "print(sector_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_sec.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN ENSEMBLE REGIME DETECTION - ASSET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"ASSET REGIME DETECTION ({TICKER})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run individual methods\n",
    "gmm_labels_ast, gmm_model_ast, gmm_n_ast, gmm_probs_ast = fit_gmm_regimes(asset_features)\n",
    "kmeans_labels_ast, kmeans_model_ast, kmeans_k_ast, kmeans_dist_ast = fit_kmeans_regimes(asset_features)\n",
    "agg_labels_ast, agg_n_ast = fit_agglomerative_regimes(asset_features)\n",
    "cpd_labels_ast, cpd_bkps_ast = detect_change_points(asset_features)\n",
    "\n",
    "# Ensemble classification\n",
    "ensemble_labels_ast, confidence_ast = ensemble_regime_classification(\n",
    "    gmm_labels_ast, kmeans_labels_ast, agg_labels_ast, cpd_labels_ast,\n",
    "    gmm_probs=gmm_probs_ast, kmeans_distances=kmeans_dist_ast\n",
    ")\n",
    "\n",
    "# Map to meaningful names\n",
    "asset_regimes = map_regime_labels(ensemble_labels_ast, asset_features,\n",
    "                                  ['returns_20d', 'vol_20d'])\n",
    "\n",
    "print(f\"\\nAsset Regime Distribution:\")\n",
    "print(asset_regimes.value_counts())\n",
    "print(f\"\\nAverage Confidence: {confidence_ast.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Regime Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REGIME VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_regime_clustering(features: pd.DataFrame, labels: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Validate clustering quality using multiple metrics.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    metrics = {\n",
    "        'silhouette': silhouette_score(X_scaled, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(X_scaled, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels),\n",
    "        'n_clusters': len(np.unique(labels)),\n",
    "        'cluster_sizes': dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Validate all regime detections\n",
    "print(\"Regime Validation Metrics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "market_validation = validate_regime_clustering(market_features, ensemble_labels_mkt)\n",
    "print(f\"\\nMarket Regimes:\")\n",
    "print(f\"  Silhouette Score: {market_validation['silhouette']:.3f} (higher is better)\")\n",
    "print(f\"  Davies-Bouldin Index: {market_validation['davies_bouldin']:.3f} (lower is better)\")\n",
    "print(f\"  Calinski-Harabasz Index: {market_validation['calinski_harabasz']:.1f} (higher is better)\")\n",
    "\n",
    "sector_validation = validate_regime_clustering(sector_features, ensemble_labels_sec)\n",
    "print(f\"\\nSector Regimes:\")\n",
    "print(f\"  Silhouette Score: {sector_validation['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {sector_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {sector_validation['calinski_harabasz']:.1f}\")\n",
    "\n",
    "asset_validation = validate_regime_clustering(asset_features, ensemble_labels_ast)\n",
    "print(f\"\\nAsset Regimes:\")\n",
    "print(f\"  Silhouette Score: {asset_validation['silhouette']:.3f}\")\n",
    "print(f\"  Davies-Bouldin Index: {asset_validation['davies_bouldin']:.3f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {asset_validation['calinski_harabasz']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 PRIORITY VISUALIZATION #3: Regime Probability Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3a: Market Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_probabilities(price_data: pd.DataFrame, regime_series: pd.Series,\n",
    "                              confidence_scores: np.ndarray, cpd_breakpoints: list,\n",
    "                              title: str, figsize: tuple = (14, 10)):\n",
    "    \"\"\"\n",
    "    Create regime visualization with price, regimes, and confidence.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=figsize, gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "    \n",
    "    # Align data\n",
    "    common_idx = price_data.index.intersection(regime_series.index)\n",
    "    price = price_data.loc[common_idx, 'close']\n",
    "    regimes = regime_series.loc[common_idx]\n",
    "    conf = pd.Series(confidence_scores, index=regime_series.index).loc[common_idx]\n",
    "    \n",
    "    # Panel 1: Price with regime shading\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(price, color='black', linewidth=1)\n",
    "    \n",
    "    # Add regime shading\n",
    "    prev_regime = None\n",
    "    segment_start = common_idx[0]\n",
    "    \n",
    "    for idx in common_idx:\n",
    "        current_regime = regimes.loc[idx]\n",
    "        if current_regime != prev_regime and prev_regime is not None:\n",
    "            color = get_regime_color(prev_regime)\n",
    "            ax1.axvspan(segment_start, idx, alpha=0.3, color=color, label=prev_regime)\n",
    "            segment_start = idx\n",
    "        prev_regime = current_regime\n",
    "    \n",
    "    # Final segment\n",
    "    if prev_regime:\n",
    "        color = get_regime_color(prev_regime)\n",
    "        ax1.axvspan(segment_start, common_idx[-1], alpha=0.3, color=color)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for regimes\n",
    "    unique_regimes = regimes.unique()\n",
    "    patches = [mpatches.Patch(color=get_regime_color(r), alpha=0.5, label=r) \n",
    "               for r in unique_regimes]\n",
    "    ax1.legend(handles=patches, loc='upper left', fontsize=9)\n",
    "    \n",
    "    # Panel 2: Regime classification\n",
    "    ax2 = axes[1]\n",
    "    regime_numeric = pd.Categorical(regimes).codes\n",
    "    ax2.plot(common_idx, regime_numeric, drawstyle='steps-post', color='navy', linewidth=1)\n",
    "    ax2.set_ylabel('Regime')\n",
    "    ax2.set_yticks(range(len(unique_regimes)))\n",
    "    ax2.set_yticklabels(unique_regimes)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add CPD breakpoints\n",
    "    if cpd_breakpoints:\n",
    "        for bkp in cpd_breakpoints[:-1]:  # Exclude last (end of data)\n",
    "            if bkp < len(common_idx):\n",
    "                ax2.axvline(x=common_idx[bkp], color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Panel 3: Confidence score\n",
    "    ax3 = axes[2]\n",
    "    ax3.fill_between(common_idx, conf, alpha=0.5, color='green')\n",
    "    ax3.plot(common_idx, conf, color='darkgreen', linewidth=1)\n",
    "    ax3.axhline(y=0.7, color='orange', linestyle='--', label='Min confidence (0.7)')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.set_xlabel('Date')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.legend(loc='lower right', fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot Market Regimes\n",
    "plot_regime_probabilities(\n",
    "    df_market, market_regimes, confidence_mkt, cpd_bkps_mkt,\n",
    "    f'{MARKET_PROXY} - Market Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3b: Sector Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "plot_regime_probabilities(\n",
    "    df_sector, sector_regimes, confidence_sec, cpd_bkps_sec,\n",
    "    f'{SECTOR_ETF} - Sector Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #3c: Asset Regime Over Time\n",
    "# ============================================================================\n",
    "\n",
    "plot_regime_probabilities(\n",
    "    df_asset, asset_regimes, confidence_ast, cpd_bkps_ast,\n",
    "    f'{TICKER} - Asset Regime Detection (Ensemble Method)',\n",
    "    figsize=FIG_SIZE_REGIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Method Agreement Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METHOD AGREEMENT VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_comparison(gmm_labels, kmeans_labels, agg_labels, cpd_labels,\n",
    "                           ensemble_labels, index, title):\n",
    "    \"\"\"\n",
    "    Compare regime assignments from each individual method.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    methods = [\n",
    "        ('GMM', gmm_labels),\n",
    "        ('K-Means', kmeans_labels),\n",
    "        ('Agglomerative', agg_labels),\n",
    "        ('CPD', cpd_labels),\n",
    "        ('Ensemble', ensemble_labels)\n",
    "    ]\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    \n",
    "    for ax, (method, labels), color in zip(axes, methods, colors):\n",
    "        ax.plot(index, labels, drawstyle='steps-post', color=color, linewidth=1)\n",
    "        ax.fill_between(index, labels, alpha=0.3, step='post', color=color)\n",
    "        ax.set_ylabel(method, fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight ensemble row\n",
    "        if method == 'Ensemble':\n",
    "            ax.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    axes[0].set_title(title, fontsize=14)\n",
    "    axes[-1].set_xlabel('Date')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot method comparison for market\n",
    "plot_regime_comparison(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    ensemble_labels_mkt, market_features.index,\n",
    "    f'{MARKET_PROXY} - Regime Method Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONSENSUS HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "def plot_ensemble_consensus(gmm_labels, kmeans_labels, agg_labels, cpd_labels, index, title):\n",
    "    \"\"\"\n",
    "    Show agreement level between methods over time.\n",
    "    \"\"\"\n",
    "    # Calculate agreement matrix\n",
    "    n_samples = len(gmm_labels)\n",
    "    agreement = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        labels = [gmm_labels[i], kmeans_labels[i], agg_labels[i], cpd_labels[i]]\n",
    "        # Count most common label\n",
    "        most_common_count = max(labels.count(l) for l in set(labels))\n",
    "        agreement[i] = most_common_count / 4  # 4 methods\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Color by agreement level\n",
    "    colors = np.where(agreement >= 0.75, '#2ecc71',  # High agreement\n",
    "              np.where(agreement >= 0.5, '#f39c12',  # Moderate\n",
    "                       '#e74c3c'))  # Low agreement\n",
    "    \n",
    "    ax.scatter(index, agreement, c=colors, s=10, alpha=0.7)\n",
    "    ax.axhline(y=0.75, color='green', linestyle='--', label='High agreement (75%+)')\n",
    "    ax.axhline(y=0.5, color='orange', linestyle='--', label='Moderate (50%)')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Agreement Level')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Agreement Statistics:\")\n",
    "    print(f\"  High agreement (75%+): {(agreement >= 0.75).mean():.1%}\")\n",
    "    print(f\"  Moderate (50-75%): {((agreement >= 0.5) & (agreement < 0.75)).mean():.1%}\")\n",
    "    print(f\"  Low (<50%): {(agreement < 0.5).mean():.1%}\")\n",
    "\n",
    "\n",
    "plot_ensemble_consensus(\n",
    "    gmm_labels_mkt, kmeans_labels_mkt, agg_labels_mkt, cpd_labels_mkt,\n",
    "    market_features.index,\n",
    "    f'{MARKET_PROXY} - Ensemble Method Agreement Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Optimal Strategy Selection & Output\n",
    "\n",
    "This section synthesizes all analysis to determine optimal strategy allocation by regime.\n",
    "\n",
    "### 7.1 Strategy Performance by Regime Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: OPTIMAL STRATEGY SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create combined regime DataFrame\n",
    "common_idx = market_regimes.index.intersection(sector_regimes.index).intersection(asset_regimes.index)\n",
    "common_idx = common_idx.intersection(vol_regime.dropna().index)\n",
    "common_idx = common_idx.intersection(strategy_returns_df.index)\n",
    "\n",
    "regime_df = pd.DataFrame({\n",
    "    'market_regime': market_regimes.loc[common_idx],\n",
    "    'sector_regime': sector_regimes.loc[common_idx],\n",
    "    'asset_regime': asset_regimes.loc[common_idx],\n",
    "    'vol_regime': vol_regime.loc[common_idx],\n",
    "    'market_confidence': pd.Series(confidence_mkt, index=market_regimes.index).loc[common_idx],\n",
    "    'sector_confidence': pd.Series(confidence_sec, index=sector_regimes.index).loc[common_idx],\n",
    "    'asset_confidence': pd.Series(confidence_ast, index=asset_regimes.index).loc[common_idx]\n",
    "}, index=common_idx)\n",
    "\n",
    "# Add strategy returns\n",
    "for strategy in STRATEGIES:\n",
    "    regime_df[f'{strategy}_return'] = strategy_returns_df[strategy].loc[common_idx]\n",
    "\n",
    "print(f\"Combined regime data: {len(regime_df)} observations\")\n",
    "print(f\"\\nRegime combinations:\")\n",
    "combo_counts = regime_df.groupby(['market_regime', 'sector_regime']).size()\n",
    "print(combo_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY PERFORMANCE BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_regime_performance(regime_df: pd.DataFrame, strategies: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate strategy performance metrics by regime combination.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by market and sector regime\n",
    "    groups = regime_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt_regime, sec_regime), group in groups:\n",
    "        for strategy in strategies:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in group.columns:\n",
    "                returns = group[col].dropna()\n",
    "                \n",
    "                if len(returns) >= 10:  # Minimum sample size\n",
    "                    total_return = (1 + returns).prod() - 1\n",
    "                    avg_return = returns.mean()\n",
    "                    vol = returns.std()\n",
    "                    sharpe = (avg_return * 252) / (vol * np.sqrt(252)) if vol > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    # Drawdown\n",
    "                    cumulative = (1 + returns).cumprod()\n",
    "                    running_max = cumulative.expanding().max()\n",
    "                    max_dd = ((cumulative - running_max) / running_max).min()\n",
    "                    \n",
    "                    # Average confidence\n",
    "                    avg_conf = (group['market_confidence'] + group['sector_confidence']).mean() / 2\n",
    "                    \n",
    "                    results.append({\n",
    "                        'market_regime': mkt_regime,\n",
    "                        'sector_regime': sec_regime,\n",
    "                        'strategy': strategy,\n",
    "                        'avg_return': avg_return,\n",
    "                        'total_return': total_return,\n",
    "                        'sharpe': sharpe,\n",
    "                        'win_rate': win_rate,\n",
    "                        'max_drawdown': max_dd,\n",
    "                        'sample_size': len(returns),\n",
    "                        'confidence': avg_conf\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "regime_performance = calculate_regime_performance(regime_df, STRATEGIES)\n",
    "\n",
    "print(\"Strategy Performance by Regime:\")\n",
    "print(regime_performance.sort_values('sharpe', ascending=False).head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 PRIORITY VISUALIZATION #4: Strategy Performance Heatmaps by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #4: Performance Heatmaps by Strategy\n",
    "# ============================================================================\n",
    "\n",
    "def plot_regime_performance_heatmap(perf_df: pd.DataFrame, strategy: str):\n",
    "    \"\"\"\n",
    "    Create heatmap showing strategy performance across regime combinations.\n",
    "    \"\"\"\n",
    "    strategy_data = perf_df[perf_df['strategy'] == strategy]\n",
    "    \n",
    "    if strategy_data.empty:\n",
    "        print(f\"No data for {strategy}\")\n",
    "        return\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    pivot_return = strategy_data.pivot(index='market_regime', columns='sector_regime', \n",
    "                                        values='avg_return').fillna(0)\n",
    "    pivot_winrate = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                         values='win_rate').fillna(0)\n",
    "    pivot_sample = strategy_data.pivot(index='market_regime', columns='sector_regime',\n",
    "                                        values='sample_size').fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=FIG_SIZE_HEATMAP)\n",
    "    \n",
    "    # Main heatmap with returns\n",
    "    color = get_strategy_color(strategy)\n",
    "    \n",
    "    # Use diverging colormap\n",
    "    vmax = max(abs(pivot_return.values.min()), abs(pivot_return.values.max()))\n",
    "    vmin = -vmax\n",
    "    \n",
    "    sns.heatmap(pivot_return * 100, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, center=0, vmin=vmin*100, vmax=vmax*100,\n",
    "                linewidths=0.5, cbar_kws={'label': 'Avg Daily Return (%)'})\n",
    "    \n",
    "    # Add sample size annotations\n",
    "    for i in range(len(pivot_return.index)):\n",
    "        for j in range(len(pivot_return.columns)):\n",
    "            n = pivot_sample.iloc[i, j]\n",
    "            wr = pivot_winrate.iloc[i, j]\n",
    "            if n > 0:\n",
    "                ax.annotate(f'n={int(n)}\\nWR={wr:.0%}',\n",
    "                           xy=(j + 0.5, i + 0.7),\n",
    "                           ha='center', va='center',\n",
    "                           fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{strategy} - Performance by Regime Combination', \n",
    "                 fontsize=14, color=color, fontweight='bold')\n",
    "    ax.set_xlabel('Sector Regime')\n",
    "    ax.set_ylabel('Market Regime')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create separate heatmaps for each strategy\n",
    "for strategy in STRATEGIES:\n",
    "    plot_regime_performance_heatmap(regime_performance, strategy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Optimal Strategy Selection by Regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMAL STRATEGY BY REGIME\n",
    "# ============================================================================\n",
    "\n",
    "def find_optimal_strategy(perf_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the optimal strategy for each regime combination.\n",
    "    \"\"\"\n",
    "    # For each regime combination, find best strategy by Sharpe\n",
    "    optimal = []\n",
    "    \n",
    "    groups = perf_df.groupby(['market_regime', 'sector_regime'])\n",
    "    \n",
    "    for (mkt, sec), group in groups:\n",
    "        if len(group) > 0:\n",
    "            best = group.loc[group['sharpe'].idxmax()]\n",
    "            \n",
    "            # Also find optimal stop loss (simplified - use best overall for now)\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best['strategy']]\n",
    "            if len(stop_data) > 0:\n",
    "                best_stop = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                best_stop = 'None'\n",
    "            \n",
    "            optimal.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Recommended_Strategy': best['strategy'],\n",
    "                'Expected_Return': best['avg_return'],\n",
    "                'Sharpe': best['sharpe'],\n",
    "                'Max_DD': best['max_drawdown'],\n",
    "                'Win_Rate': best['win_rate'],\n",
    "                'Sample_Size': best['sample_size'],\n",
    "                'Ensemble_Confidence': best['confidence'],\n",
    "                'Stop_Loss_%': best_stop\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(optimal)\n",
    "\n",
    "\n",
    "optimal_strategies = find_optimal_strategy(regime_performance)\n",
    "\n",
    "print(\"OPTIMAL STRATEGY ALLOCATION BY REGIME:\")\n",
    "print(\"=\"*80)\n",
    "print(optimal_strategies.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE OUTPUT TABLE\n",
    "# ============================================================================\n",
    "\n",
    "# Add volatility regime and asset regime for complete picture\n",
    "def create_comprehensive_recommendations() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive strategy recommendations table.\n",
    "    \"\"\"\n",
    "    # Get unique regime combinations\n",
    "    combos = regime_df.groupby(['market_regime', 'sector_regime', 'asset_regime', 'vol_regime']).size()\n",
    "    combos = combos[combos >= 5].reset_index(name='sample_size')  # Min 5 days\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for _, row in combos.iterrows():\n",
    "        mkt, sec, ast, vol = row['market_regime'], row['sector_regime'], row['asset_regime'], row['vol_regime']\n",
    "        \n",
    "        # Filter data for this regime combination\n",
    "        mask = (\n",
    "            (regime_df['market_regime'] == mkt) &\n",
    "            (regime_df['sector_regime'] == sec) &\n",
    "            (regime_df['asset_regime'] == ast) &\n",
    "            (regime_df['vol_regime'] == vol)\n",
    "        )\n",
    "        subset = regime_df[mask]\n",
    "        \n",
    "        if len(subset) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Find best strategy\n",
    "        best_strategy = None\n",
    "        best_sharpe = -np.inf\n",
    "        best_return = 0\n",
    "        best_winrate = 0\n",
    "        \n",
    "        for strategy in STRATEGIES:\n",
    "            col = f'{strategy}_return'\n",
    "            if col in subset.columns:\n",
    "                returns = subset[col].dropna()\n",
    "                if len(returns) >= 5:\n",
    "                    avg_ret = returns.mean()\n",
    "                    vol_ret = returns.std()\n",
    "                    sharpe = (avg_ret * 252) / (vol_ret * np.sqrt(252)) if vol_ret > 0 else 0\n",
    "                    win_rate = (returns > 0).mean()\n",
    "                    \n",
    "                    if sharpe > best_sharpe:\n",
    "                        best_sharpe = sharpe\n",
    "                        best_strategy = strategy\n",
    "                        best_return = avg_ret\n",
    "                        best_winrate = win_rate\n",
    "        \n",
    "        if best_strategy:\n",
    "            # Determine delta based on strategy type\n",
    "            call_delta = 'N/A'\n",
    "            put_delta = 'N/A'\n",
    "            \n",
    "            if 'Call' in best_strategy:\n",
    "                call_delta = '0.40'\n",
    "            if 'Put' in best_strategy:\n",
    "                put_delta = '-0.20'\n",
    "            if best_strategy in ['Straddle', 'Strangle']:\n",
    "                call_delta = '0.30'\n",
    "                put_delta = '-0.30'\n",
    "            if 'Calendar' in best_strategy or 'Diagonal' in best_strategy:\n",
    "                call_delta = '0.50'\n",
    "            \n",
    "            # Get recommended stop loss\n",
    "            stop_data = stop_loss_df[stop_loss_df['strategy'] == best_strategy]\n",
    "            if len(stop_data) > 0:\n",
    "                stop_loss = stop_data.loc[stop_data['sharpe'].idxmax(), 'stop_loss']\n",
    "            else:\n",
    "                stop_loss = 'None'\n",
    "            \n",
    "            # Average confidence\n",
    "            avg_conf = subset[['market_confidence', 'sector_confidence', 'asset_confidence']].mean().mean()\n",
    "            \n",
    "            recommendations.append({\n",
    "                'Regime_Market': mkt,\n",
    "                'Regime_Sector': sec,\n",
    "                'Regime_Asset': ast,\n",
    "                'Volatility_Regime': vol,\n",
    "                'Ensemble_Confidence': f'{avg_conf:.2f}',\n",
    "                'Recommended_Strategy': best_strategy,\n",
    "                'Call_Delta': call_delta,\n",
    "                'Put_Delta': put_delta,\n",
    "                'Stop_Loss_%': stop_loss,\n",
    "                'Expected_Return': f'{best_return*100:.2f}%',\n",
    "                'Sharpe': f'{best_sharpe:.2f}',\n",
    "                'Win_Rate': f'{best_winrate:.0%}',\n",
    "                'Sample_Size': len(subset)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "\n",
    "comprehensive_recommendations = create_comprehensive_recommendations()\n",
    "\n",
    "print(\"\\nCOMPREHENSIVE STRATEGY RECOMMENDATIONS:\")\n",
    "print(\"=\"*120)\n",
    "print(comprehensive_recommendations.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# comprehensive_recommendations.to_csv('strategy_recommendations.csv', index=False)\n",
    "# print(\"\\nRecommendations saved to strategy_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Current Regime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CURRENT REGIME ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Get current (most recent) regime states\n",
    "current_market = market_regimes.iloc[-1]\n",
    "current_sector = sector_regimes.iloc[-1]\n",
    "current_asset = asset_regimes.iloc[-1]\n",
    "current_vol = vol_regime.iloc[-1] if len(vol_regime) > 0 else 'Unknown'\n",
    "\n",
    "current_market_conf = confidence_mkt[-1] if len(confidence_mkt) > 0 else 0\n",
    "current_sector_conf = confidence_sec[-1] if len(confidence_sec) > 0 else 0\n",
    "current_asset_conf = confidence_ast[-1] if len(confidence_ast) > 0 else 0\n",
    "\n",
    "print(\"CURRENT REGIME STATES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Date: {regime_df.index[-1]}\")\n",
    "print(f\"\\nMarket Regime: {current_market} (Confidence: {current_market_conf:.2%})\")\n",
    "print(f\"Sector Regime: {current_sector} (Confidence: {current_sector_conf:.2%})\")\n",
    "print(f\"Asset Regime:  {current_asset} (Confidence: {current_asset_conf:.2%})\")\n",
    "print(f\"Volatility:    {current_vol}\")\n",
    "\n",
    "# Find recommended strategy for current regime\n",
    "current_rec = comprehensive_recommendations[\n",
    "    (comprehensive_recommendations['Regime_Market'] == current_market) &\n",
    "    (comprehensive_recommendations['Regime_Sector'] == current_sector)\n",
    "]\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    print(\"\\nRECOMMENDED STRATEGY FOR CURRENT REGIME:\")\n",
    "    print(\"-\"*60)\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Call Delta: {rec['Call_Delta']}\")\n",
    "    print(f\"Put Delta: {rec['Put_Delta']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "    print(f\"Expected Return: {rec['Expected_Return']}\")\n",
    "    print(f\"Sharpe Ratio: {rec['Sharpe']}\")\n",
    "    print(f\"Win Rate: {rec['Win_Rate']}\")\n",
    "else:\n",
    "    print(\"\\nNo historical data for current regime combination.\")\n",
    "    print(\"Consider using the most similar regime or conservative positioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 PRIORITY VISUALIZATION #5: Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRIORITY VISUALIZATION #5: Summary Dashboard\n",
    "# ============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=FIG_SIZE_DASHBOARD)\n",
    "\n",
    "# Create 3x3 grid\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Panel 1: Current regime states\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "regimes = ['Market', 'Sector', 'Asset']\n",
    "regime_values = [current_market, current_sector, current_asset]\n",
    "confidence_values = [current_market_conf, current_sector_conf, current_asset_conf]\n",
    "colors = [get_regime_color(r) for r in regime_values]\n",
    "\n",
    "bars = ax1.barh(regimes, confidence_values, color=colors, alpha=0.7)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_xlabel('Confidence')\n",
    "ax1.set_title('Current Regime States', fontsize=12)\n",
    "\n",
    "for bar, val, regime in zip(bars, confidence_values, regime_values):\n",
    "    ax1.annotate(f'{regime}\\n{val:.0%}', \n",
    "                xy=(val + 0.02, bar.get_y() + bar.get_height()/2),\n",
    "                va='center', fontsize=9)\n",
    "\n",
    "# Panel 2: Volatility distribution\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "vol_hist = realized_vol['rv_true'].dropna()\n",
    "current_vol_value = vol_hist.iloc[-1] if len(vol_hist) > 0 else 0\n",
    "\n",
    "ax2.hist(vol_hist, bins=30, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "ax2.axvline(x=current_vol_value, color='red', linewidth=2, label=f'Current: {current_vol_value:.2%}')\n",
    "ax2.axvline(x=vol_hist.mean(), color='green', linestyle='--', label=f'Mean: {vol_hist.mean():.2%}')\n",
    "ax2.set_title('Volatility Distribution', fontsize=12)\n",
    "ax2.set_xlabel('Volatility')\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# Panel 3: Recommended strategy\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    strategy_color = get_strategy_color(rec['Recommended_Strategy'])\n",
    "    \n",
    "    text = f\"RECOMMENDED STRATEGY\\n\\n\"\n",
    "    text += f\"{rec['Recommended_Strategy']}\\n\\n\"\n",
    "    text += f\"Stop Loss: {rec['Stop_Loss_%']}\\n\"\n",
    "    text += f\"Expected Return: {rec['Expected_Return']}\\n\"\n",
    "    text += f\"Sharpe: {rec['Sharpe']}\"\n",
    "    \n",
    "    ax3.text(0.5, 0.5, text, ha='center', va='center', fontsize=11,\n",
    "             transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor=strategy_color, alpha=0.3))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No recommendation\\nfor current regime', \n",
    "             ha='center', va='center', fontsize=11, transform=ax3.transAxes)\n",
    "\n",
    "ax3.set_title('Strategy Recommendation', fontsize=12)\n",
    "\n",
    "# Panel 4: Performance metrics table\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.axis('off')\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    table_data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Win Rate', rec['Win_Rate']],\n",
    "        ['Sharpe', rec['Sharpe']],\n",
    "        ['Sample Size', str(rec['Sample_Size'])],\n",
    "        ['Confidence', rec['Ensemble_Confidence']]\n",
    "    ]\n",
    "    \n",
    "    table = ax4.table(cellText=table_data, loc='center', cellLoc='center',\n",
    "                      colWidths=[0.4, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "\n",
    "ax4.set_title('Expected Performance', fontsize=12)\n",
    "\n",
    "# Panel 5: Historical performance of recommended strategy\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec_strategy = current_rec.iloc[0]['Recommended_Strategy']\n",
    "    strategy_returns_series = strategy_returns_df[rec_strategy]\n",
    "    cumulative = (1 + strategy_returns_series).cumprod()\n",
    "    \n",
    "    color = get_strategy_color(rec_strategy)\n",
    "    ax5.plot(cumulative, color=color, linewidth=1.5)\n",
    "    ax5.set_title(f'{rec_strategy} Historical Performance', fontsize=12)\n",
    "    ax5.set_ylabel('Cumulative Return')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 6: Risk warnings\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.axis('off')\n",
    "\n",
    "# Check for warnings\n",
    "warnings = []\n",
    "avg_conf = (current_market_conf + current_sector_conf + current_asset_conf) / 3\n",
    "\n",
    "if avg_conf < 0.6:\n",
    "    warnings.append(\"\u26a0\ufe0f Low regime confidence\")\n",
    "if current_vol == 'Extreme':\n",
    "    warnings.append(\"\u26a0\ufe0f Extreme volatility detected\")\n",
    "if len(current_rec) == 0:\n",
    "    warnings.append(\"\u26a0\ufe0f Novel regime combination\")\n",
    "if len(current_rec) > 0 and int(current_rec.iloc[0]['Sample_Size']) < 20:\n",
    "    warnings.append(\"\u26a0\ufe0f Limited historical data\")\n",
    "\n",
    "if warnings:\n",
    "    warning_text = \"RISK WARNINGS\\n\\n\" + \"\\n\".join(warnings)\n",
    "    ax6.text(0.5, 0.5, warning_text, ha='center', va='center', fontsize=10,\n",
    "             transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ffcccc', alpha=0.5))\n",
    "else:\n",
    "    ax6.text(0.5, 0.5, '\u2705 No significant warnings\\n\\nRegime stable with\\nhigh confidence',\n",
    "             ha='center', va='center', fontsize=10, transform=ax6.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='#ccffcc', alpha=0.5))\n",
    "\n",
    "ax6.set_title('Risk Alerts', fontsize=12)\n",
    "\n",
    "# Panel 7: Method agreement pie chart\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "# Calculate method agreement\n",
    "agreement_levels = {\n",
    "    'High (75%+)': (confidence_mkt >= 0.75).mean(),\n",
    "    'Moderate (50-75%)': ((confidence_mkt >= 0.5) & (confidence_mkt < 0.75)).mean(),\n",
    "    'Low (<50%)': (confidence_mkt < 0.5).mean()\n",
    "}\n",
    "\n",
    "colors_pie = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax7.pie([v for v in agreement_levels.values()], labels=agreement_levels.keys(),\n",
    "        autopct='%1.0f%%', colors=colors_pie, startangle=90)\n",
    "ax7.set_title('Ensemble Agreement Distribution', fontsize=12)\n",
    "\n",
    "# Panel 8: PCA feature space (simplified 2D)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "# Quick PCA for visualization\n",
    "scaler_viz = StandardScaler()\n",
    "X_viz = scaler_viz.fit_transform(asset_features.dropna())\n",
    "pca_viz = PCA(n_components=2)\n",
    "X_pca = pca_viz.fit_transform(X_viz)\n",
    "\n",
    "# Color by regime\n",
    "colors_scatter = [get_regime_color(r) for r in asset_regimes.loc[asset_features.dropna().index]]\n",
    "ax8.scatter(X_pca[:, 0], X_pca[:, 1], c=colors_scatter, alpha=0.5, s=10)\n",
    "ax8.scatter(X_pca[-1, 0], X_pca[-1, 1], c='black', s=100, marker='*', \n",
    "            label='Current', edgecolors='white', linewidth=1)\n",
    "ax8.set_xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]:.0%})')\n",
    "ax8.set_ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]:.0%})')\n",
    "ax8.set_title('Feature Space Position', fontsize=12)\n",
    "ax8.legend(loc='upper right', fontsize=9)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 9: Regime transition probabilities\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "# Get transition probabilities from current regime\n",
    "if current_vol in transition_probs.index:\n",
    "    trans = transition_probs.loc[current_vol]\n",
    "    colors_bar = [get_vol_regime_color(r) for r in trans.index]\n",
    "    ax9.bar(trans.index, trans.values, color=colors_bar, alpha=0.7)\n",
    "    ax9.set_ylim(0, 1)\n",
    "    ax9.set_ylabel('Probability')\n",
    "    ax9.set_title(f'Next Regime Probability\\n(from {current_vol})', fontsize=12)\n",
    "    ax9.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax9.text(0.5, 0.5, 'Transition data\\nnot available', \n",
    "             ha='center', va='center', transform=ax9.transAxes)\n",
    "    ax9.set_title('Regime Transition', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'{TICKER} - Volatility Trading Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Portfolio Extension Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PORTFOLIO EXTENSION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "# This section provides structure for future multi-asset portfolio optimization\n",
    "\n",
    "class VolatilityTradingPortfolio:\n",
    "    \"\"\"\n",
    "    Framework for multi-asset volatility trading portfolio.\n",
    "    \n",
    "    Designed to be extended for portfolio-level optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tickers: List[str]):\n",
    "        self.tickers = tickers\n",
    "        self.asset_data = {}\n",
    "        self.regime_data = {}\n",
    "        self.strategy_allocations = {}\n",
    "        \n",
    "    def add_asset(self, ticker: str, data: pd.DataFrame, regimes: pd.Series):\n",
    "        \"\"\"Add asset data to portfolio.\"\"\"\n",
    "        self.asset_data[ticker] = data\n",
    "        self.regime_data[ticker] = regimes\n",
    "        \n",
    "    def calculate_cross_asset_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate correlation matrix across assets.\"\"\"\n",
    "        returns_df = pd.DataFrame()\n",
    "        for ticker, data in self.asset_data.items():\n",
    "            returns_df[ticker] = data['returns'] if 'returns' in data.columns else data['close'].pct_change()\n",
    "        return returns_df.corr()\n",
    "    \n",
    "    def calculate_portfolio_greeks(self, positions: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate aggregate portfolio Greeks.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        positions : dict\n",
    "            Dictionary of position sizes by ticker and strategy\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Aggregate delta, gamma, theta, vega\n",
    "        \"\"\"\n",
    "        # Placeholder for portfolio-level Greek calculations\n",
    "        return {\n",
    "            'portfolio_delta': 0,\n",
    "            'portfolio_gamma': 0,\n",
    "            'portfolio_theta': 0,\n",
    "            'portfolio_vega': 0\n",
    "        }\n",
    "    \n",
    "    def check_regime_correlation(self) -> pd.DataFrame:\n",
    "        \"\"\"Check if assets share regimes simultaneously.\"\"\"\n",
    "        regime_df = pd.DataFrame()\n",
    "        for ticker, regimes in self.regime_data.items():\n",
    "            regime_df[ticker] = pd.Categorical(regimes).codes\n",
    "        return regime_df.corr()\n",
    "    \n",
    "    def optimize_allocation(self, constraints: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Optimize strategy allocation across assets.\n",
    "        \n",
    "        Placeholder for future implementation of:\n",
    "        - Mean-variance optimization\n",
    "        - Risk parity\n",
    "        - Regime-based allocation\n",
    "        \"\"\"\n",
    "        # Placeholder - to be implemented\n",
    "        return {ticker: {'strategy': None, 'weight': 0} for ticker in self.tickers}\n",
    "\n",
    "\n",
    "# Example usage (for future expansion)\n",
    "print(\"Portfolio Extension Framework initialized.\")\n",
    "print(\"\")\n",
    "print(\"Key methods for future implementation:\")\n",
    "print(\"  - add_asset(): Add multiple assets to portfolio\")\n",
    "print(\"  - calculate_cross_asset_correlation(): Diversification analysis\")\n",
    "print(\"  - calculate_portfolio_greeks(): Aggregate risk exposure\")\n",
    "print(\"  - check_regime_correlation(): Regime synchronization\")\n",
    "print(\"  - optimize_allocation(): Multi-asset strategy allocation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Conclusions\n",
    "\n",
    "This notebook has implemented a comprehensive volatility trading research framework including:\n",
    "\n",
    "### Key Components:\n",
    "1. **Volatility Modeling**: Multiple models (Historical, EWMA, Parkinson, Garman-Klass, Rogers-Satchell, ATR) with PCA-based aggregate forecasting\n",
    "\n",
    "2. **Mean Reversion Analysis**: Volatility regime classification with transition probabilities and DTE recommendations\n",
    "\n",
    "3. **Options Strategies**: Six strategies (Calendar Spread, Double Diagonal, Straddle, Strangle, Bull Put Spread, Bull Call Spread) with delta optimization\n",
    "\n",
    "4. **Ensemble Regime Detection**: Combined GMM, K-Means, Agglomerative Clustering, and Change-Point Detection for robust regime identification at Market, Sector, and Asset levels\n",
    "\n",
    "5. **Stop Loss Optimization**: Systematic testing of stop loss thresholds by strategy\n",
    "\n",
    "6. **Comprehensive Output**: Machine-readable recommendations table with regime-specific strategy allocation\n",
    "\n",
    "### Technical Implementation:\n",
    "- All computations use vectorized operations (NumPy/Pandas)\n",
    "- No multiprocessing (QuantConnect compatible)\n",
    "- Ticker-agnostic design for easy portfolio extension\n",
    "- Consistent color palettes across all visualizations\n",
    "\n",
    "### Next Steps:\n",
    "1. Connect to live QuantConnect options data\n",
    "2. Implement actual options pricing models\n",
    "3. Extend to multi-asset portfolio optimization\n",
    "4. Add real-time regime monitoring\n",
    "5. Backtest with actual transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# END OF NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VOLATILITY TRADING RESEARCH NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Ticker analyzed: {TICKER}\")\n",
    "print(f\"Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"\")\n",
    "print(f\"Current Regimes:\")\n",
    "print(f\"  Market: {current_market}\")\n",
    "print(f\"  Sector: {current_sector}\")\n",
    "print(f\"  Asset:  {current_asset}\")\n",
    "print(f\"  Volatility: {current_vol}\")\n",
    "print(f\"\")\n",
    "\n",
    "if len(current_rec) > 0:\n",
    "    rec = current_rec.iloc[0]\n",
    "    print(f\"Recommended Strategy: {rec['Recommended_Strategy']}\")\n",
    "    print(f\"Stop Loss: {rec['Stop_Loss_%']}\")\n",
    "else:\n",
    "    print(\"No specific recommendation for current regime.\")\n",
    "\n",
    "print(f\"\")\n",
    "print(\"See comprehensive_recommendations DataFrame for full regime-strategy mapping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}